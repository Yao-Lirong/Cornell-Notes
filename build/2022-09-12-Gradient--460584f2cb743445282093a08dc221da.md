---
title: Gradient Descent
date: 2022-09-12
tags:
  - CS4787
---

## Optimization Methods

### Empirical Risk Minimization

For a model / hypothesis $h$ and its associated parameters $w \in \R^d$, we can write out its loss in the following form

$$
f(w) = \frac{1}{|\mathcal{D}|} \sum_{x \in \mathcal{D}} f(w; x)
$$

### Gradient Descent

With $w_0$ initialized to some value, we then update $w$ in each step according to the following rule

$$
w_{t+1} = w_t - \alpha_t \cdot \nabla f(w_t) = w_t - \alpha_t \cdot \frac{1}{n} \sum_{x \in \mathcal{D}} \nabla f(w; x)
$$

GD takes $O(nd)$ and $O(d)$ memory.

### Newton's Method

$$
w_{t+1} = w_t -  H(w_t) \nabla f(w_t).
$$

$H(x)$ is the Hessian matrix, which is the second order derivatives matrix. Sometimes people directly write $H(w_t) = \left( \nabla^2 f(w_t) \right)^{-1}$ .

Newton's method takes $O(nd^2 + d^3)$ time and $O(d^2)$ memory. Despite the problem with existence of second order derivative, the enormous extra time we have to spend calculating Hessian matrix when $d$ gets big is the main reason why people avoid Newton's method.

## Gradient Descent Converges

### L-Smooth

We can prove that gradient descent works under the assumption that the second derivative of the objective is bounded. Formally, such function is called **L-smooth**.

We call a function _L-smooth_ when its gradient is _L-Lipschitz continuous_. That is

$$
\| \nabla f(w) - \nabla f(u) \|_2 \le L \| u-v \|_2
$$

From the above definition, we can derive the following property, which guarantees that **small change in weight only results in small change in gradient** (the rate of gradient change is bounded by $L$) This is the intuition behind why we need L-smooth for GD to converge.

$$
\forall \mathbf w,\mathbf u \in \R^d, \alpha \in \R \; s.t. \|\mathbf u\|=1, \; \|\frac{\partial^2}{\partial\alpha^2} f(\mathbf w+\alpha \mathbf u) \| \le L
$$

For example, take $f(x) = \sin(x)$ as an example, we have $|f''(x)| = |-\sin(x)| \le 1$, so we call $f$ is 1-smooth and $f'$ is 1-Lipchitz continuous. Another example $g(x) = x^3$, so $g'(x) = 3x^2, g''(x) = 6x$ and a small x change can result in arbitrary large gradient descent, so this is bad for gradient descent.

### Proof

Assume there exists a global minimum $f^* \; s.t. \; \forall w \in \R^d, \; f(w) \ge f^*$

First, we start with looking at how the loss changes from step $t$ to step $t+1$.

$$
f(w_{t+1}) = f(w_t - \alpha \nabla f(w_t))
$$

By Fundamental Theorem of Calculus: $h(a) = h(0) + \int^a_0 h'(t) dt$

$$
f(w_{t+1}) = f(w_t) + \int^\alpha_0 \frac{\partial}{\partial\eta} f(w_t - \eta \nabla f(w_t)) \; d\eta
$$

We rewrite this equation with $\hat w = w_t - \eta \nabla f(w_t)$ (refer to the appendix for a detailed derivation)

$$
f(w_{t+1}) = f(w_t) + \int^\alpha_0 -\nabla f(\hat w)^T \nabla f(w_t) \; d\eta
$$

Subtract and add $\nabla f(w_t)^T \nabla f(w_t)$

$$
\begin{align}
f(w_{t+1}) &= f(w_t) + \int^\alpha_0 - \nabla f(w_t)^T \nabla f(w_t) +  \nabla f(w_t)^T \nabla f(w_t) - \nabla f(\hat w)^T \nabla f(w_t) \; d\eta  \\
&= f(w_t) + \int^\alpha_0 - \nabla f(w_t)^T \nabla f(w_t) +  (\nabla f(w_t)^T - \nabla f(\hat w)^T)  \nabla f(w_t) \; d\eta \\
&= f(w_t) - \int^\alpha_0 \nabla f(w_t)^T \nabla f(w_t) \; d\eta
 + \int^\alpha_0  (\nabla f(w_t) - \nabla f(\hat w))^T  \nabla f(w_t) \; d\eta
\end{align}
$$

Evaluate the first integral and replace the second integral with the triangular inequality

$$
f(w_{t+1}) \le f(w_t) -\alpha \nabla f(w_t)^T \nabla f(w_t) + \int_{0}^{\alpha} \| \nabla f(w_t) \| \cdot \| \nabla f(w_t) - \nabla f(w_t - \eta \nabla f(w_t)) \| \; d \eta \\
$$

Since our function is L-smooth,

$$
\begin{align}
f(w_{t+1}) &\le f(w_t) - \alpha \nabla f(w_t)^T \nabla f(w_t) + \int_{0}^{\alpha} \| \nabla f(w_t) \| \cdot L \| w_t - w_t - \eta \nabla f(w_t) \| \; d \eta \\
&\le f(w_t) - \alpha \nabla f(w_t)^T \nabla f(w_t) + \int_{0}^{\alpha} \| \nabla f(w_t) \| \cdot L \| \eta \nabla f(w_t) \| \; d \eta \\
&\le f(w_t) - \alpha \| \nabla f(w_t) \|^2 + L \| \nabla f(w_t) \|^2 \int_{0}^{\alpha} \eta \; d \eta \\
&\le f(w_t) - \alpha \| \nabla f(w_t) \|^2 + \frac{\alpha^2 L}{2} \| \nabla f(w_t) \|^2 \\
&\le f(w_t) - \alpha \left(1 - \frac{\alpha L}{2}  \right) \cdot \| \nabla f(w_t) \|^2 \\
\end{align}
$$

Note Gradient Descent only converges when $1 - \frac{\alpha L}{2} \gt 0$, so the loss function is guaranteed to decrease at each step. We now make an assumption that $\alpha L \le 1$ and we can safely **conclude that GD converges**.

### How fast it Converges & Where does it Go?

From our assumption $\alpha L \le 1$ above, we can get rid of the $\frac{\alpha L}{2}$ term

$$
f(w_{t+1}) \le f(w_t) - \frac{\alpha}{2} \| \nabla f(w_t) \|^2
$$

We now move stuff a little bit and calculate the accumulated value when we run $T$ iterations

$$
\begin{align}
\frac{\alpha}{2} \| \nabla f(w_t) \|^2  &\le f(w_t) - f(w_{t+1}) \\
\frac{\alpha}{2} \sum_{t=0}^{T-1} \| \nabla f(w_t) \|^2 &\le f(w_0) - f(w_{T})
\end{align}
$$

Recall we have a global minimum loss $f^*$. We can also find the minimum gradient norm $\| \nabla f(w_t) \|^2$ when we run these $T$ iterations.

$$
\frac{\alpha T}{2} \min_{t \in [0, T-1]} \| \nabla f(w_t) \|^2
\le \frac{\alpha}{2} \sum_{t=0}^{T-1} \| \nabla f(w_t) \|^2
\le f(w_0) - f(w_{T})
\le f(w_0) - f^*
$$

We now have this following equation. We started off looking at the change in loss from each iteration. Then we accumulated the difference to look at the change by running $T$ iterations. Now we ended up here with this inequality with a $\min$.

$$
\min_{t \in [0, T-1]} \| \nabla f(w_t) \|^2  \le 2\frac{f(w_0) - f^*}{\alpha T}
$$

What is the minimum norm of gradient we can get in theory? It's 0 when we reach a minimum of loss function $f$, though we do not know whether that is a local minimum or a global one. No matter what kind of a minimum it is, if we reach it, we can call Gradient Descent has converged. Therefore, we want to make the bound on the right as tight as possible.

We can do this by either running more iterations $T$ or have a faster learning rate $\alpha$. Within limited iterations, let's tweak $\alpha$ here. Recall that $\alpha L \le 1$, so replace $\alpha = \frac 1  L$, we have

$$
\min_{t \in [0, T-1]} \| \nabla f(w_t) \|^2  \le 2L \frac{f(w_0) - f^*}{T}
$$

This means that the smallest gradient we observe after $T$ iterations is getting smaller proportional to $1/T$ and GD converges.

Note this proof doesn't show it reaches a global minimum for the reasons explained above.

### Appendix

The derivative of $f$ at point $w$ in the direction $u$ is defined as the following

$$
\nabla f(w)^T u = \lim_{\delta \rightarrow 0} \frac{f(w + \delta u) - f(w)}{\delta}
$$

Also the definition of a partial derivative is

$$
\frac{\partial}{\partial x}f(x,y,z) = \lim_{\delta\to0} \frac{f(x+\delta,y,z) - f(x,y,z)}{\delta}
$$

According to the definition of partial derivative, we add a little $\delta$ on the $\eta$ we are differentiating with:

$$
\begin{align}

\frac{\partial}{\partial\eta} f(w_t - \eta \nabla f(w_t))
&= \lim_{\delta \to 0} \frac{f(w_t - (\eta + \delta) \nabla f(w_t)) - f(w_t - \eta \nabla f(w_t))}{\delta} && \text{partial derivative definition} \\
&= \lim_{\delta \to 0} \frac{f(w_t - \eta \nabla f(w_t) - \delta \nabla f(w_t)) - f(w_t - \eta \nabla f(w_t))}{\delta}\\
&= \lim_{\delta \to 0} \frac{f(\hat w - \delta \nabla f(w_t)) - f(\hat w)}{\delta} && \text{Call $\hat w = w_t - \eta \nabla f(w_t)$} \\
&= -\nabla f(\hat w)^T \nabla f(w_t) && \text{definition of gradient}\\
&= -\nabla f(w_t - \eta \nabla f(w_t))^T \nabla f(w_t)

\end{align}
$$

So we have translated this partial derivative to the gradient at point $w_t - \eta \nabla f(w_t)$ in the direction of $-\nabla f(w_t)$.
