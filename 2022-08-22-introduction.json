{"version":2,"kind":"Article","sha256":"5492531c965282910aa2cbfe2f8e4fb4659ce8da2e589cc86086e33b9c2241a6","slug":"2022-08-22-introduction","location":"/CS4787/2022-08-22-Introduction.md","dependencies":[],"frontmatter":{"title":"Introduction","tags":["CS4787"],"date":"2022-08-22","authors":[{"nameParsed":{"literal":"Yao Lirong","given":"Yao","family":"Lirong"},"name":"Yao Lirong","affiliations":["Cornell University"],"url":"https://yao-lirong.github.io","linkedin":"https://www.linkedin.com/in/yao-lirong/","id":"contributors-myst-generated-uid-0"}],"keywords":["Cornell","CS","Yao Lirong"],"affiliations":[{"id":"Cornell University","name":"Cornell University"}],"numbering":{"title":{"offset":1}},"exports":[{"format":"md","filename":"2022-08-22-Introduction.md","url":"/cornell-notes/build/2022-08-22-Introduct-9b82349020ae461354de166bae7e53ba.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Three principles that allow us to scale Machine Learning:","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"aSiTZVxlz2"}],"key":"epyIiBAWFY"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"strong","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Optimization","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"OfRJq0g9rX"}],"key":"qSoLQOYrT0"},{"type":"text","value":": Write learning task as an optimization problem and solve it with fast and canned gradient-based algorithm using linear algebra.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"BxlzcFgGdU"}],"key":"u6pdyxykyh"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Recall the perceptron: it was a prediction model + a specific learning algorithm to this model. Having a different learning algorithm for each model isnâ€™t what we want. Optimization allows generalization.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"q2mAqiujV8"}],"key":"hkZnQ7yhOV"}],"key":"st3seWNf4d"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"strong","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Statistics","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"tOmM0dveiN"}],"key":"U8Q68H9Pm6"},{"type":"text","value":": To process a large dataset, we can just process a small random subsample instead.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"Wt4NHpDq22"}],"key":"KR4bDbaY86"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":16,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Stochastic Gradient Descent: use a subset of loss to do GD","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"w8cCtIEMnq"}],"key":"rvbIm6WfBN"},{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"cross-validation / train-validation-test split: use a subsample of whole dataset to represent the whole","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"XWbZDGISuy"}],"key":"APtBsBxZwD"}],"key":"Mv07Ck4sIc"}],"key":"UX3R7nDcUM"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"strong","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"Hardware","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"HlGbaTEAR2"}],"key":"NR1aabKyTG"},{"type":"text","value":": use algorithms that fit your hardware and use hardware that fits your algorithm","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"hSUxJvKR0F"}],"key":"ZMAdGS5hCD"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":21,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"request memory in a power of 2","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"fvPCzcbQ6e"}],"key":"RhIBvkf4BC"},{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"build TPU to accelerate computation","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"en7ijGDAj2"}],"key":"xENSpzqw2u"}],"key":"Mb5LFz6laj"}],"key":"sLQ8SqcOZf"}],"key":"eQhXt0Wwdx"}],"key":"zljRBZ1pHs"}],"key":"qNENLp2sqB"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Boosting","url":"/2021-11-18-boosting","group":"CS4780 Intro to Machine Learning"},"next":{"title":"Linear Algebra and NumPy","url":"/2022-08-24-linear-algebra-and-numpy","group":"CS4787 Principles of Large-Scale Machine Learning"}}},"domain":"http://localhost:3000"}