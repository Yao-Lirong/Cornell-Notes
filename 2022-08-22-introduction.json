{"version":2,"kind":"Article","sha256":"5492531c965282910aa2cbfe2f8e4fb4659ce8da2e589cc86086e33b9c2241a6","slug":"2022-08-22-introduction","location":"/CS4787/2022-08-22-Introduction.md","dependencies":[],"frontmatter":{"title":"Introduction","tags":["CS4787"],"date":"2022-08-22","authors":[{"nameParsed":{"literal":"Yao Lirong","given":"Yao","family":"Lirong"},"name":"Yao Lirong","affiliations":["Cornell University"],"url":"https://yao-lirong.github.io","linkedin":"https://www.linkedin.com/in/yao-lirong/","id":"contributors-myst-generated-uid-0"}],"keywords":["Cornell","CS","Yao Lirong"],"affiliations":[{"id":"Cornell University","name":"Cornell University"}],"numbering":{"title":{"offset":1}},"exports":[{"format":"md","filename":"2022-08-22-Introduction.md","url":"/cornell-notes/build/2022-08-22-Introduct-9b82349020ae461354de166bae7e53ba.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Three principles that allow us to scale Machine Learning:","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"onHXx3Fe2o"}],"key":"jZjCrxAFYc"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"strong","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Optimization","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"Z8XCpN8wRL"}],"key":"oDOa0AuCoC"},{"type":"text","value":": Write learning task as an optimization problem and solve it with fast and canned gradient-based algorithm using linear algebra.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"oAnoOwPVaX"}],"key":"yQyE4hgJOK"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Recall the perceptron: it was a prediction model + a specific learning algorithm to this model. Having a different learning algorithm for each model isnâ€™t what we want. Optimization allows generalization.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"jsK3Q077WD"}],"key":"jKHGDIgeBh"}],"key":"AFvs1mkv3E"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"strong","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Statistics","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"ISsrR3Tp5Y"}],"key":"n2LL5nncAM"},{"type":"text","value":": To process a large dataset, we can just process a small random subsample instead.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"khRdQkpamG"}],"key":"DogIUYpRsf"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":16,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Stochastic Gradient Descent: use a subset of loss to do GD","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"tTSDons0Lf"}],"key":"j5uqi4xQQA"},{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"cross-validation / train-validation-test split: use a subsample of whole dataset to represent the whole","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"WA0JdIrVp1"}],"key":"v6xBkVwcgo"}],"key":"GdcJZeye9A"}],"key":"dLmy0sp9mU"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"strong","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"Hardware","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"aWPkPpAHto"}],"key":"vuNx1DzCKx"},{"type":"text","value":": use algorithms that fit your hardware and use hardware that fits your algorithm","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"tmlke8NfCn"}],"key":"YVFa6mCBJQ"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":21,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"request memory in a power of 2","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"DDWXd0zqPA"}],"key":"rRmP3j66MS"},{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"build TPU to accelerate computation","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"Mt1Q3wigyZ"}],"key":"TgC74N6fHS"}],"key":"MLLJDQvHU6"}],"key":"jH2P1eOGBW"}],"key":"jcDdEE8q87"}],"key":"XBu7x1cnOg"}],"key":"lojHokONBa"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Boosting","url":"/2021-11-18-boosting","group":"CS4780 Intro to Machine Learning"},"next":{"title":"Linear Algebra and NumPy","url":"/2022-08-24-linear-algebra-and-numpy","group":"CS4787 Principles of Large-Scale Machine Learning"}}},"domain":"http://localhost:3000"}