{"version":2,"kind":"Article","sha256":"0d5f73812d60f220e69f88588b263de860db74f07b1f91e1ba9811328f2fe955","slug":"2022-10-17-accelerate-dnn-training","location":"/CS4787/2022-10-17-Accelerate-DNN-Training.md","dependencies":[],"frontmatter":{"title":"Accelerate DNN Training","tags":["CS4787"],"date":"2022-10-17","authors":[{"nameParsed":{"literal":"Yao Lirong","given":"Yao","family":"Lirong"},"name":"Yao Lirong","affiliations":["Cornell University"],"url":"https://yao-lirong.github.io","linkedin":"https://www.linkedin.com/in/yao-lirong/","id":"contributors-myst-generated-uid-0"}],"keywords":["Cornell","CS","Yao Lirong"],"affiliations":[{"id":"Cornell University","name":"Cornell University"}],"numbering":{"title":{"offset":1}},"exports":[{"format":"md","filename":"2022-10-17-Accelerate-DNN-Training.md","url":"/cornell-notes/build/2022-10-17-Accelerat-09d4ff2985a6c91f15846c4c761f83c8.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Describing Capacity","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"q1Nlk2VUsb"}],"identifier":"describing-capacity","label":"Describing Capacity","html_id":"describing-capacity","implicit":true,"key":"hqZcU9yK9G"},{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"strong","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Capacity","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"UdfG4V08EB"}],"key":"RO8nHP9QvF"},{"type":"text","value":" of a model informally refers to the ability of the model to fit a wide range of possible functions.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"km2Ibt3tTV"}],"key":"pLVUZG1WiM"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":12,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Models with high capacity tend to overfit.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"TFGWsQXmIg"}],"key":"mbPYOGiOYd"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Models with low capacity tend to underfit.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"iEtXWDxG4G"}],"key":"OJpsxFpnio"}],"key":"VlLnIbgaJ5"},{"type":"paragraph","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"strong","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"Representational Capacity","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"aNByyEYCZ5"}],"key":"g0Hds3yHCg"},{"type":"text","value":" of a model refers to the extent of functions this model can approximate well in theory. Deep neural networks have very high representational capacity. In fact, they’re universal approximators.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"ry9AP0a1EU"}],"key":"nnO1vSuMPo"},{"type":"paragraph","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"strong","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"Effective Capacity","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"dLMr77nBzI"}],"key":"ubsqVyNu5F"},{"type":"text","value":" of a model refers to the extent of functions this model can approximate well in practice given a specific learning algorithm and dataset.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"XyqlutUR66"}],"key":"zV8Zpwt3NI"},{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"For convex optimization problem, a models’ effective capacity is just the representational capacity since local minimum is a global minimum. But for nonconvex problems, a model’s effective capacity also depends on whether the dataset is representative or whether the learning algorithm is powerful. These two factors can have great effect on effective capacity, though have zero effect on representational capacity.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"NeCjVou3Pw"}],"key":"UpzAXQqXJR"},{"type":"heading","depth":2,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"Early Stopping","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"OU6Lfienal"}],"identifier":"early-stopping","label":"Early Stopping","html_id":"early-stopping","implicit":true,"key":"z52772gvR3"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"We stop the training when validation loss starts to get bigger.","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"bVFOjohJyY"}],"key":"KHD2FPUtA9"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":25,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":25,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"inlineCode","value":"patience = K","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"duLrRLc7v7"},{"type":"text","value":" - terminate training when validation loss has no improvement in the past K epochs","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"uN7SEyMX23"}],"key":"XprK5yQJ2W"}],"key":"aLpL0Texjj"},{"type":"listItem","spread":true,"position":{"start":{"line":27,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"inlineCode","value":"min_delta=0.01","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"UaWHVgDCQK"},{"type":"text","value":" - only an improvement greater than 0.01 is considered as an improvement. Anything lower than that is considered as no improvement.","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"FaBLLHhZUM"}],"key":"Fi0WCCSmGP"}],"key":"USpFxE23aU"}],"key":"b4Q6lbuv06"},{"type":"heading","depth":2,"position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"Dropout","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"bNyUlz0B2e"}],"identifier":"dropout","label":"Dropout","html_id":"dropout","implicit":true,"key":"SUpUvZlJhl"},{"type":"paragraph","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"During training, randomly drop some neurons (force their activation value to 0) so they do not count as computation. For these neurons, we use 0 to back propagate their values.","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"MxmYSVovaR"}],"key":"gFWSNXW2xv"},{"type":"paragraph","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"In this way, we force all neurons to participate in training: imagine we have two neurons A and B. B always outputs a constant and it’s A always changing to approximate all the values. Now we force A to output 0, B can no longer remain constant and it has to also participate in learning.","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"sCqSanB6zL"}],"key":"W5mfP0O0Gh"},{"type":"heading","depth":2,"position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"Batch Normalization","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"Qc4PASCVll"}],"identifier":"batch-normalization","label":"Batch Normalization","html_id":"batch-normalization","implicit":true,"key":"gtNdt5awE9"},{"type":"paragraph","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"To avoid great variance in the output of each neuron, we restrain the output of one neuron on a minibatch to have 0 mean and unit standard deviation.","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"Cx0N55wSDG"}],"key":"jVchqqR23a"},{"type":"paragraph","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"text","value":"Think of one specific neuron, let ","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"XleoIaTbwl"},{"type":"inlineMath","value":"u = [u_1, u_2, \\dots, u_B] \\in \\mathbb R^B","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>u</mi><mo>=</mo><mo stretchy=\"false\">[</mo><msub><mi>u</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>u</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mo>…</mo><mo separator=\"true\">,</mo><msub><mi>u</mi><mi>B</mi></msub><mo stretchy=\"false\">]</mo><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mi>B</mi></msup></mrow><annotation encoding=\"application/x-tex\">u = [u_1, u_2, \\dots, u_B] \\in \\mathbb R^B</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">u</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathnormal\">u</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">u</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\">…</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">u</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05017em;\">B</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">]</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413em;\"></span><span class=\"mord\"><span class=\"mord mathbb\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05017em;\">B</span></span></span></span></span></span></span></span></span></span></span>","key":"B66oxgz4ho"},{"type":"text","value":" be its output for all samples in this batch. Let","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"Cq9hDjWlj6"}],"key":"IEeYoBcXSB"},{"type":"math","value":"BN(u) = \\frac {u-\\mu} {\\sigma}","position":{"start":{"line":41,"column":1},"end":{"line":43,"column":1}},"html":"<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>B</mi><mi>N</mi><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mi>u</mi><mo>−</mo><mi>μ</mi></mrow><mi>σ</mi></mfrac></mrow><annotation encoding=\"application/x-tex\">BN(u) = \\frac {u-\\mu} {\\sigma}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">BN</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">u</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.9463em;vertical-align:-0.686em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.2603em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">u</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord mathnormal\">μ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>","enumerator":"1","key":"BsbgDAGYHC"},{"type":"paragraph","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"children":[{"type":"text","value":"where ","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"key":"yMWIwpZEZa"},{"type":"inlineMath","value":"\\mu","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">μ</span></span></span></span>","key":"CCu6EPwQiN"},{"type":"text","value":" and ","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"key":"bs1Sc4c5H6"},{"type":"inlineMath","value":"\\sigma","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span></span></span></span>","key":"s92aOUcfi2"},{"type":"text","value":" are the mean and standard deviation of ","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"key":"Ln5g7NtiKR"},{"type":"inlineMath","value":"u = [u_1, u_2, \\dots, u_B]","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>u</mi><mo>=</mo><mo stretchy=\"false\">[</mo><msub><mi>u</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>u</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mo>…</mo><mo separator=\"true\">,</mo><msub><mi>u</mi><mi>B</mi></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">u = [u_1, u_2, \\dots, u_B]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">u</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathnormal\">u</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">u</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\">…</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">u</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05017em;\">B</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">]</span></span></span></span>","key":"AfytkuzSEt"},{"type":"text","value":". After BN layer, it has 0 mean and unit sd.","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"key":"W9flbFwzNv"}],"key":"va61RDLMXy"},{"type":"paragraph","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"This is how we calculate BN during training. In training, we also keep an ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"sMEfXC1g0z"},{"type":"strong","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"exponential running average","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"rtHrO3xP96"}],"key":"Jr6GMKwJuA"},{"type":"text","value":" of ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"pZFT3YcFGL"},{"type":"inlineMath","value":"\\mu","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">μ</span></span></span></span>","key":"g8b29pJhFx"},{"type":"text","value":" and ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"fvfO1MZhLZ"},{"type":"inlineMath","value":"\\sigma^2","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\sigma^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span>","key":"OfApUKSVSX"},{"type":"text","value":", so ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"bXLM6mcucO"},{"type":"strong","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"during evaluation","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"IgNx5ILD2P"}],"key":"iFlVIVnuet"},{"type":"text","value":", we use these averages as the batch statistics. Batch Norm is usually ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"X3sPUBMILf"},{"type":"strong","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"applied before activation","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"y7qVezXqZX"}],"key":"NtBx1I05RE"},{"type":"text","value":".","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"G1hHIBJ6Jf"}],"key":"bqAVbPxNu8"},{"type":"paragraph","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"children":[{"type":"text","value":"There are two problems with Batch Norm:","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"VgghhNx9jg"}],"key":"CMwpYN03uQ"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":51,"column":1},"end":{"line":62,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":51,"column":1},"end":{"line":60,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"children":[{"type":"text","value":"We effectively reduced the representative capacity of our model by constraining mean to only be 0 and sd to only be 1. Note previously, each neuron can have different mean and sd, but now they all have the same.","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"key":"imgWTVUOeb"}],"key":"G5JXuStb3l"},{"type":"paragraph","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"children":[{"type":"text","value":"We try to solve this problem by adding an affine term to assign a new non-zero mean ","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"key":"tMDy0g2R6d"},{"type":"inlineMath","value":"\\beta","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span>","key":"t7W4KjUsB4"},{"type":"text","value":" and a new non-unit sd ","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"key":"PefTVk7L8C"},{"type":"inlineMath","value":"\\gamma","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span></span></span></span>","key":"bOgTxexEd4"},{"type":"text","value":" to the activation value on this batch from this neuron. ","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"key":"RIxcFoLfY3"},{"type":"inlineMath","value":"\\gamma, \\beta","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi><mo separator=\"true\">,</mo><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma, \\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span>","key":"JAn24UeMfw"},{"type":"text","value":" are two learnable terms here. (Note ","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"key":"TnzN4EI6de"},{"type":"inlineMath","value":"\\mu, \\sigma, \\gamma, \\beta","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi><mo separator=\"true\">,</mo><mi>σ</mi><mo separator=\"true\">,</mo><mi>γ</mi><mo separator=\"true\">,</mo><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\mu, \\sigma, \\gamma, \\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">μ</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span>","key":"mYPMwcFuMG"},{"type":"text","value":" are all scalars here)","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"key":"HDKCRkp0jS"}],"key":"bReLhrHH5e"},{"type":"math","value":"BN(u) = \\frac {u-\\mu} {\\sigma} \\gamma + \\beta","position":{"start":{"line":55,"column":1},"end":{"line":57,"column":1}},"html":"<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>B</mi><mi>N</mi><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mi>u</mi><mo>−</mo><mi>μ</mi></mrow><mi>σ</mi></mfrac><mi>γ</mi><mo>+</mo><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">BN(u) = \\frac {u-\\mu} {\\sigma} \\gamma + \\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">BN</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">u</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.9463em;vertical-align:-0.686em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.2603em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">u</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord mathnormal\">μ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span></span>","enumerator":"2","key":"JkWuhIFJyz"},{"type":"paragraph","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"children":[{"type":"text","value":"However, doesn’t this defeat the purpose of normalizing in the first place? This considered, the above equation is still the common practice though we don’t know why it works","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"key":"yxTFsI4YUe"}],"key":"a6rMlVAxO7"}],"key":"OsITQRq48e"},{"type":"listItem","spread":true,"position":{"start":{"line":61,"column":1},"end":{"line":62,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"children":[{"type":"text","value":"All previous proofs depend on the fact that calculation of gradient step on each sample in minibatch is independent. However, by adding the mean ","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"key":"utSHL5VE7C"},{"type":"inlineMath","value":"\\mu","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">μ</span></span></span></span>","key":"Dc66UCVxzC"},{"type":"text","value":" and sd ","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"key":"yVck8jK011"},{"type":"inlineMath","value":"\\sigma","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span></span></span></span>","key":"dZEXyS0C7k"},{"type":"text","value":", we make the update step dependent of each other, since mean and sd depend on all samples. This nullifies all the proofs we did previously ensuring good properties of SGD. Despite this, it performs good and people use it.","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"key":"CS1oDDLXao"}],"key":"UbyruhPK4U"}],"key":"yovfU3soMg"}],"key":"KQd2anRH4c"},{"type":"heading","depth":2,"position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"children":[{"type":"text","value":"Residual Block / Skip Connection","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"key":"ZNCc8sQCFL"}],"identifier":"residual-block-skip-connection","label":"Residual Block / Skip Connection","html_id":"residual-block-skip-connection","implicit":true,"key":"sm6vRZmKgi"},{"type":"code","lang":"","value":"L0 ---- L1 ---- sigma1 ---- L2 ---- sigma2 --+--\n     |                                       |\n     -----------------------------------------","position":{"start":{"line":65,"column":1},"end":{"line":69,"column":1}},"key":"jHedbS7tgv"},{"type":"paragraph","position":{"start":{"line":71,"column":1},"end":{"line":71,"column":1}},"children":[{"type":"text","value":"The L here are some computation layers and sigma is the non-linear activation layer.","position":{"start":{"line":71,"column":1},"end":{"line":71,"column":1}},"key":"dytxQgIlnh"}],"key":"DuGE69TYNP"},{"type":"paragraph","position":{"start":{"line":73,"column":1},"end":{"line":73,"column":1}},"children":[{"type":"text","value":"So why do we add this skip connection? Imagine L2 does something weird that prevent learning (maybe all its weights go to 0, so gradient of this point to earlier layers all go to 0 when we compute it in back propagation). By adding L0 value, we ensure that even if L2 doesn’t work, the whole network can still work (for this particular example, we can then make L0 gradient non-zero even what’s in between all have gradient 0)","position":{"start":{"line":73,"column":1},"end":{"line":73,"column":1}},"key":"UcRrsLzYJU"}],"key":"F1yWcy5edm"},{"type":"paragraph","position":{"start":{"line":75,"column":1},"end":{"line":75,"column":1}},"children":[{"type":"text","value":"The classic ResNet employs this architecture:","position":{"start":{"line":75,"column":1},"end":{"line":75,"column":1}},"key":"gXBb5h6JU0"}],"key":"xtmXLC8SlD"},{"type":"code","lang":"","value":"L0 ---- conv1 ---- BN1 ---- sigma1 ---- conv2 ---- BN2 --+-- sigma2\n     |                                                   |\n     -----------------------------------------------------","position":{"start":{"line":77,"column":1},"end":{"line":81,"column":1}},"key":"BKVS1asoRO"},{"type":"code","lang":"python","value":"# https://github.com/akamaster/pytorch_resnet_cifar10/blob/master/resnet.py/ class BasicBlock\ndef forward(self, x):\n    out = F.relu(self.bn1(self.conv1(x)))\n    out = self.bn2(self.conv2(out))\n    out += self.shortcut(x)\n    out = F.relu(out)\n    return out","position":{"start":{"line":83,"column":1},"end":{"line":91,"column":1}},"key":"eIGe1jOC7W"},{"type":"paragraph","position":{"start":{"line":93,"column":1},"end":{"line":93,"column":1}},"children":[{"type":"text","value":"People don’t usually relate this to RNN because an RNN takes in sequence of inputs and each token of it is passed into the same weight / model.","position":{"start":{"line":93,"column":1},"end":{"line":93,"column":1}},"key":"afpM7CeTSP"}],"key":"cI6aacsc2S"}],"key":"h3hOccvHhE"}],"key":"nWjR3cIFn8"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Neural Network Review","url":"/2022-10-12-neural-network-review","group":"CS4787 Principles of Large-Scale Machine Learning"},"next":{"title":"Beyond supervised learning","url":"/2022-10-19-beyond-supervised-learning","group":"CS4787 Principles of Large-Scale Machine Learning"}}},"domain":"http://localhost:3000"}