{"version":"1","records":[{"hierarchy":{"lvl1":"CS2024 C++ Programming"},"type":"lvl1","url":"/2020-09-07-cs2024-c-programming","position":0},{"hierarchy":{"lvl1":"CS2024 C++ Programming"},"content":"The goal of CS2024 is to teach as much of the C++ language as possible with an eye towards your being able to use it effectively in future classes that may depend on it and/or in a professional setting. C++ is ever changing with new standards released every three years. We look to strike a balance between making sure you thoroughly understand “historic” C++ as well as introducing you to new features enabled in the language in the past decade.more","type":"content","url":"/2020-09-07-cs2024-c-programming","position":1},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec01 Introduction"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec01-introduction","position":2},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec01 Introduction"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec01-introduction","position":3},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Explaining our First Program","lvl2":"Lec01 Introduction"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#explaining-our-first-program","position":4},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Explaining our First Program","lvl2":"Lec01 Introduction"},"content":"#include <iostream>\n\nTells the compiler that we would like to load definitions from a header file named “iostream”.\n\nThe # (pound sign) indicates this is a preprocessor directive, it gets dealt with BEFORE your code is compiled\n\nstd::cout << “Hello World!” << std::endl;\n\n<< is an operator that directs content from the right to the left. In this case, we direct the string “Hello World” to std::cout, which is the console","type":"content","url":"/2020-09-07-cs2024-c-programming#explaining-our-first-program","position":5},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Compiling C++","lvl2":"Lec01 Introduction"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#compiling-c","position":6},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Compiling C++","lvl2":"Lec01 Introduction"},"content":"Windows: use Visual Studio\n\nLinux: g++ -std=c++11 -lstdc++ -o demo1 demo1.cpp: -o specifies the name of the compiled file\n\nCompiler takes the text of the source code and converts it into a binary object so that it can execute it a bit more efficiently.","type":"content","url":"/2020-09-07-cs2024-c-programming#compiling-c","position":7},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec02 Input/Output and Operators"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec02-input-output-and-operators","position":8},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec02 Input/Output and Operators"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec02-input-output-and-operators","position":9},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Input and Output","lvl2":"Lec02 Input/Output and Operators"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#input-and-output","position":10},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Input and Output","lvl2":"Lec02 Input/Output and Operators"},"content":">> stream extraction operator\n\nstd::cin >> k take a value from cin, which is the input stream keyboard, and assign it to k \n\ngetline(cin,str): cin uses space as delimiter so it won’t read in a whole line. Use this to read a full line","type":"content","url":"/2020-09-07-cs2024-c-programming#input-and-output","position":11},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Using","lvl2":"Lec02 Input/Output and Operators"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#using","position":12},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Using","lvl2":"Lec02 Input/Output and Operators"},"content":"using is similar to import in java, so that you don’t have to use the full name of a function when calling it.using std::cout;\nusing std::endl;\nint main(int argc, char *argv[]) {\n    // No longer need to use the std:: prefix\n    cout << “Hello World” << endl;\n}\n","type":"content","url":"/2020-09-07-cs2024-c-programming#using","position":13},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec03 Introduction to Classes"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec03-introduction-to-classes","position":14},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec03 Introduction to Classes"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec03-introduction-to-classes","position":15},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Struct","lvl2":"Lec03 Introduction to Classes"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#struct","position":16},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Struct","lvl2":"Lec03 Introduction to Classes"},"content":"C-Style structure definition: (Define a structure called Course, which has three fields )typedef struct {\n    string name;\n    string instructor;\n    int numStudents;\n} Course;","type":"content","url":"/2020-09-07-cs2024-c-programming#struct","position":17},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Classes","lvl2":"Lec03 Introduction to Classes"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#classes","position":18},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Classes","lvl2":"Lec03 Introduction to Classes"},"content":"Variables defined inside that class are called member variables.\n\nFunctions defined inside the class are called member functions","type":"content","url":"/2020-09-07-cs2024-c-programming#classes","position":19},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl4":"Public vs. Private","lvl3":"Classes","lvl2":"Lec03 Introduction to Classes"},"type":"lvl4","url":"/2020-09-07-cs2024-c-programming#public-vs-private","position":20},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl4":"Public vs. Private","lvl3":"Classes","lvl2":"Lec03 Introduction to Classes"},"content":"public and private keywords can appear as many times as you want in the class definition.class Course {\npublic:   // These can be seen outside the class\n  // Define member functions\n  int getStudentCount() {  return numStudents; }\n\nprivate:  // These can be seen inside the class only\n  // Define member variables\n  string name;\n  string instructor;\n  int numStudents;\n}","type":"content","url":"/2020-09-07-cs2024-c-programming#public-vs-private","position":21},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl4":"Declaration and Definition of Member Functions","lvl3":"Classes","lvl2":"Lec03 Introduction to Classes"},"type":"lvl4","url":"/2020-09-07-cs2024-c-programming#declaration-and-definition-of-member-functions","position":22},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl4":"Declaration and Definition of Member Functions","lvl3":"Classes","lvl2":"Lec03 Introduction to Classes"},"content":"You don’t have to define the functions where they are declared. Instead, you can define them outside of the class declaration. When you define them outside of the class declaration, you can still access the member variables inside that class. That’s because you are telling the compiler that this is a member function.class Course {\npublic:   // These can be seen outside the class\n  // Define member functions\n  int getStudentCount();\n  void setStudentCount(int count);\n    \nprivate:\n    ...\n}\n\nstring Course::getCourseName()\n{return name;}\n\nint Course::getStudentCount()\n{return numStudents;}\n\n\nYou usually want to define your getter and setter functions inside class definition.\n\nWhen other functions you are trying to define are too big, we usually define them outside the class definition and usually in a separate file. So we declare the functions in header file **.h and define them in another file **.cpp/* <Courses.h> */\nclass Course {\nprivate:\n    void complexLogic();\n}\n\n/* <Courses.cpp> */\n#incldue \"Courses.h\"\nvoid Courses::complexLogic(){\n    ...\n};\n","type":"content","url":"/2020-09-07-cs2024-c-programming#declaration-and-definition-of-member-functions","position":23},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl4":"Constructors","lvl3":"Classes","lvl2":"Lec03 Introduction to Classes"},"type":"lvl4","url":"/2020-09-07-cs2024-c-programming#constructors","position":24},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl4":"Constructors","lvl3":"Classes","lvl2":"Lec03 Introduction to Classes"},"content":"Constructors have to have the same name as the class. Constructors have no return type. You can define Constructors outside of class definition too.\n\nConstructors are called when you declare an instance of that type: MyClass instance. Note defining a pointer of that class without allocating memory to that pointer MyClass *p will not call the constructor, but declaring a pointer and allocating memory will call the constructor, because that’s the real time an instance is created MyClass *p = new MyClass().","type":"content","url":"/2020-09-07-cs2024-c-programming#constructors","position":25},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec5 Functions I"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec5-functions-i","position":26},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec5 Functions I"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec5-functions-i","position":27},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Enum","lvl2":"Lec5 Functions I"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#enum","position":28},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Enum","lvl2":"Lec5 Functions I"},"content":"If you don’t assign values to the ones following the first, they will all have value of previous increment 1.// Define error codes\nenum RonsError {\n  cNoError = 0,    // Values are optional, default is 0\n  cBadArg,         // If a value is not present,\n  cBadResult,      // assign previous value + 1\n  cUnknownErr\n};\n\n\nIn C++11, we can use the class keyword to define sort of a “namespace” for the enum.enum class Months {\n  JAN = 1, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV, DEC\n}\nif ((month == Months::DEC) || (month < Months::MAR))\n\t...\nMonths get_march(){\n    return Months::MAR;\n}","type":"content","url":"/2020-09-07-cs2024-c-programming#enum","position":29},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Function Declaration and Definition Revisited","lvl2":"Lec5 Functions I"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#function-declaration-and-definition-revisited","position":30},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Function Declaration and Definition Revisited","lvl2":"Lec5 Functions I"},"content":"// mymath.h -- header file for math functions\nlong squareIt(long);\n\n// mymath.cpp -- implementation of math functions\nlong squareIt(long x)\n{  return x * x;}\n\n// main.cpp\n#include “mymath.h”\nvoid main()\n{  cout << “5 squared is “ << squareIt(5) << endl;}\n\n\nYou should never include a “.c++” file in another c++ file.","type":"content","url":"/2020-09-07-cs2024-c-programming#function-declaration-and-definition-revisited","position":31},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec6 Function II"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec6-function-ii","position":32},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec6 Function II"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec6-function-ii","position":33},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Inline Functions","lvl2":"Lec6 Function II"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#inline-functions","position":34},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Inline Functions","lvl2":"Lec6 Function II"},"content":"inline int performAddition(int x,int y) \n{\n  return x+y;\n}\n\n\nWherever this function is called the compiler has the option of replacing the call with the body of the actual function, instead of creating a memory stack for that function call and etc.\n\nThe compiler may not do that when it’s a recursive call or that function is really long.","type":"content","url":"/2020-09-07-cs2024-c-programming#inline-functions","position":35},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Pass By Reference","lvl2":"Lec6 Function II"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#pass-by-reference","position":36},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Pass By Reference","lvl2":"Lec6 Function II"},"content":"Why and when do you want to use pass by reference?\n\nYou need to return multiple values. C++ only allows you to return one value. So you send those values as pass by reference parameters\n\nYou are passing a large structure/class. When passing values, the compiler will make a copy of those structure/class and pass them, which takes up a lot of stack space.\n\nIn the second case, maybe you don’t want to change anything in the structure, but passing by reference makes such a mistake likely to happen. To fix this, you can declare this passed by value as const, so when you accidentally modify it, you will get a compile-time error.bool isBusy(const BIGDataType &arg1)\n{\n  if (arg1.busyField = 0)\n    return true;\n  return false;}","type":"content","url":"/2020-09-07-cs2024-c-programming#pass-by-reference","position":37},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Default Argument","lvl2":"Lec6 Function II"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#default-argument","position":38},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Default Argument","lvl2":"Lec6 Function II"},"content":"When we declare a function, we can set a default value to its argument. (Don’t set a default value in function definition)class Counter {\n  …\n  void increment(int incrementBy=1);\n  … };\nvoid Counter::increment(int incrementBy);\n{  mycount += incrementBy;}\n\n  x.increment(); // increment x by 1\n  y.increment(2); // increment y by 2\n","type":"content","url":"/2020-09-07-cs2024-c-programming#default-argument","position":39},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Unary Scope Operator","lvl2":"Lec6 Function II"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#unary-scope-operator","position":40},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Unary Scope Operator","lvl2":"Lec6 Function II"},"content":"When you have 3 variables with the same name defined in global scope, local scope, and a nested scope inside local scope and you want to access the variable in the global scope inside some scope, you can use the :: before calling this variable. There is no way for you in the nested scope to access the variable with a same name in local scope (parent scope).int x=1;    // in the global scope\nint main(int argc, char *argv[]) {\n  int x = 6;  // local variable to main()\n    \t\t  // cannot be accessed in the following nested scope\n  {  int x = 5;  // local variable in a sub-scope of main()\n\t cout << “x is : “ << ::x << endl;  // \"x is : 1\"\n  }\n}\n","type":"content","url":"/2020-09-07-cs2024-c-programming#unary-scope-operator","position":41},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec7 Function III"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec7-function-iii","position":42},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec7 Function III"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec7-function-iii","position":43},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Function Templates","lvl2":"Lec7 Function III"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#function-templates","position":44},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Function Templates","lvl2":"Lec7 Function III"},"content":"template <typename a,typename b,…> return_type function_name (formal args)\n\nAt compilation time the compiler will look at your code and generate a separate function for each type used throughout your code when calling template functions. For example, for this maximum below, when the compiler sees the call to maximum(3,5,8), it uses the function template to automatically generate an overloaded version of maximum() that takes three variables of type int as its arguments.template <class T> T maximum(T val1, T val2, T val3)\n{  T maxValue = val1;\n  if (val2 > maxValue)\n    maxValue = val2;\n  if (val3 > maxValue)\n    maxValue = val3;\n  return maxValue;\n}\n\nreturn maximum(3,5,8);","type":"content","url":"/2020-09-07-cs2024-c-programming#function-templates","position":45},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec8 Arrays and Vectors"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec8-arrays-and-vectors","position":46},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec8 Arrays and Vectors"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec8-arrays-and-vectors","position":47},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Arrays","lvl2":"Lec8 Arrays and Vectors"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#arrays","position":48},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Arrays","lvl2":"Lec8 Arrays and Vectors"},"content":"Arrays don’t have boundary checking.#include <array>\n\n// initialization\nconst int size = 5;\narray<int,size> myArray;\n\n// range based for-loop\nfor (int item : myArray)\n    cout << “Next item is: “ << item;\n\n// sorting and searching\nsort(myArray.begin(),myArray.end()); //ascending order\nbool found = binary_search(myArray.begin(),myArray.end(),2); ","type":"content","url":"/2020-09-07-cs2024-c-programming#arrays","position":49},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Vectors","lvl2":"Lec8 Arrays and Vectors"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#vectors","position":50},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Vectors","lvl2":"Lec8 Arrays and Vectors"},"content":"#include<vectors>\n\nvector<int> primeVector{2,3,5,7,11,13};  \n\nprimeVector[6] = 17; //valid syntax but can crash the program\nprimeVector.at(6) = 17; //involves boundary checking and throw an error ","type":"content","url":"/2020-09-07-cs2024-c-programming#vectors","position":51},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec9 Pointers"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec9-pointers","position":52},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec9 Pointers"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec9-pointers","position":53},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Dynamic Allocation","lvl2":"Lec9 Pointers"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#dynamic-allocation","position":54},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Dynamic Allocation","lvl2":"Lec9 Pointers"},"content":"int *iPtr;  // declares a pointer to int\niPtr = new int;  // \"new int\" gives a dynamically allocated instance of int\n\t\t\t\t //\tthen we assign this space to iPtr\n\nNote: in the above example, a memory in the heap is allocated to this pointer\n\niPtr contains one of the following:\n\nA pointer to the newly allocated data type (in this case, an int)\n\nNULL (if the pointer could not be allocated due to insufficient memory)\n\nWe should always check whether it is NULL before using a dynamically allocated pointer.\n\nWe can use delete iPtr to dispose a dynamically allocated pointer.int *iPtr; // iPtr points to some random memory\niPtr = new int; // iPtr points to some memory allocated to it in heap\n*iPtr = 5; // write 5 to the memory iPtr is allocated to\ndelete iPtr; // release the memory assigned to iPtr / iPtr now no longer points to that memory\nreturn 0;","type":"content","url":"/2020-09-07-cs2024-c-programming#dynamic-allocation","position":55},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Pointers to Already Existing Values","lvl2":"Lec9 Pointers"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#pointers-to-already-existing-values","position":56},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Pointers to Already Existing Values","lvl2":"Lec9 Pointers"},"content":"Existing values are in stack frame, so when our pointers point to something already existed, they point to something in the stack frame, but remember variables in stack frame can disappear when out of scope.int main()\n{\n  int *iPtr;\n  if (true) {\n    int p = 5;\n    iPtr = &p; }  \n  cout << “*iPtr is “ << *iPtr << endl;\n}\n\nSo the danger is you will have to know how long this stack frame will live, or you will lose track of what you are pointing to and end up pointing something totally irrelevant.","type":"content","url":"/2020-09-07-cs2024-c-programming#pointers-to-already-existing-values","position":57},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Common Confusion with *","lvl2":"Lec9 Pointers"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#common-confusion-with","position":58},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Common Confusion with *","lvl2":"Lec9 Pointers"},"content":"int *p - declaring a pointer: The star is part of the type name, and says that we want a pointer to some other type (in our example, int * is the type of p).\n\nr = *p - dereferencing a pointer (RHS): The star is the dereference operator. This assignment gives the variable r a new value, namely the value inside the box that the pointer p points to.\n\n*p = r - dereferencing a pointer (LHS): The star is the dereference operator. This assignment changes the value inside the box that p points to be a new value, namely the value of the variable r.","type":"content","url":"/2020-09-07-cs2024-c-programming#common-confusion-with","position":59},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Pointer Chaos","lvl2":"Lec9 Pointers"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#pointer-chaos","position":60},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Pointer Chaos","lvl2":"Lec9 Pointers"},"content":"int *a = 5, *b = 7;\n\n// dereference a, get the value stored in the memory a is pointing to,\n// and write a same value to the memory b is pointing to \n*b = *a;  \n\n// let b point to the same address as a is pointing to\nb = a; \n\n// release the memory allocated to a, \n// also doing that for b since they are pointing at the same thing\ndelete a; \n\n// throw \"pointer being freed is not allocated\" error\n// since we already deleted it when we did that for a\ndelete b;","type":"content","url":"/2020-09-07-cs2024-c-programming#pointer-chaos","position":61},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Pointers to User-Defined Types","lvl2":"Lec9 Pointers"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#pointers-to-user-defined-types","position":62},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Pointers to User-Defined Types","lvl2":"Lec9 Pointers"},"content":"When we want to use member member (functions/ variables), we can use one of the following:Course *aCourse = new Course;\n(*aCourse).setStudentCount(45);\naCourse->setStudentCount(45);","type":"content","url":"/2020-09-07-cs2024-c-programming#pointers-to-user-defined-types","position":63},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Passing Pointers as Arguments","lvl2":"Lec9 Pointers"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#passing-pointers-as-arguments","position":64},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Passing Pointers as Arguments","lvl2":"Lec9 Pointers"},"content":"int *a = new int;\nint x = 5;\n// store 0 in the memory location pointed at by intPtr\nvoid setToZero(int *intPtr) { *intPtr = 0; } \n\nsetToZero(a); // pass to it a pointer whose value is some address\nsetToZero(&x);  // pass the address of some variable to it","type":"content","url":"/2020-09-07-cs2024-c-programming#passing-pointers-as-arguments","position":65},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Const with Pointers","lvl2":"Lec9 Pointers"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#const-with-pointers","position":66},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Const with Pointers","lvl2":"Lec9 Pointers"},"content":"Principle of Least Privilege: Any operation you do should only be given the opptunity to happen if it absolutely needs to.\n\nFollowing this principle, we don’t want to give writing privilege to functions doing reading.\n\nThere are four possibilities between constant/non-constant pointers pointing to constant/non-constant data:// Non-Constant Pointer, Non-Constant Data\n// Free for the pointer to point to something else,\n// Free for the data it is pointing to be written as something else\nint *intPtr = new int;\n\n// Non-Constnat Pointer, Constant Data\n// We can’t modify the data pointed at by coursePtr\n// We CAN set coursePtr to a different value\nvoid printAllCourseData(const Course *coursePtr, const int size)\n{\n  \t// For this function, maybe we will direct pointer to some other course \n    // once one course's info has been printed, \n    // while we don't want to change that info \n    // because this is just a reading function\n}\n\n// Constant Pointer, Non-Consant Data\n// Pointer can only point to a specific memory\n// The data it is pointing to can be changed\nvoid setupCourse(Course *const coursePtr)\n{\n  // For this function, we only want to change information of this course passed in.\n}\n\n// Constant Pointer, Constant Data\n// We can’t modify the data pointed at by coursePtr\n// We can’t set coursePtr to a different value either\nvoid printCourseData(const Course *const coursePtr)\n{\n\t// We only want to print out the info of this course passed in and do nothing else\n}","type":"content","url":"/2020-09-07-cs2024-c-programming#const-with-pointers","position":67},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec10 Classic Arrays and Pointer Arithmetic"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec10-classic-arrays-and-pointer-arithmetic","position":68},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec10 Classic Arrays and Pointer Arithmetic"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec10-classic-arrays-and-pointer-arithmetic","position":69},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Classic Array","lvl2":"Lec10 Classic Arrays and Pointer Arithmetic"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#classic-array","position":70},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Classic Array","lvl2":"Lec10 Classic Arrays and Pointer Arithmetic"},"content":"int *j[4]; == (int *) j[4]// array of 4 pointers\nint (*p)[4]; // a pointer to an array of 4 integers \n\nArrays are somewhat pointers. For example, if we have int b[10], b always points to the first element in this array: b == &b[0]","type":"content","url":"/2020-09-07-cs2024-c-programming#classic-array","position":71},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Pointer Arithmetic","lvl2":"Lec10 Classic Arrays and Pointer Arithmetic"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#pointer-arithmetic","position":72},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Pointer Arithmetic","lvl2":"Lec10 Classic Arrays and Pointer Arithmetic"},"content":"For any array p[n] == *(p+n). In particular, *(p+n) gives the contents of we have after advancing n steps from p. In fact, we also have p[n] == n[p], because our a[m] is just a syntactic sugar for *(a+m)","type":"content","url":"/2020-09-07-cs2024-c-programming#pointer-arithmetic","position":73},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Dynamic Allocation of Arrays","lvl2":"Lec10 Classic Arrays and Pointer Arithmetic"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#dynamic-allocation-of-arrays","position":74},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Dynamic Allocation of Arrays","lvl2":"Lec10 Classic Arrays and Pointer Arithmetic"},"content":"int a1[8] = new int; // WRONG\nint *a = new int[8];   // RIGHT\ndelete [] a;  // Must use this, ”delete a” is undefined\n\nThere are more scope issues when you use arrays as pointers. For example, the following code returns  a pointer to something inside current call stack frame. It will disappear when out of the scope. Therefore, the returned pointer from function MakeArray() actually points to something undefined.int *MakeArray() {\n  int iArray[50];\n  return iArray; }\n\nThe following code behaves differently. Instead of returning a pointer to something in the call stack, it returns something in the heap, which will not disappear after the function finishes execution.int *MakeArray(int size) {\n  int *anArray = new int[size];\n  return anArray; \t \t }","type":"content","url":"/2020-09-07-cs2024-c-programming#dynamic-allocation-of-arrays","position":75},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Passing Arrays as Parameters","lvl2":"Lec10 Classic Arrays and Pointer Arithmetic"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#passing-arrays-as-parameters","position":76},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Passing Arrays as Parameters","lvl2":"Lec10 Classic Arrays and Pointer Arithmetic"},"content":"Since arrays are pointers, you can only pass the real array to a function. There is no concept of passing a copy of that array. These are standard ways of declaring a function taking in arrays as its parameters.void swap(int *A, int j, int k);\nvoid swap(int A[], int j, int k);","type":"content","url":"/2020-09-07-cs2024-c-programming#passing-arrays-as-parameters","position":77},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Memory Allocation with malloc and sizeof","lvl2":"Lec10 Classic Arrays and Pointer Arithmetic"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#memory-allocation-with-malloc-and-sizeof","position":78},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Memory Allocation with malloc and sizeof","lvl2":"Lec10 Classic Arrays and Pointer Arithmetic"},"content":"malloc is a function for dynamic memory allocation and it only takes in byte.\n\nsizeof(SomeDataType) returns the number of bytes this data type needs.\n\nSay we want to declare an array of 6 Courses in heap here.Course *courseArray = malloc(sizeof(Course) * 6); // Old C way to initialize array in heap\nCourse *courseArray = new Course[6];  // The C++ way to do it","type":"content","url":"/2020-09-07-cs2024-c-programming#memory-allocation-with-malloc-and-sizeof","position":79},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec11 Classes – A Deeper Look"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec11-classes-a-deeper-look","position":80},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec11 Classes – A Deeper Look"},"content":"clang -std=c+11 -lstdc++ -c MyString.cpp","type":"content","url":"/2020-09-07-cs2024-c-programming#lec11-classes-a-deeper-look","position":81},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Implicit Inline","lvl2":"Lec11 Classes – A Deeper Look"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#implicit-inline","position":82},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Implicit Inline","lvl2":"Lec11 Classes – A Deeper Look"},"content":"When you define a function right in the class definition, you make this function implicitly inline. Therefore, there’s no actual method/function created; the code of the method is substituted through the rest of the code wherever that method is called.","type":"content","url":"/2020-09-07-cs2024-c-programming#implicit-inline","position":83},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Multiple Constructors","lvl2":"Lec11 Classes – A Deeper Look"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#multiple-constructors","position":84},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Multiple Constructors","lvl2":"Lec11 Classes – A Deeper Look"},"content":"You can use a delegate constructors to save yourself from writing duplicate code. It will just call that constructor, if the delegate constructors take in arguments, you can just pass in those arguments there.// older c++ style\nMyString::MyString(string initValue) : MyString() {\n  if (growStorage(initValue.length())) {\n    strcpy(storagePtr, initValue.c_str())\n    stringLength = initValue.length();\n  }}\n\n// c++ 11 style\nMyString::MyString(string initValue) : MyString{} { ... }}\n\n// Another Example\nMenu::Menu(MenuItem* list[], int n, char prom, string title) : MenuItem(prom, title){\n\tfor (int i = 0; i < n; i++)\n\t\titems.push_back(list[i]);\n};","type":"content","url":"/2020-09-07-cs2024-c-programming#multiple-constructors","position":85},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Destructor","lvl2":"Lec11 Classes – A Deeper Look"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#destructor","position":86},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Destructor","lvl2":"Lec11 Classes – A Deeper Look"},"content":"The destructor is a special method (similar to constructor) that is called just before an object is destroyed. There is only one destructor per class (can’t overload). It takes no arguments. A destructor should be used to clean up any dynamically allocated resources (memory, OS objects). You call the destructor when using delete keyword.","type":"content","url":"/2020-09-07-cs2024-c-programming#destructor","position":87},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Passing and Returning Reference","lvl2":"Lec11 Classes – A Deeper Look"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#passing-and-returning-reference","position":88},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Passing and Returning Reference","lvl2":"Lec11 Classes – A Deeper Look"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#passing-and-returning-reference","position":89},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl4":"Passing Reference","lvl3":"Passing and Returning Reference","lvl2":"Lec11 Classes – A Deeper Look"},"type":"lvl4","url":"/2020-09-07-cs2024-c-programming#passing-reference","position":90},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl4":"Passing Reference","lvl3":"Passing and Returning Reference","lvl2":"Lec11 Classes – A Deeper Look"},"content":"If you modified a parameter passed by reference in a function, the change would persist in the calling function. Note that the way we call this function has not changed. We still pass in two strings instead of pointers.\n\nYou don’t have to do anything differently to specify that the string arguments are being passed “pass-by-reference” when I call the function; I only need to specify that I want to use pass-by-reference when I declare the getTimeAndTemp function.void getTimeAndTemp(string &time,string &temp){ \n  time = getTheREALTime();\n  temp = getTheREALTemp();}\n\nint main() {\n  string theTime,theTemp;\n  getTimeAndTemp(theTime,theTemp); // theTime and theTemp will be changed.\n}","type":"content","url":"/2020-09-07-cs2024-c-programming#passing-reference","position":91},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl4":"Returning Reference","lvl3":"Passing and Returning Reference","lvl2":"Lec11 Classes – A Deeper Look"},"type":"lvl4","url":"/2020-09-07-cs2024-c-programming#returning-reference","position":92},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl4":"Returning Reference","lvl3":"Passing and Returning Reference","lvl2":"Lec11 Classes – A Deeper Look"},"content":"When we add a & before the function name, the function still returns whatever type it returns, but now the function call can appear on left side of assignment operator and we can write a new value to the memory address the returned value is stored in.\n\nFor the following example, charAt still returns a char type. The only difference is that we can now directly change the returned value stored in the object by using the assignment operator.char &MyString::charAt(int index) {\n  // boundary checking is omitted for clarity\n  return storagePtr[index];\n}\n\nint main() {\n  MyString str(“Hello World!”);\n  char c =str.charAt(11); cout << c; // '!'\n  str.charAt(11) = ‘?’;  // legal because we are returning reference\n  cout << str.charAt(11); // '?'\n  cout << “str is now: “ << str.MakeString() << endl; // Hello World?\n}","type":"content","url":"/2020-09-07-cs2024-c-programming#returning-reference","position":93},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"const in class","lvl2":"Lec11 Classes – A Deeper Look"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#const-in-class","position":94},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"const in class","lvl2":"Lec11 Classes – A Deeper Look"},"content":"As a qualifier to a member variable. It means that the member variable cannot be changed\n\nAs a qualifier to a member function. It means that the member function cannot change anything in the class:string getName() const { return mName; }","type":"content","url":"/2020-09-07-cs2024-c-programming#const-in-class","position":95},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"static in class","lvl2":"Lec11 Classes – A Deeper Look"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#static-in-class","position":96},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"static in class","lvl2":"Lec11 Classes – A Deeper Look"},"content":"There is ever only one copy of that variable that is shared among all the instances of the class.\n\nThe storage for this variable must be declared in the global scope using the fully qualified name of the variable (classname::static_variable_name)\n\nThe shared copy of the variable can be accessed either as a field of any instance or using the fully qualified name of the variable// \"Person.h\"\nclass Person {\n    static int number_of_persons;\n}\n\n// \"Person.cpp\"\nint Person::number_of_person = 0; \n\n// \"main.cpp\"\ncout << Person::number_of_person; // 0\nPerson p(\"Harmony\"); // increment number_of_person by 1 in the constructor\ncout << p.number_of_person; // 1 ","type":"content","url":"/2020-09-07-cs2024-c-programming#static-in-class","position":97},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"this in class","lvl2":"Lec11 Classes – A Deeper Look"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#this-in-class","position":98},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"this in class","lvl2":"Lec11 Classes – A Deeper Look"},"content":"Its “type” is pointer to class type. So, if we have a Person class, Person has an implicitly defined member variable named this that is of type Person *\n\nAny of the member variable and functions in the class can be referenced from this","type":"content","url":"/2020-09-07-cs2024-c-programming#this-in-class","position":99},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec12 Operator Overloads"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec12-operator-overloads","position":100},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec12 Operator Overloads"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec12-operator-overloads","position":101},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Unary Operator Overloads","lvl2":"Lec12 Operator Overloads"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#unary-operator-overloads","position":102},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Unary Operator Overloads","lvl2":"Lec12 Operator Overloads"},"content":"we just have to use the operator keyword.// \"MyString.h\"\nint operator~();\nstd::string operator+();\n\n// \"MyString.cpp\"\nint MyString::operator~(){\n    return stringLength;\n}\nstring MyString::operator+(){  \n    return MakeString(); // returns a std::string from our MyString instance\n}","type":"content","url":"/2020-09-07-cs2024-c-programming#unary-operator-overloads","position":103},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Binary Operator Overloads","lvl2":"Lec12 Operator Overloads"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#binary-operator-overloads","position":104},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Binary Operator Overloads","lvl2":"Lec12 Operator Overloads"},"content":"We define most binary operator overloads globally when it doesn’t make “sense” which of the two instances of the operands should “host” the overload. (Expressions on both ends are to some extent equal to the other)\n\nWe use inline to allow us to place this in the header file without causing multiple definition errors, so we are never really “defining” it, but just replace the code whenever it is called.inline MyString operator+(const MyString &str1,\n                          const MyString &str2) \n{\n  // use the overloaded unary + sign to return a std::string\n  // then use the std::string overloaded binary + sign to concatenate two strings\n  MyString temp( (+str1) + (+str2) );   \n  return temp;\n}\n\nHere we have an instance of binary overload not done globally. It is a “binary operator” but only takes one argument.T &operator[](int i) {\n    return *(mStoragePtr + i); // equivalent to return mStoragePtr[i];\n}","type":"content","url":"/2020-09-07-cs2024-c-programming#binary-operator-overloads","position":105},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Copy Constructors","lvl2":"Lec12 Operator Overloads"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#copy-constructors","position":106},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Copy Constructors","lvl2":"Lec12 Operator Overloads"},"content":"Whenever we use the assignment operator to initialize a variable when it is declared, the compiler actually looks for a constructor that takes in a single argument that matches the type of the value you are assigning to the newly declared instance.\n\nIf we have MyString str2 = 1;, the compiler would look for a constructor for MyString that took a single integer: MyString::MyString(int arg). If you copy constructors take in some object, it must be pass-by-reference!Point::Point(Point &anotherPoint) {\n\t// ...\n}\n\nint main(){\n    Point p1(4,5);\t// will use our custom constructor\n\tPoint p2(p1);   // will use the copy constructor (just as a constructor function)\n\tPoint p3 = p1;  // will use the copy constructor\n}","type":"content","url":"/2020-09-07-cs2024-c-programming#copy-constructors","position":107},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Overloading Assignment =","lvl2":"Lec12 Operator Overloads"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#overloading-assignment","position":108},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Overloading Assignment =","lvl2":"Lec12 Operator Overloads"},"content":"Rather than initialize some variable, we want now to assign a new value to an existing variable. Rather than a global function, we will define it as a member function in our class.MyString &MyString::operator=(const MyString &sourceStr)\n{ // convert sourceStr to a std::String with our predefined unary + \n  // setValue takes a C++ string\n  // return the address of this object\n  setValue(+sourceStr); \n  return *this;\n}","type":"content","url":"/2020-09-07-cs2024-c-programming#overloading-assignment","position":109},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Overloading Stream Direction << and >>","lvl2":"Lec12 Operator Overloads"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#overloading-stream-direction-and","position":110},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Overloading Stream Direction << and >>","lvl2":"Lec12 Operator Overloads"},"content":"inline ostream& operator<<(ostream &os, MyString &str) {  \n  os << +str;\n  // we must always return the stream that was passed in. That allows \"chaining\"(cout<<a<<\"good\"<<endl;) to work\n  return os;\n}\n\ninline istream& operator>>(istream &is, MyString &str) {  \n  int allocatedSpace = str.getAllocatedSpace();\n  char *tempBuf = new char[allocatedSpace];  // allocate temp\n  is.get(tempBuf,allocatedSpace-1);  // read from instream into location of tempBuf [tempBuf] up to [tempBuf + allocatedSpace - 1]\n  string tempStr = tempBuf;          // convert tempBuf to a std::string\n  str.setValue(tempStr);             // set str of MyString class to be tempStr\n  delete [] tempBuf;                 // delete temp memory, realease space\n  return is;                         // return stream\n}","type":"content","url":"/2020-09-07-cs2024-c-programming#overloading-stream-direction-and","position":111},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec13 Inheritance"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec13-inheritance","position":112},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec13 Inheritance"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec13-inheritance","position":113},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Basic Syntax","lvl2":"Lec13 Inheritance"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#basic-syntax","position":114},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Basic Syntax","lvl2":"Lec13 Inheritance"},"content":"class DerivedClass : public BaseClass\n{ \n  <member variables unique to Derived Class>\n  ...\n};\n\nclass Student : public Person\n{ \n  int studentID;\n};","type":"content","url":"/2020-09-07-cs2024-c-programming#basic-syntax","position":115},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Override","lvl2":"Lec13 Inheritance"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#override","position":116},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Override","lvl2":"Lec13 Inheritance"},"content":"We can override a function by just reimplementing it in our derived class. To access the original implementation from the base class, we use its fully qualified name in the derived class.void Person::printInfo()\n{ \n  cout << “Name:  “ << name << endl;    \n  cout << “Addr:  “ << address << endl; \n  cout << “Phone: ” << phone << endl;   \n};\n\nvoid Student::printInfo()\n{\n  Person::printInfo();\n  cout << “Student ID: “  << studentID << endl;\n};","type":"content","url":"/2020-09-07-cs2024-c-programming#override","position":117},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Virtual Functions","lvl2":"Lec13 Inheritance"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#virtual-functions","position":118},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Virtual Functions","lvl2":"Lec13 Inheritance"},"content":"Say we overwrite the printInfo function in Person and define a global function that takes in a Person class and call the printInfo function on that class. When we pass a Student instance to it, it will actually use the printInfo function of Person instead of Student. That’s because the compiler thinks the function just takes in a Person.void printPersonInfo(Person &aPerson)\n{\n  aPerson.printInfo();\n};\n\nStudent s;\nprintPersonInfo(s); // prints out Name, Addr, Phone\n\nIf you want to use the overridden version of the function, you will have to declare the function in base class as a virtual function. By defining a virtual function, we tell the compiler to call the overridden version no matter what type that instance may be cast to. However, to achieve this effect, we should also pass in an reference or pointer of instance of our derived class. Only in this way can the compiler knows what type our object was declared as. If we just pass by value (a copy of that instance), it will create a copy of our instance with whatever type specified in the function. More specifically, it calls the copy constructor of the specified class. It has no knowledge of what the original type of the argument was.class Person\n{\n  virtual void printInfo();\n}\n\nvoid printPersonInfo(Person &aPerson) // use overriden version\nvoid printPersonInfo(Person *aPerson) // use overriden version\nvoid printPersonInfo(Person aPerson) // use function in Person; \n//in fact in the last function, aPerson only has the \"Person\" part and doesn't contain any information specific to the derived class","type":"content","url":"/2020-09-07-cs2024-c-programming#virtual-functions","position":119},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec14 Polymorphism"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec14-polymorphism","position":120},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec14 Polymorphism"},"content":"We can dynamically allocate an instance of the derived class and store it in a base class pointer variable. Since Instructor is derived from Person, this is legal.Person *aPerson = new Student(); // a pointer of base class(Person) pointing to its derived class(Student)\naPerson->printInfo(); // calls the overridden method in derived class","type":"content","url":"/2020-09-07-cs2024-c-programming#lec14-polymorphism","position":121},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Abstract Class","lvl2":"Lec14 Polymorphism"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#abstract-class","position":122},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Abstract Class","lvl2":"Lec14 Polymorphism"},"content":"We can make a function to be pure virtual (abstract) by adding a = 0 after its declaration. Any new class derived from this class must implement pure virtual methods if the class is going to work. A class with pure virtual functions is an abstract class.","type":"content","url":"/2020-09-07-cs2024-c-programming#abstract-class","position":123},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Virtual Destructors","lvl2":"Lec14 Polymorphism"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#virtual-destructors","position":124},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Virtual Destructors","lvl2":"Lec14 Polymorphism"},"content":"If you have an abstract class, you would need to have an abstract/virtual destructor. That is because when a derived class’s destructor is called, it will (implicitly) call destructors in all base classes it inherits from as well.// Person.h\nvirtual ~Person() {cout<<\"base class destructor called\"<<endl;}\n\n// Student.h\n~Students() {cout<<\"derived class Studenet destructor called\"<<endl;}","type":"content","url":"/2020-09-07-cs2024-c-programming#virtual-destructors","position":125},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec15 Stream"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec15-stream","position":126},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec15 Stream"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec15-stream","position":127},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl4":"Simple Stream I/O","lvl2":"Lec15 Stream"},"type":"lvl4","url":"/2020-09-07-cs2024-c-programming#simple-stream-i-o","position":128},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl4":"Simple Stream I/O","lvl2":"Lec15 Stream"},"content":"put/get: For any stream, the simplest I/O routines let you input or output one character at a time.\n\nEnd of File eof:  a special character (usually has value -1) that signals you’ve reached an end of file state. When we reach eof, we cannot read any further from the file. (Ctrl+Z on Windows, Ctrl+D on other OS)\n\ngetline: pass in a whole line of characters ( read in until encountering with a \\n)\n\nWhen you type in “This” while running the following code without hitting Enter, it will not print anything, because all characters you typed in have not been sent into the buffer yet. After you hit Enter, “This” will be echoed back. So everything got sent into the buffer, we get one out of it each time, and put it to the outstream, repeat the process until we encounter an eof (Ctrl+Z).while (!cin.eof()) {\n    char c = cin.get();\n    cout.put(c);\n}","type":"content","url":"/2020-09-07-cs2024-c-programming#simple-stream-i-o","position":129},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Error Handling","lvl2":"Lec15 Stream"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#error-handling","position":130},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Error Handling","lvl2":"Lec15 Stream"},"content":"Once an cin attempt failed, an error flag is set and future attempts to get input will fail. Failure happens when type entered doesn’t match the type of the variable you are assigning value to.\n\ncin.fail(): returns true if the last cin assignment failed.\n\ncin.clear(): repairs the stream by clearing the error flag in cin.\n\ncin.ignore(n, c): ignores the following n characters or one c character. Therefore, cin.ignore(100,'\\n') ignore all input until you’ve already ignored 100 of them or ignore 1 ‘\\n’ character.cin >> id;\nwhile (cin.fail() || id<0 || id>99) {\n    cin.clear(); cin.ignore(99, '\\n');\n    cout << \"invalid number, try again > \";\n    cin >> id;\n}","type":"content","url":"/2020-09-07-cs2024-c-programming#error-handling","position":131},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Int Stream Manipulator","lvl2":"Lec15 Stream"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#int-stream-manipulator","position":132},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Int Stream Manipulator","lvl2":"Lec15 Stream"},"content":"dec: decimal, base 10\n\noct: octal, base 8\n\nhex: hexadecimal, base 16\n\nsetbase(n): set to n base\n\nThese stream manipulators are “sticky”. They will remain the format of your output (even though you start another sentence of cout), until you set another stream manipulator.cout << oct << 8; //10\ncout << 9; // 11\ncout << setbase(10) << 16; // 16","type":"content","url":"/2020-09-07-cs2024-c-programming#int-stream-manipulator","position":133},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Float Stream Manipulator","lvl2":"Lec15 Stream"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#float-stream-manipulator","position":134},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Float Stream Manipulator","lvl2":"Lec15 Stream"},"content":"fixed: print out float number in decimal/fixed point notation\n\nscientific: print out float number in scientific notation\n\nsetprecision(n): always print out 3 digits after the decimal point\n\nThey are all “sticky”. You’ll have to manually set it back to previous state.int curPrecision = cout.precision();  // current setting\ncout << setprecision(2) << 3.12545 << endl; // 3.13\ncout.precision(curPrecision); // Restore original setting","type":"content","url":"/2020-09-07-cs2024-c-programming#float-stream-manipulator","position":135},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Fixed Width","lvl2":"Lec15 Stream"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#fixed-width","position":136},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Fixed Width","lvl2":"Lec15 Stream"},"content":"left: align to left, the output is padded to the field width appending fill characters at the end\n\nright: align to right, the output is padded to the field width by inserting fill characters at the beginning\n\nThese two stream manipulators are sticky.\n\nWe also use setw(n) to make sure at least n characters are printed. If the string to print has fewer than n characters, fill with space. If it has more than n characters, print everything. setw(n) is not sticky. That’s because most output methods automatically calls setw(0) each time you call them. setw(n) is in the library #include <iomanip> .#include <iomanip>\nint n = -77, m = 13579;\ncout << setw(6) << left << n << endl;\ncout << setw(6) << right << n << endl;\ncout << setw(2) << m << endl;\n\n//-77   \n//   -77\n//13579","type":"content","url":"/2020-09-07-cs2024-c-programming#fixed-width","position":137},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Custom Manipulator","lvl2":"Lec15 Stream"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#custom-manipulator","position":138},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Custom Manipulator","lvl2":"Lec15 Stream"},"content":"Manipulators are just globally defined functions that take an ostream reference and return an ostream reference. Following are some examples:ostream& beep(ostream &output)\n{  return output << “\\a”;} // displaying \\a causes beep\n\nostream &aReallyLongTokenForNewline(ostream &output)\n{  return output << “\\n”;}\n\ncout << “This will cause a beep: “ << aReallyLongTokenForNewLine;\ncout << beep;","type":"content","url":"/2020-09-07-cs2024-c-programming#custom-manipulator","position":139},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec16 Functional Programming"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec16-functional-programming","position":140},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec16 Functional Programming"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec16-functional-programming","position":141},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"auto keyword","lvl2":"Lec16 Functional Programming"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#auto-keyword","position":142},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"auto keyword","lvl2":"Lec16 Functional Programming"},"content":"The auto keyword is used to declare a variable whose type is determined by the value it is initialized to. It must be initialized at the moment it is declared (or it will cause a static time compiler error).auto f = 3.14   // f is made a double\nauto k;  // NO INITIALIZER – This would be a compiler error","type":"content","url":"/2020-09-07-cs2024-c-programming#auto-keyword","position":143},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Function Pointers","lvl2":"Lec16 Functional Programming"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#function-pointers","position":144},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Function Pointers","lvl2":"Lec16 Functional Programming"},"content":"When we define a function pointer, we need to define its return type and what type of arguments it takes in: return_type (* function_name) (argument_type1, argument_type2, ...). Note: All these parameters are required. We can declare a function pointer with no allocation. We can assign it to any function that matches the argument type and return type as we do to most pointers.\n\nWe can also use the C++11 style function in STL functional: std::function< return_type (argument_type1, argument_type2, ...) > function_name, but this is much heavier.int SimpleAdd(int arg1,int arg2)\n{  return arg1 + arg2; }\n\nint main(int argc, char *argv[])\n{\n  int (*f)(int start,int stop); // define a function pointer that takes in two ints and returns an int\n  f = SimpleAdd; // f now points at the function “SimpleAdd”\n  int x = (*f)(3,4);  // dereference f, get the function it points to and applies it to 3,4\n  int y = f(3,4);     // A syntactic sugar provided. Compiler will do the dereference\n  cout << \" x is: \" << x << \", y is:  << y << endl;\n      \n  function<int(int,int)> g; g = simpleAdd;\n  cout << g(3,4) << endl; // also gives 7\n  // *g(3,4) doesn't work because g here is an std::function, not a pointer to a C-style function\n}","type":"content","url":"/2020-09-07-cs2024-c-programming#function-pointers","position":145},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Function as Parameter","lvl2":"Lec16 Functional Programming"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#function-as-parameter","position":146},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Function as Parameter","lvl2":"Lec16 Functional Programming"},"content":"When we want to pass a function as a parameter of another function, we can pass it as a C-style pointer or C++11 std::funciton . We can also use a template and let the compiler to figure it out.void OldCallMe(int (*f)(int), int x) {...}\nvoid NewCallMe(std::function<int(int)> f, int x) {...}\n\ntemplate<typename T>\nvoid CallMe(T fn, int x)\n{\n  // a syntax error will result if the fn passed in of type T doesn't support the following line\n  int newValue = fn(x);\n  cout << \"CallMe-newValue is: \" << newValue << endl;\n}","type":"content","url":"/2020-09-07-cs2024-c-programming#function-as-parameter","position":147},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Lambda Expressions","lvl2":"Lec16 Functional Programming"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#lambda-expressions","position":148},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Lambda Expressions","lvl2":"Lec16 Functional Programming"},"content":"A lambda expression evaluates to a function pointer. It takes the following format: [vars](args) -> returntype {  // body of function };, where return type and arrow can be omitted.// Declare a lambda with the auto keyword (we don't know what type of a function that is)\n// func is a function that takes no variables or arguments and simply prints out Hello World\nauto func = []() { cout << \"Hello world!\" << endl; };\n\n// Declare a lambda with a function pointer:\n// func2 is a pointer to a function that takes in a string as parameter\n// More specifically, that function takes in a string and prints it out\nvoid (*func2)(string) = [](string s) {\n            cout << “Hello “ << s << endl; };\n\n// Use function template (C++11) to store lambda\nstd::function<void(string)> func3 = [](string s) {\n    cout << “Hello “ << s << endl; };","type":"content","url":"/2020-09-07-cs2024-c-programming#lambda-expressions","position":149},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Capture Local Variables","lvl2":"Lec16 Functional Programming"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#capture-local-variables","position":150},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Capture Local Variables","lvl2":"Lec16 Functional Programming"},"content":"We can use lambda expressions to capture local variables. This will be an important way to still be able to use variables in a function that no longer exists when the lambda finally gets executed. If you want to capture local variables, always use C++11 std::function when defining either the lambda expression or the function you want to take this lambda expression.void CallMe(std::function<int()> fn) {...};\n\n// template also works because it will automatically identify fn as an std::function\ntemplate<typename T>\nvoid CallMe(T fn) {...};\n\nint myX = 300;\nCallMe([myX]()->int{ return myX*2; });\n\nYou also have the option of capturing local variables by reference. That means if the lambda expression modifies them, the modifications persist back into the “hosting” function where these variables were defined. (Just like any pass by reference function call). Pass by reference or pass a pointer will do.int myX = 300; \nint *myY = new int; *myY = 3;\nCallMe([&myX](){ myX *= 2; });\nCallMe([myY](){ *myY *= 2; });\ncout << \"myX is \" << myX << endl << \"myY is \" << *myY << endl; // 600","type":"content","url":"/2020-09-07-cs2024-c-programming#capture-local-variables","position":151},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec17 Files I/O"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec17-files-i-o","position":152},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec17 Files I/O"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec17-files-i-o","position":153},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"ofstream","lvl2":"Lec17 Files I/O"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#ofstream","position":154},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"ofstream","lvl2":"Lec17 Files I/O"},"content":"We use ofstream to write to file. A constructor of ofstream takes in two arguments\n\nname of the file to open\n\nspecifies which mode to use:\n\nios::out open file for writing, overwrite existing file\n\nios::app open file for writing, append to existing file\n\nWe don’t have to close the stream after writing, ofstream has a destructor that is automatically called at the end of the program. That being said, we can still call out.close() manually.\n\nWe use out.is_open() to make sure the file is indeed successfully opened and ready to be written in. Directly evaluating the stream variable out itself as a boolean also does the check.#include<fstream>\n\nofstream out(“myFile”,ios::out);   // create an ofstream, pass in name of the file and ios::out to indicate you want to use it for output\nif (out.is_open())\t               // make sure we successfully opend the file\n    out << “Hello world!” << endl;\nout.close();","type":"content","url":"/2020-09-07-cs2024-c-programming#ofstream","position":155},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"ifstream","lvl2":"Lec17 Files I/O"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#ifstream","position":156},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"ifstream","lvl2":"Lec17 Files I/O"},"content":"We use ifstream to read from a file.ifstream in(“myFile”,ios::in);\nstring str = \"Hello\";\nif (in.is_open())\n    in >> str;","type":"content","url":"/2020-09-07-cs2024-c-programming#ifstream","position":157},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Sequential Files","lvl2":"Lec17 Files I/O"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#sequential-files","position":158},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Sequential Files","lvl2":"Lec17 Files I/O"},"content":"Suppose we have a csv file that uses comma as the delimiter and space as a record separator. If we want to change only a specific record, how are we supposed to move around in that file?\n\ntellg(): returns the offset from the beginning of the file where the next read operation will get data from.\n\ntellp(): returns the offset from the beginning of the file where the next write operation will put data to.\n\nseekg(n): sets the “get” offset to the nth character in the file\n\nseekp(n): sets the “put” offset to the nth character in the file","type":"content","url":"/2020-09-07-cs2024-c-programming#sequential-files","position":159},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Reading and Writing at the Same Time","lvl2":"Lec17 Files I/O"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#reading-and-writing-at-the-same-time","position":160},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Reading and Writing at the Same Time","lvl2":"Lec17 Files I/O"},"content":"When declare an fstream variable, we can specify using multiple “modes” at the same time by putting the or operator | between different modes. We can then use whatever function those modes givefstream file(“ages.dat”, ios::in | ios::out) //reads and write to \"ages.dat\" at the same time\nstring str; file >> str; // read works\nfile << \"That's good\"; // write also works","type":"content","url":"/2020-09-07-cs2024-c-programming#reading-and-writing-at-the-same-time","position":161},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec18 Standard Template Library"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec18-standard-template-library","position":162},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec18 Standard Template Library"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec18-standard-template-library","position":163},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Iterator","lvl2":"Lec18 Standard Template Library"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#iterator","position":164},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Iterator","lvl2":"Lec18 Standard Template Library"},"content":"begin() points to the first element in the object. end() points to one after the last element. Common operators like + < > are all overloaded for iterators. for (vector<string>::iterator p = stringVector.begin();\n      p < stringVector.end(); ++p)\n    cout << “Next Vector Element is: “ << *p << endl; ","type":"content","url":"/2020-09-07-cs2024-c-programming#iterator","position":165},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Vector","lvl2":"Lec18 Standard Template Library"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#vector","position":166},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Vector","lvl2":"Lec18 Standard Template Library"},"content":"vector<string>::iterator q = stringVector.begin();\nstringVector.erase(q+5); // erase the 6th element\nstringVector.erase(q,q+5); // erase [q, q+5), so erase 1st to the 5th element","type":"content","url":"/2020-09-07-cs2024-c-programming#vector","position":167},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Map","lvl2":"Lec18 Standard Template Library"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#map","position":168},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Map","lvl2":"Lec18 Standard Template Library"},"content":"Map is based on valuetype, which has type <key, value>. All operations come from this pair. We can use typedef to name some very complicated data type that is frequently used.typedef map<int,string>::value_type IDRecord; // IDRecord is in fact of \"pair<int,string>\" type\ntypedef map<int,string>::iterator   IDRecordIterator;\nint main()\n{\n\tmap<int,string> ids;\n\tIDRecord rec1(12345,\"Ron DiNapoli\");\n\tIDRecord rec2(34564,\"Darpan Kaplan\");\n\tids.insert(rec1); // alway insert a key-value pair \n\tids.insert(rec2);\n\t\n    cout << \"ID 34564 belongs to: \" << ids[34564] << endl; // use array-like way to access map\n    \n\tIDRecordIterator p = ids.find(12345); // find returns the address of that entry with a matched key , returns map::end() if key doesn't exist\n\tIDRecordIterator q = ++p;\n\tcout << \"Next entry of ID 12345 is: \" << (*q).second << endl;\n}","type":"content","url":"/2020-09-07-cs2024-c-programming#map","position":169},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec19 Exceptions"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec19-exceptions","position":170},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec19 Exceptions"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec19-exceptions","position":171},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Basic Syntax","lvl2":"Lec19 Exceptions"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#basic-syntax-1","position":172},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Basic Syntax","lvl2":"Lec19 Exceptions"},"content":"Exceptions can be of any type. We can do throw 3.14, throw \"Unexpected\", or throw some object.enum MathErr { noErr, divByZero, genericOverflow };\nthrow divByZero;\n\ntry {\n    ...\n} catch(MathErr e) {\n    ...\n}\n\n// Or\ncatch(...) {} // catches all kinds of Exceptions","type":"content","url":"/2020-09-07-cs2024-c-programming#basic-syntax-1","position":173},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Exception Object and Inheritance","lvl2":"Lec19 Exceptions"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#exception-object-and-inheritance","position":174},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Exception Object and Inheritance","lvl2":"Lec19 Exceptions"},"content":"As said in previous section, we can throw an object.class MyIndexError {\n  MyIndexError(int i,char *msg):badIndex(i),theMsg(msg){}\n  int getBadIndex() { return badIndex; }\n  string getMessage() { return theMsg; }\nprivate:\n  int badIndex;\n  string theMsg;\n};\nchar &MyString::operator[](int index)\n{\n  if ((index < 0) || (index >= stringLength))\n    throw MyIndexError(index,”Index out of bounds”);\n  return storagePtr[index];\n}\n\n\nclass BaseException\n{\npublic:\n  BaseException(string msg,int err=0):message(msg),\n\terrorCode(err){}\n  virtual string getMessage() \n       { return “BASE EXCEPTION: “ + message; }\n  int getErrorCode() { return errorCode; }\nprotected:\n  string message;\n  int errorCode;\n};\n","type":"content","url":"/2020-09-07-cs2024-c-programming#exception-object-and-inheritance","position":175},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec20 Custom Templates"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec20-custom-templates","position":176},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec20 Custom Templates"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec20-custom-templates","position":177},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Basic Syntax","lvl2":"Lec20 Custom Templates"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#basic-syntax-2","position":178},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Basic Syntax","lvl2":"Lec20 Custom Templates"},"content":"template <class placeholder>\t// declare placeholders\nclass SimpleClass\t\t\t// regular class definition\n{\npublic:\n…\n};\n\n// define a function outside of template class\nvoid SimpleClass<placeholder>::FunctionName() {...}\n// define constructor/destructor outside of template class\nvoid SimpleClass<placeholder>::SimpleClass()\n\n“Definition” template class should also be in the same .h file. Because the compiler needs to generate a separate set of member functions for each type used to create an instance of this class at compile time. That means that these definitions are needed at compile time and not at link time, so .cpp won’t enable us to actually call those functions.","type":"content","url":"/2020-09-07-cs2024-c-programming#basic-syntax-2","position":179},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Non-Type Parameters","lvl2":"Lec20 Custom Templates"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#non-type-parameters","position":180},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Non-Type Parameters","lvl2":"Lec20 Custom Templates"},"content":"We specified a data type calls placeholder in the template class. We can also specify a constant expression when we declare a template class. This will have the same effect as setting a const value specific for that instance, except previously we couldn’t assign values to const variable.template <class storageType,int size> class MyArray {...}\ntemplate <class storageType=int,int size=5> class MyArray {...} // give a default value","type":"content","url":"/2020-09-07-cs2024-c-programming#non-type-parameters","position":181},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec22 STL Algorithms"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec22-stl-algorithms","position":182},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec22 STL Algorithms"},"content":"#include<algorithm> for all functions below.\n\nfill(iterator begin, iterator end, T value): take two iterators/pointers and one value. Fill every position in between with that value.:char *ptr = new char[10]\t  // An array of 10 chars\nfill(ptr,ptr+9,’A’);\n\ngenerate(iterator begin, iterator end, function g): assigns every position in between the two iterators/pointers according to the generating function g.int nextVal() {  static int number = 0;\n                 return number++;}\n\nint main(int argc,char *argv[]) {\n  std::vector<int> intVector(10);\t// A vector of integers\n  std::generate(intVector.begin(),intVector.end(),nextVal);\n}\n\nfill_n(begin,count,value): fill from begin to begin+count with specified value\n\ngenerate_n(begin,count,function): fill from begin to begin+count with generated value\n\nremove(begin,end,value): remove all elements == value in range from begin to end\n\nreplace(begin,end,value,replaceWith): replace all elements == value in range from begin to end WITH replaceWith","type":"content","url":"/2020-09-07-cs2024-c-programming#lec22-stl-algorithms","position":183},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec23 Smart Pointers"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec23-smart-pointers","position":184},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec23 Smart Pointers"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec23-smart-pointers","position":185},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Shared Pointer","lvl2":"Lec23 Smart Pointers"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#shared-pointer","position":186},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Shared Pointer","lvl2":"Lec23 Smart Pointers"},"content":"You can declare multiple pointers pointing to the same thing using shared pointer and they will all be released when you release one of them, so it’s safer than the classic pointer, where the pointer will hang over there.\n\nYou can call the use_count() method to get how many shared pointers are out there pointing to this same thing.int main(int argc,char *argv[]) {\n  shared_ptr<Point> pointPtr(new Point(1,2));\n  shared_ptr<Point> pointPtr2(pointPtr);\n  cout << “x coordinate is: “ << (*pointPtr).x << endl;\n  cout << “reference count is: “ << pointPtr.use_count();\n}","type":"content","url":"/2020-09-07-cs2024-c-programming#shared-pointer","position":187},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Unique Pointer","lvl2":"Lec23 Smart Pointers"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#unique-pointer","position":188},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Unique Pointer","lvl2":"Lec23 Smart Pointers"},"content":"There is only this one pointer pointing to that thing. No other shared pointers can be created pointing to the same thing. For the same reason, use_count() is not available either.","type":"content","url":"/2020-09-07-cs2024-c-programming#unique-pointer","position":189},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec24 Namespaces and C/C++ Differences"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec24-namespaces-and-c-c-differences","position":190},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec24 Namespaces and C/C++ Differences"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#lec24-namespaces-and-c-c-differences","position":191},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Namespace","lvl2":"Lec24 Namespaces and C/C++ Differences"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#namespace","position":192},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"Namespace","lvl2":"Lec24 Namespaces and C/C++ Differences"},"content":"","type":"content","url":"/2020-09-07-cs2024-c-programming#namespace","position":193},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl4":"Declaring Namespace","lvl3":"Namespace","lvl2":"Lec24 Namespaces and C/C++ Differences"},"type":"lvl4","url":"/2020-09-07-cs2024-c-programming#declaring-namespace","position":194},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl4":"Declaring Namespace","lvl3":"Namespace","lvl2":"Lec24 Namespaces and C/C++ Differences"},"content":"We define a namespace by putting it inside a namespace declaration and its corresponding scope, just like what we do to a class. What’s different is that a single namespace may span multiple files. Therefore, we can declare/define a single namespace in multiple files.namespace CornellCS2024\t{ // Defines a namespace named CornellCS2024 \n  class MyString {\n    public:\n    ...};\n  class AnotherClass {...}\n}","type":"content","url":"/2020-09-07-cs2024-c-programming#declaring-namespace","position":195},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl4":"Using Namespace","lvl3":"Namespace","lvl2":"Lec24 Namespaces and C/C++ Differences"},"type":"lvl4","url":"/2020-09-07-cs2024-c-programming#using-namespace","position":196},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl4":"Using Namespace","lvl3":"Namespace","lvl2":"Lec24 Namespaces and C/C++ Differences"},"content":"We can use anything declared in the namespace by quoting the fully qualified nameCornellCS2024::MyString aString;\n\nWe can designate a specific class to use in the rest of the file.using CornellCS2024::MyString;\nMyString aString;\n\nWe can simply state that we want to use everything declared in this namespace. That’s what we usually do to std in small file.using namespace CornellCS2024;\nMyString aString;","type":"content","url":"/2020-09-07-cs2024-c-programming#using-namespace","position":197},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"C/C++ Difference","lvl2":"Lec24 Namespaces and C/C++ Differences"},"type":"lvl3","url":"/2020-09-07-cs2024-c-programming#c-c-difference","position":198},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl3":"C/C++ Difference","lvl2":"Lec24 Namespaces and C/C++ Differences"},"content":"only supports /* block comments */\n\nvariable declarations had to appear the beginning of a scope before any other statements were encountered\n\nonly has struct, no class\n\nno overloads, Namespaces, Declaring a counter variable in a loop, String type, Exceptions, Templates\n\ndoes not use new/delete for dynamic memory allocation/deallocation. Instead, C uses\n\nmalloc() allocates memory. It needs to be given the exact number of bytes you want to dynamically allocate\n\ncalloc() is the same as malloc() but initializes all allocated memory to 0\n\nrealloc() “grow” a dynamic allocation: basically allocates new space and copies all original memory to new space.\n\nfree() releases allocated memory","type":"content","url":"/2020-09-07-cs2024-c-programming#c-c-difference","position":199},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec99 From Assignments"},"type":"lvl2","url":"/2020-09-07-cs2024-c-programming#lec99-from-assignments","position":200},{"hierarchy":{"lvl1":"CS2024 C++ Programming","lvl2":"Lec99 From Assignments"},"content":"new keyword returns a pointer to an object. You don’t have to use new when creating a new object. \n\nReferencetest t = test(\"rrr\", 8);\ntest t(\"rrr\", 8);\ntest *t = new test(\"rrr\", 8);","type":"content","url":"/2020-09-07-cs2024-c-programming#lec99-from-assignments","position":201},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning"},"type":"lvl1","url":"/2020-10-02-info1998-intro-to-machine-learning","position":0},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning"},"content":"The goal of this course is to provide you with a high-level exposure to a wide range of Data Science techniques and Machine Learning models. From the basics of getting your Jupyter environment setup, to manipulating  and visualizing data, to building supervised and unsupervised models,  this class aims to give you the base intuition and skillset to continue  developing and working on ML projects. We hope you exit the course with  an understanding of how models and optimization techniques work, as well as have the confidence and tools to solve future problems on your own.more","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning","position":1},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl2":"Lec2 Data Manipulation"},"type":"lvl2","url":"/2020-10-02-info1998-intro-to-machine-learning#lec2-data-manipulation","position":2},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl2":"Lec2 Data Manipulation"},"content":"","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#lec2-data-manipulation","position":3},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Introduction to Pandas","lvl2":"Lec2 Data Manipulation"},"type":"lvl3","url":"/2020-10-02-info1998-intro-to-machine-learning#introduction-to-pandas","position":4},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Introduction to Pandas","lvl2":"Lec2 Data Manipulation"},"content":"Series: one dimensional array\n\nDataFrame: 2-D table\n\nFiltering DataFrames: loc\n\nCleaning-Up DataFrames: df.dropna(), df[df['Open'].notnull()] (These two methods both return a new DataFrame instead of modifying the existed one)\n\nView DataFrames: head, tail, ...\n\nSummary Statistics: mean, median, ... describe","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#introduction-to-pandas","position":5},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Dealing with missing data","lvl2":"Lec2 Data Manipulation"},"type":"lvl3","url":"/2020-10-02-info1998-intro-to-machine-learning#dealing-with-missing-data","position":6},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Dealing with missing data","lvl2":"Lec2 Data Manipulation"},"content":"Fill in some random info of our choice:#if we there is no record about which cabin he is in, we assume he is on the Top Deck\ndf['Cabin']=df['Cabin'].fillna('Top Deck') \n\nUsing summary statistics: fill missing entries with median or mean\n\nworks well with small set\n\nUse regression and clustering: will be covered later","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#dealing-with-missing-data","position":7},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl2":"Lec3 Data Visualization"},"type":"lvl2","url":"/2020-10-02-info1998-intro-to-machine-learning#lec3-data-visualization","position":8},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl2":"Lec3 Data Visualization"},"content":"","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#lec3-data-visualization","position":9},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Types of Graphs","lvl2":"Lec3 Data Visualization"},"type":"lvl3","url":"/2020-10-02-info1998-intro-to-machine-learning#types-of-graphs","position":10},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Types of Graphs","lvl2":"Lec3 Data Visualization"},"content":"Heatmap\n\nCorrelation Plots","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#types-of-graphs","position":11},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Coloring Graphs","lvl2":"Lec3 Data Visualization"},"type":"lvl3","url":"/2020-10-02-info1998-intro-to-machine-learning#coloring-graphs","position":12},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Coloring Graphs","lvl2":"Lec3 Data Visualization"},"content":"plt.scatter(Longitude, Latitude, c=Temp.values.ravel(),cmap=plt.cm.OrRd) color a scattered plot based on values of Temp with color scheme cm.OrRd. Find more color schemes from \n\nmatplotlib manual.","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#coloring-graphs","position":13},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl2":"Lec4 Linear Regression"},"type":"lvl2","url":"/2020-10-02-info1998-intro-to-machine-learning#lec4-linear-regression","position":14},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl2":"Lec4 Linear Regression"},"content":"","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#lec4-linear-regression","position":15},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Preparing Data","lvl2":"Lec4 Linear Regression"},"type":"lvl3","url":"/2020-10-02-info1998-intro-to-machine-learning#preparing-data","position":16},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Preparing Data","lvl2":"Lec4 Linear Regression"},"content":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n# X must be a table (in case there are multiple x in y = a1*x1 + a2*x2 + ... + k)\nX = data[['cost','compl_4']] \n# Y must be one column\nY = data['median_earnings'] \n\nfrom sklearn.model_selection import train_test_split\n# test is 20% of all data\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#preparing-data","position":17},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Predicting and Fitting","lvl2":"Lec4 Linear Regression"},"type":"lvl3","url":"/2020-10-02-info1998-intro-to-machine-learning#predicting-and-fitting","position":18},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Predicting and Fitting","lvl2":"Lec4 Linear Regression"},"content":"# creates Linear Regression model \nLR = LinearRegression()\n# note LR is an object by calling fit, we set all of its coefficients\nLR.fit(x_train, y_train)\n# predict() returns the predicted value\ny_predicted = LR.predict(x_test)\n# score(x,y') first computes the predicted value y based on x and our model, then compare it with y'\nscore = LR.score(x_test,y_test)","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#predicting-and-fitting","position":19},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Describing the Model","lvl2":"Lec4 Linear Regression"},"type":"lvl3","url":"/2020-10-02-info1998-intro-to-machine-learning#describing-the-model","position":20},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Describing the Model","lvl2":"Lec4 Linear Regression"},"content":"# Gives a comprehensive view of Y = a1*x1 + a2*x2 + ... + k\nLR?\n\n# coefficients of x (a1, a2, ...)\nLR.coef_\n\n# intercept k\nLR.intercept_","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#describing-the-model","position":21},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl2":"Lec5 Measuring Model’s Accuracy"},"type":"lvl2","url":"/2020-10-02-info1998-intro-to-machine-learning#lec5-measuring-models-accuracy","position":22},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl2":"Lec5 Measuring Model’s Accuracy"},"content":"When determining accuracy, usually want to compare our model to a baseline. Therefore, instead of comparing our model’s prediction to each specific y value, we compare it with the mean y value.from sklearn.metrics import mean_squared_error\ncelcius_MSE = mean_squared_error(y_test, celcius_predictions)\n\ntest_goal_mean = y_test.mean()\nbaseline = np.full((len(celcius_predictions),), test_goal_mean)\nbaseline_MSE = mean_squared_error(baseline, celcius_predictions)\n\noverfitting: too specific to the data given, doesn’t predict any other data\n\nunderfitting: no matter what data you use to train this model, it gives the same curve, so it doesn’t have prediction power either because it doesn’t show any pattern of the data.","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#lec5-measuring-models-accuracy","position":23},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl2":"Lec6 Classifiers"},"type":"lvl2","url":"/2020-10-02-info1998-intro-to-machine-learning#lec6-classifiers","position":24},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl2":"Lec6 Classifiers"},"content":"Linear regression is used to predict the value of a continuous variable. Classifiers are used to predict categorical or binary variables.","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#lec6-classifiers","position":25},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"KNN","lvl2":"Lec6 Classifiers"},"type":"lvl3","url":"/2020-10-02-info1998-intro-to-machine-learning#knn","position":26},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"KNN","lvl2":"Lec6 Classifiers"},"content":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.2)\n\nk = 10\nmodel = KNeighborsClassifier(k) # specify k nearest elements\nmodel.fit(x_train,y_train)\npredictions = model.predict(x_test)","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#knn","position":27},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl2":"Lec7 Other Supervised Learning Models"},"type":"lvl2","url":"/2020-10-02-info1998-intro-to-machine-learning#lec7-other-supervised-learning-models","position":28},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl2":"Lec7 Other Supervised Learning Models"},"content":"","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#lec7-other-supervised-learning-models","position":29},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Decision Trees","lvl2":"Lec7 Other Supervised Learning Models"},"type":"lvl3","url":"/2020-10-02-info1998-intro-to-machine-learning#decision-trees","position":30},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Decision Trees","lvl2":"Lec7 Other Supervised Learning Models"},"content":"from sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = tree.DecisionTreeClassifier(max_depth=5)\n\nHow to reduce overfitting?\n\nReduce levels of trees\n\nTrain multiple decision trees (maybe one for each training data) and take its average as final result","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#decision-trees","position":31},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Logistic Regression","lvl2":"Lec7 Other Supervised Learning Models"},"type":"lvl3","url":"/2020-10-02-info1998-intro-to-machine-learning#logistic-regression","position":32},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Logistic Regression","lvl2":"Lec7 Other Supervised Learning Models"},"content":"Value always between 0 and 1. Accept if value higher than threshold, reject if lower.","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#logistic-regression","position":33},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"K-fold Cross Validation","lvl2":"Lec7 Other Supervised Learning Models"},"type":"lvl3","url":"/2020-10-02-info1998-intro-to-machine-learning#k-fold-cross-validation","position":34},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"K-fold Cross Validation","lvl2":"Lec7 Other Supervised Learning Models"},"content":"Rather than doing test-train split only once, we do it k times: First separate our sample into k pieces and each time we take one of them as test set, the others as training set. Use from sklearn.model_selection import KFold to achieve this. Calculate a score for each of the split and take its average as the final score. This score is usually closer to real errors.from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, accuracy_score\n\nincX = inc_data[['education.num']]\nincY = inc_data['income']\n\nkf = KFold(n_splits = 5)\naccuracy = 0\nfor train_index, test_index in kf.split(incX):\n    X_train = incX.iloc[train_index]\n    Y_train = incY.iloc[train_index]\n    X_test = incX.iloc[test_index]\n    Y_test = incY.iloc[test_index]\n    \n    # best_depth 是我们前一题找到的使分最高的 depth level of decision tree\n    model = tree.DecisionTreeClassifier (max_depth = best_depth)\n    model.fit(X_train, Y_train)\n    pred_test = model.predict(X_test)\n    accuracy += accuracy_score(Y_test, pred_test)\n    \naccuracy /= 5\nprint(accuracy)","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#k-fold-cross-validation","position":35},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl2":"Lec9 Unsupervised Learning"},"type":"lvl2","url":"/2020-10-02-info1998-intro-to-machine-learning#lec9-unsupervised-learning","position":36},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl2":"Lec9 Unsupervised Learning"},"content":"Supervised Learning: The desired solution (target) is also included in the dataset\n\nUnsupervised Learning: The training data is unlabeled and algorithm tries to learn by itself","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#lec9-unsupervised-learning","position":37},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Hierarchical Clustering","lvl2":"Lec9 Unsupervised Learning"},"type":"lvl3","url":"/2020-10-02-info1998-intro-to-machine-learning#hierarchical-clustering","position":38},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"Hierarchical Clustering","lvl2":"Lec9 Unsupervised Learning"},"content":"Hierarchical clustering groups observations into multiple levels of  sets; the top-level set includes all of the data, and the bottom-level  sets contain individual observations. The levels in between contain sets of observations with similar features.from sklearn.preprocessing import StandardScaler\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom matplotlib import pyplot as plt\n\n# Standardize features by removing the mean and scaling to unit variance\ndata = StandardScaler().fit_transform(data)\n# build our model from data\nclust = linkage(data) \n# draw the dendrogram visulization\ndendrogram(clust)\nplt.show()","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#hierarchical-clustering","position":39},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"K-Means Clustering","lvl2":"Lec9 Unsupervised Learning"},"type":"lvl3","url":"/2020-10-02-info1998-intro-to-machine-learning#k-means-clustering","position":40},{"hierarchy":{"lvl1":"INFO1998 Intro to Machine Learning","lvl3":"K-Means Clustering","lvl2":"Lec9 Unsupervised Learning"},"content":"We want to cluster the data into k groups. We first randomly choose k points in this dataset. Then we assign other data points to the group they are closest to. After assigning all data points to some group, we recompute the center of each group by taking the means of all points in that group. Repeat this process until no points change group assignment after one iteration.from sklearn import cluster\nk = 3\nkmeans = cluster.KMeans(n_clusters = k) #cluster into k groups\nkmeans.fit(data)","type":"content","url":"/2020-10-02-info1998-intro-to-machine-learning#k-means-clustering","position":41},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms"},"type":"lvl1","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms","position":0},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms"},"content":"CS 4820 develops techniques used in the design and analysis of algorithms, with an emphasis on problems arising in computing applications. Example applications are drawn from systems and networks, artificial intelligence, computer vision, data mining, and computational biology. This course covers four major algorithm design techniques (greedy algorithms, divide and conquer, dynamic programming, and network flow), computability theory focusing on undecidability, computational complexity focusing on NP-completeness, and algorithmic techniques for intractable problems, including identification of structured special cases, approximation algorithms, and randomization.more","type":"content","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms","position":1},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl2":"Greedy Algorithm"},"type":"lvl2","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#greedy-algorithm","position":2},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl2":"Greedy Algorithm"},"content":"","type":"content","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#greedy-algorithm","position":3},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl3":"Greedy Stays Ahead","lvl2":"Greedy Algorithm"},"type":"lvl3","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#greedy-stays-ahead","position":4},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl3":"Greedy Stays Ahead","lvl2":"Greedy Algorithm"},"content":"Greedy is at least as good as the optimal solution in each step","type":"content","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#greedy-stays-ahead","position":5},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl3":"Exchange Argument","lvl2":"Greedy Algorithm"},"type":"lvl3","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#exchange-argument","position":6},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl3":"Exchange Argument","lvl2":"Greedy Algorithm"},"content":"Take any optimal solution, we can make it exactly the same as our greedy solution without having the optimal solution produce a worse result.\n\nThere is some “structure” unique to this problem. All solutions have this “structure” give the same number of lateness.\n\nOur greedy solution has this “structure”\n\nWe can exchange any optimal solution to have this “structure” without making this solution worse","type":"content","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#exchange-argument","position":7},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl2":"Divide and Conquer"},"type":"lvl2","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#divide-and-conquer","position":8},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl2":"Divide and Conquer"},"content":"Master theorem says that for an algorithm with running time T(n) = aT(\\frac{n}{b}) + f(n). f(n) is some polynomial of n, so we have T(n) = aT(\\frac{n}{b}) + O(n^c).\n\na = b^c: T(n) = O(n^c \\; logn) - A balance between constant work at each level and number of subproblems at each level.\n\na < b^c: T(n) = O(n^c) - Time dominated by the constant work we do at upper levels: take a=1 as an extreme example, all of the time will be spent on top level.\n\na>b^c: T(n) = O(n^{log_ba}) -  Time dominated by each subproblems we have as the recursion go deeper. A lot of branches of subproblems will be generated.","type":"content","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#divide-and-conquer","position":9},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl2":"Network Flow"},"type":"lvl2","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#network-flow","position":10},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl2":"Network Flow"},"content":"Max flow 问题转换为 Min Cut 问题，Min Cut 问题永远可以给自己不想要的边 infinite capacity 来将它排除在 min cut 之外。\n\neffectively infinite: 任何一个无法达到的数，都可以视作 infinite，比如 infinite capacity 可以是一个已知的 cut 值+1 (max flow 必然小于任意一个 cut，所以没有任何一个 flow 可以达到 cut + 1)","type":"content","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#network-flow","position":11},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl2":"NP"},"type":"lvl2","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#np","position":12},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl2":"NP"},"content":"","type":"content","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#np","position":13},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl3":"Proving Reduction","lvl2":"NP"},"type":"lvl3","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#proving-reduction","position":14},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl3":"Proving Reduction","lvl2":"NP"},"content":"Show that your reduction σ takes polynomial time.\n\nShow that x is a solution to the problem you are reducing from if and only if σ(x) is a solution to the\nproblem you are trying to show is NP-hard. You need to show the implication in both directions.","type":"content","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#proving-reduction","position":15},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl3":"Proving NP, NP-Hard, NP-Completeness","lvl2":"NP"},"type":"lvl3","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#proving-np-np-hard-np-completeness","position":16},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl3":"Proving NP, NP-Hard, NP-Completeness","lvl2":"NP"},"content":"NP: prove you can verify a solution in polynomial time\n\nNP-hard: prove some known NP-Hard (or NP-complete) problem can be reduced to A in polynomial time (注意是别的已知问题可以被转换成我们要证明的问题)\n\nNP-completeness: it is NP-hard and it is NP","type":"content","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#proving-np-np-hard-np-completeness","position":17},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl3":"Important NP-Complete Problem","lvl2":"NP"},"type":"lvl3","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#important-np-complete-problem","position":18},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl3":"Important NP-Complete Problem","lvl2":"NP"},"content":"satisfiability problems: Boolean satisfiability, CNFSAT (conjunctive normal form satisfiability) ,  3CNFSAT (aka 3SAT)\n\ngraph problems: Clique, Independent Set, Vertex Cover, Dominating Set, Colorability, Planar 3-colorability\n\ncovering problems: Set Cover, 3-dimensional matching (3DM)\n\ntour problems: directed and undirected Hamiltonian circuit (HC), Traveling Salesperson (TSP)\n\nnumerical problems: Subset Sum (SS), Partition, Knapsack, Bin Packing","type":"content","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#important-np-complete-problem","position":19},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl3":"Tips","lvl2":"NP"},"type":"lvl3","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#tips","position":20},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl3":"Tips","lvl2":"NP"},"content":"If a problem asks you to decide if there exists a set of at least k objects satisfying some property, try reducing from another problem that involves picking at least k objects, e.g. Independent Set or Clique.\n\nSimilarly, if a problem asks you to decide if there exists a set of at most k objects satisfying some property, try reducing from another problem that involves picking at most k objects, e.g. Vertex Cover or Set Cover.\n\nWhen reducing Independent Set / Vertex Cover to another graph-like problem. We find out adding a node representing edges is very useful. (Dominating Set, practicefsol 4, fakesol7 3)\n\nWhen a problem does not easily fit into either of the general categories listed above, usually the best thing to try first is 3CNFSAT.\n\nWhen do the reduction from A to B, try to reduce A to a special case of B. (hw5 P3 Clique -> Submatrix Domination, hw6 P2 Vertex Cover -> Dominating Set)","type":"content","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#tips","position":21},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl2":"Turing Machine"},"type":"lvl2","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#turing-machine","position":22},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl2":"Turing Machine"},"content":"","type":"content","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#turing-machine","position":23},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl3":"Decidability","lvl2":"Turing Machine"},"type":"lvl3","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#decidability","position":24},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl3":"Decidability","lvl2":"Turing Machine"},"content":"Give a total Turing Machine (one that always halts) to accept any “yes” instance and reject any “no” instance","type":"content","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#decidability","position":25},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl3":"Undecidability","lvl2":"Turing Machine"},"type":"lvl3","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#undecidability","position":26},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl3":"Undecidability","lvl2":"Turing Machine"},"content":"Prove by Diagonalization\n\nProve by Reduction: we usually reduce our problem to Halting Problem or the complement of it (Non-Halting Problem aka. Looping Problem). Note: σ in this case has to be computable instead of polynomial-time\n\nTo prove some problem is undecidable within a certain time bound, use clocked diagonalization.","type":"content","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#undecidability","position":27},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl2":"Crucial Facts"},"type":"lvl2","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#crucial-facts","position":28},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl2":"Crucial Facts"},"content":"","type":"content","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#crucial-facts","position":29},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl3":"Minimum Spanning Tree","lvl2":"Crucial Facts"},"type":"lvl3","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#minimum-spanning-tree","position":30},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl3":"Minimum Spanning Tree","lvl2":"Crucial Facts"},"content":"cut property: Let A and B partitions vertices V, if e is the minimum edge connecting A and B, e must be in every minimum spanning tree.\n\ncycle property: Let C be any cycle in G, e be the maximum cost edge on that cycle, e is not in any minimum spanning tree","type":"content","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#minimum-spanning-tree","position":31},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl2":"Proof Techniques"},"type":"lvl2","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#proof-techniques","position":32},{"hierarchy":{"lvl1":"CS4820 Intro to Analysis of Algorithms","lvl2":"Proof Techniques"},"content":"Loop Invariant and Recursion: 一个很好的例子是 T7.42 的证明\n\nrecursion = induction\n\nloop invariant = induction hypothesis\n\ntermination condition = basis  computation = logic\n  \t\t\t\t\t\t\t\t\t\t-- Dexter Kozen","type":"content","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms#proof-techniques","position":33},{"hierarchy":{"lvl1":"ORIE4350 Game Theory"},"type":"lvl1","url":"/2022-02-08-orie4350-game-theory","position":0},{"hierarchy":{"lvl1":"ORIE4350 Game Theory"},"content":"","type":"content","url":"/2022-02-08-orie4350-game-theory","position":1},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl2":"Rational Decision Making"},"type":"lvl2","url":"/2022-02-08-orie4350-game-theory#rational-decision-making","position":2},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl2":"Rational Decision Making"},"content":"","type":"content","url":"/2022-02-08-orie4350-game-theory#rational-decision-making","position":3},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"The Single-Person Decision Problem","lvl2":"Rational Decision Making"},"type":"lvl3","url":"/2022-02-08-orie4350-game-theory#the-single-person-decision-problem","position":4},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"The Single-Person Decision Problem","lvl2":"Rational Decision Making"},"content":"","type":"content","url":"/2022-02-08-orie4350-game-theory#the-single-person-decision-problem","position":5},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Actions, Outcomes, and Preferences","lvl3":"The Single-Person Decision Problem","lvl2":"Rational Decision Making"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#actions-outcomes-and-preferences","position":6},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Actions, Outcomes, and Preferences","lvl3":"The Single-Person Decision Problem","lvl2":"Rational Decision Making"},"content":"","type":"content","url":"/2022-02-08-orie4350-game-theory#actions-outcomes-and-preferences","position":7},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Decision Problem and Preference Relation","lvl4":"Actions, Outcomes, and Preferences","lvl3":"The Single-Person Decision Problem","lvl2":"Rational Decision Making"},"type":"lvl5","url":"/2022-02-08-orie4350-game-theory#decision-problem-and-preference-relation","position":8},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Decision Problem and Preference Relation","lvl4":"Actions, Outcomes, and Preferences","lvl3":"The Single-Person Decision Problem","lvl2":"Rational Decision Making"},"content":"A decision problem consists of three features:\n\nActions are all the alternatives from which the player can choose. A\n\nOutcomes are the possible consequences that can result from any of the actions. X\n\nPreferences describe how the player ranks the set of possible outcomes. The preference relation \\succsim describes the player’s preferences, and the notation x \\succsim y means “x is at least as good as y.” This is an equivalence relation.","type":"content","url":"/2022-02-08-orie4350-game-theory#decision-problem-and-preference-relation","position":9},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Payoff Function","lvl4":"Actions, Outcomes, and Preferences","lvl3":"The Single-Person Decision Problem","lvl2":"Rational Decision Making"},"type":"lvl5","url":"/2022-02-08-orie4350-game-theory#payoff-function","position":10},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Payoff Function","lvl4":"Actions, Outcomes, and Preferences","lvl3":"The Single-Person Decision Problem","lvl2":"Rational Decision Making"},"content":"We can quantify the preference relation by introducing the payoff function u: X \\to \\mathbb{R}. Payoff is an ordinal construct: It is only used for ranking and does not carry any practical meaning.\n\nif x(a) is the outcome resulting from action a, then the payoff from action a is given by v(a) = u(x(a)), the payoff from x(a). Function v: A \\to \\mathbb R.","type":"content","url":"/2022-02-08-orie4350-game-theory#payoff-function","position":11},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Rational Choice Paradigm","lvl3":"The Single-Person Decision Problem","lvl2":"Rational Decision Making"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#rational-choice-paradigm","position":12},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Rational Choice Paradigm","lvl3":"The Single-Person Decision Problem","lvl2":"Rational Decision Making"},"content":"Rational Choice Paradigm says that all players will choose rationally - they will choose the best strategy. A player is rational if he chooses an action a^* \\in A that maximizes his payoff.a^* \\in A \\text{ is chosen when } \\forall a \\in A, v(a^∗) \\ge v(a)\n\nThis paradigm needs several assumptions: The player fully understands the decision problem by knowing:\n\nA, all possible actions\n\nX, all possible outcomes\n\nhow each action leads to which outcome\n\nhis rational preferences (payoffs) over outcomes.","type":"content","url":"/2022-02-08-orie4350-game-theory#rational-choice-paradigm","position":13},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Uncertainty and Time","lvl2":"Rational Decision Making"},"type":"lvl3","url":"/2022-02-08-orie4350-game-theory#uncertainty-and-time","position":14},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Uncertainty and Time","lvl2":"Rational Decision Making"},"content":"","type":"content","url":"/2022-02-08-orie4350-game-theory#uncertainty-and-time","position":15},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Evaluating Random Outcomes","lvl3":"Uncertainty and Time","lvl2":"Rational Decision Making"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#evaluating-random-outcomes","position":16},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Evaluating Random Outcomes","lvl3":"Uncertainty and Time","lvl2":"Rational Decision Making"},"content":"","type":"content","url":"/2022-02-08-orie4350-game-theory#evaluating-random-outcomes","position":17},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Expected Payoff","lvl4":"Evaluating Random Outcomes","lvl3":"Uncertainty and Time","lvl2":"Rational Decision Making"},"type":"lvl5","url":"/2022-02-08-orie4350-game-theory#expected-payoff","position":18},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Expected Payoff","lvl4":"Evaluating Random Outcomes","lvl3":"Uncertainty and Time","lvl2":"Rational Decision Making"},"content":"Imagine a case where results are non-deterministic. That is, after we take an action a, the payoff we get is drawn from a probability distribution. For example, if we choose to buy a lottery, there is a slight chance we win a big amount of money and a big chance that money is just gone. On the other hand, if we choose not to buy, our money remains unchanged.\n\nTo describe such a situation, we will need to calculate the expected payoff from action a: E[u(x) | a].\n\nThe rational paradigm changes accordingly: A rational player choose an action that maximizes the expected payoff.","type":"content","url":"/2022-02-08-orie4350-game-theory#expected-payoff","position":19},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Risk Attitude","lvl4":"Evaluating Random Outcomes","lvl3":"Uncertainty and Time","lvl2":"Rational Decision Making"},"type":"lvl5","url":"/2022-02-08-orie4350-game-theory#risk-attitude","position":20},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Risk Attitude","lvl4":"Evaluating Random Outcomes","lvl3":"Uncertainty and Time","lvl2":"Rational Decision Making"},"content":"Consider two actions: one guarantees to give you 9 yuan, and the other gives you 4 yuan with probability \\frac{7}{12} or 16 yuan with probability \\frac{5}{12}.\n\nA player is\n\nrisk neutral if he is willing to exchange any sure monetary reward with any lottery that promises the same expected reward. That is, this player has the payoff function u(x) = x. In our example, \\frac{7}{12}u(4)+\\frac{5}{12}u(16) = u(9).\n\nrisk aversive if he prefers not to be exposed to risk for the same expected monetary reward, so \\frac{7}{12}u(4)+\\frac{5}{12}u(16) \\lt u(9).\n\nrisk loving if he strictly prefers any lottery that promises the same expected monetary reward \\frac{7}{12}u(4)+\\frac{5}{12}u(16) \\gt u(9).\n\nIn real world, most people are risk aversive.","type":"content","url":"/2022-02-08-orie4350-game-theory#risk-attitude","position":21},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl2":"Static Games of Complete Information"},"type":"lvl2","url":"/2022-02-08-orie4350-game-theory#static-games-of-complete-information","position":22},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl2":"Static Games of Complete Information"},"content":"","type":"content","url":"/2022-02-08-orie4350-game-theory#static-games-of-complete-information","position":23},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Introduction","lvl2":"Static Games of Complete Information"},"type":"lvl3","url":"/2022-02-08-orie4350-game-theory#introduction","position":24},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Introduction","lvl2":"Static Games of Complete Information"},"content":"A game of complete information requires that the following be common knowledge among all the players of the game:\n\nall the possible actions of all the players\n\nall the possible outcomes\n\nhow each combination of actions of all players affects which outcome will materialize\n\nthe preferences of each and every player over outcomes\n\nA game is static if each player simultaneously and independently chooses an action.","type":"content","url":"/2022-02-08-orie4350-game-theory#introduction","position":25},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Normal-Form Games with Pure Strategies","lvl3":"Introduction","lvl2":"Static Games of Complete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#normal-form-games-with-pure-strategies","position":26},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Normal-Form Games with Pure Strategies","lvl3":"Introduction","lvl2":"Static Games of Complete Information"},"content":"A normal-form game consists of three features:\n\nA set of players - N = \\{1, 2, ..., n\\}\n\nCollection of sets of pure strategies - \\{S_1, S_2, ..., S_n\\}, where S_i = \\{s_1, s_2, ..., s_m\\}\n\nA set of payoff functions for each player that give a payoff value to each combination of the players’ chosen actions - v_i: S_1 \\times S_2 \\times \\dots \\times S_n \\to \\mathbb R\n\nStrategy is a plan of actions describing what action a player takes in every possible situation of the games. It is a set of actions not necessarily specific to a single task, but for a goal possibly containing multiple tasks and is influenced by other player’s actions.\n\nPure strategy indicate that the players choose actions deterministically, not stochastically.\n\nSince the players choose actions once and for all and simultaneously, we can equate pure strategies with actions in this chapter, but this wordy definition will be useful in later chapters.","type":"content","url":"/2022-02-08-orie4350-game-theory#normal-form-games-with-pure-strategies","position":27},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Rationality and Common Knowledge","lvl2":"Static Games of Complete Information"},"type":"lvl3","url":"/2022-02-08-orie4350-game-theory#rationality-and-common-knowledge","position":28},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Rationality and Common Knowledge","lvl2":"Static Games of Complete Information"},"content":"Define S_{-i} as the set of set of all the strategy sets of all players who are not player i, so S_{-i}\\equiv S_1 \\times \\dots S_{i-1} \\times S_{i+1} \\times \\dots S_n and (s_i, s_{-i}) = (s_1, \\dots, s_{i-1}, s_i, s_{i+1}, \\dots, s_n).","type":"content","url":"/2022-02-08-orie4350-game-theory#rationality-and-common-knowledge","position":29},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Dominance","lvl3":"Rationality and Common Knowledge","lvl2":"Static Games of Complete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#dominance","position":30},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Dominance","lvl3":"Rationality and Common Knowledge","lvl2":"Static Games of Complete Information"},"content":"For a player i, s'_i \\in S_i is strictly dominated by s_i\\in S_i when for any possible combination of the other player’s strategy, s_{-i} \\in S_{-i}, payoff from s'_i is strictly less than that from s_i:\\forall s_{-i} \\in S_{-i}, \\; v_i(s_i, s_{-i}) \\gt v_i(s'_i, s_{-i})\n\ns_i\\in S_i is a strictly dominant strategy for i if every other strategy of i is strictly dominated by it:\\forall s_{-i} \\in S_{-i}, \\; \\forall s'_i \\in S_i, \\; v_i(s_i, s_{-i}) \\gt v_i(s'_i, s_{-i})\n\nFrom the two definitions above, observe that:\n\nA rational player will never play a strictly dominated strategy.\n\nA rational player will always play the strictly dominant strategy if he has one.","type":"content","url":"/2022-02-08-orie4350-game-theory#dominance","position":31},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Iterated Elimination of Strictly Dominated Pure Strategies","lvl3":"Rationality and Common Knowledge","lvl2":"Static Games of Complete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#iterated-elimination-of-strictly-dominated-pure-strategies","position":32},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Iterated Elimination of Strictly Dominated Pure Strategies","lvl3":"Rationality and Common Knowledge","lvl2":"Static Games of Complete Information"},"content":"We will now on make the following assumptions that:\n\nAll players are rational.\n\nPlayers have complete information about the game.\n\nThe facts above are common knowledge.\n\nNow we introduce IESDS - iterated elimination of strictly dominated strategies. It is just what its name tells: it iteratively eliminates the strictly dominated strategies of any player. A formal description of the algorithm is as follows:\n\n\\forall i, define S^0_i = S_i.\n\nfor (k = 0; ; k++):\n\nstop_iteration = True\n\nfor (i = 1; i <= N; i++):\n\n\\hat S^{k}_i = filter(fun s: strictly_dominated(s), S^{k}_i)\n\nif \\hat S^{k}_i \\not= \\emptyset, stop_iteration = False\n\nS^{k+1}_i =  S^{k}_i - \\hat S^{k}_i\n\nif stop_iteration: return  S^k\n\nEliminate continuous strategies by truncating the interval we can choose this strategy from.\n\nEliminate discrete strategy by deleting a column or row of the matrix.","type":"content","url":"/2022-02-08-orie4350-game-theory#iterated-elimination-of-strictly-dominated-pure-strategies","position":33},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Beliefs, Best Response, and Rationalizability","lvl3":"Rationality and Common Knowledge","lvl2":"Static Games of Complete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#beliefs-best-response-and-rationalizability","position":34},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Beliefs, Best Response, and Rationalizability","lvl3":"Rationality and Common Knowledge","lvl2":"Static Games of Complete Information"},"content":"","type":"content","url":"/2022-02-08-orie4350-game-theory#beliefs-best-response-and-rationalizability","position":35},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Best Response and Beliefs","lvl4":"Beliefs, Best Response, and Rationalizability","lvl3":"Rationality and Common Knowledge","lvl2":"Static Games of Complete Information"},"type":"lvl5","url":"/2022-02-08-orie4350-game-theory#best-response-and-beliefs","position":36},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Best Response and Beliefs","lvl4":"Beliefs, Best Response, and Rationalizability","lvl3":"Rationality and Common Knowledge","lvl2":"Static Games of Complete Information"},"content":"IESDS is based on eliminating actions that players would never play. An alternative is to ask: what possible strategies might players choose to play given other players strategies? In this case, he has to choose a best strategy as a response to the strategies of his opponents.\n\nThe strategy s_i \\in S_i is player’s i’s best response to his opponents’ strategy s_{-i} \\in S_{-i} when\\forall s'_{i} \\in S_{i}, \\; v_i(s_i, s_{-i}) \\ge v_i(s'_i, s_{-i})\n\nBy introducing this notion of best strategy in response to other’s strategies, we are in effect introducing the idea of belief: What should I play if I believe the others will play s_{-i}? More formally, a belief is a strategy profile of his opponents’ strategies s_{-i} \\in S_{-i}.\n\nStrategy profile is a vector containing strategies from some players.\n\nNote that t\n\nThere can be more than one best responses, because we used the weekly preference relation in the definition.\n\nA rational player who believes his opponents are playing s_{-i} will always choose a best response to s_{-i}.\n\nIf we compare best response to strictly dominant strategy, the latter is the best in any given situation, while the former is the best in a particular situation. Therefore, a strictly dominated strategy cannot be a best response to any s_{-i}. On the other hand, a strictly dominant strategy is a best response to all s_{-i}\n\nThe best response correspondence BR_i(s_{-i}) \\subseteq S_i for player i to a belief s_{-i} is all the best responses he has to s_{-i}. That is, the set of strategies he would play if he knows that the others play s_{-i}.","type":"content","url":"/2022-02-08-orie4350-game-theory#best-response-and-beliefs","position":37},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Rationalizability","lvl4":"Beliefs, Best Response, and Rationalizability","lvl3":"Rationality and Common Knowledge","lvl2":"Static Games of Complete Information"},"type":"lvl5","url":"/2022-02-08-orie4350-game-theory#rationalizability","position":38},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Rationalizability","lvl4":"Beliefs, Best Response, and Rationalizability","lvl3":"Rationality and Common Knowledge","lvl2":"Static Games of Complete Information"},"content":"More generally, the set of rational strategies for player i is the set \\cup_{s_{-i} \\in \\text{i's beiliefs}} BR_i(s_{-i}). Remember rational strategies are define above: the strategies that maximize payoff.\n\nNext, we introduce the process of iterative rationalizability. Similar to IESDS, we iteratively delete strategies, but this time instead of asking “What would a rational player not do?” we ask “What might a rational player do?” A rational player will select only strategies that are a best response to some profile of his opponents, so we eliminate those strategies that are not best response to any beliefs.\n\nWe will start with belief = S_{-i} for all players (one believes that the others can play anything), and then we will update iteratively according to the above algorithm.\n\nThis strategy profiles that remain after this iterative procedure are called rationalizable strategies.\n\nE.g. the Battle of Sex: rationality failing> BR_1(\\{O\\}) = \\{O\\} \\\\\n> BR_1(\\{F\\}) = \\{F\\}\n>\n\nWe see the rational strategies are all the strategies. We cannot go any further. Basically any thing can happen, 1 still can play anything. So we cannot eliminate anything.\n\nFinally, we will emphasize that Rationalizable equilibrium \\subseteq IESDS equilibrium. This is a direct result of definition: recall that IESDS only removes strictly dominated strategy, which is a more strict requirement than removing a strategy that is never a best response. Therefore, we always remove more (or equal) strategies in rationalizability elimination.","type":"content","url":"/2022-02-08-orie4350-game-theory#rationalizability","position":39},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Nash Equilibrium","lvl2":"Static Games of Complete Information"},"type":"lvl3","url":"/2022-02-08-orie4350-game-theory#nash-equilibrium","position":40},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Nash Equilibrium","lvl2":"Static Games of Complete Information"},"content":"The reason why our approach failed in the Battle of Sex is that we cannot determine what exactly they will play - our beliefs can be wrong. What happens if what we believe is always true?\n\nThe pure-strategy profile is (s_1^*, s_2^*, \\dots, s_n^*) is a Nash Equilibrium if all the players’ beliefs are correct (each player plays exactly the way others think they will play):\\forall i \\in N, \\; s_i^* \\in BR_i(s_{-i}^*)\n\nE.g. the Battle of Sex with Nash\n\nSo if we analyze the problem under Nash Equilibrium assumptions, (O,F) is not possible in the battle of sex: If 1 thinks 2 will play F (and 2 actually does, because 1’s belief is true), 1 will play F, not O. A similar reasoning makes (F,O) also impossible. This leaves us with two Nash Equilibrium: (O, O) and (F, F).\n\nFind a Nash Equilibrium: simply take the intersection of all player’s Best Responses to each of their beliefs. \\bigcap (BR_i(s_{-i}), s_{-i}). For a continuous example, refer to Tadelis 5.2.2 The Tragedy of the Commons; for a discrete example, refer to 5.1.1 Pure-Strategy Nash Equilibrium in a Matrix.\n\nStrictly Dominant Equilibrium \\subseteq Nash Equilibrium \\subseteq Rationalizable Strategies \\subseteq IESDS\n\nIn this section, we are in fact introducing some kind of a “self-fulfilling beliefs”, “norm of society”. If I believe something, people will do that way.","type":"content","url":"/2022-02-08-orie4350-game-theory#nash-equilibrium","position":41},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Mixed Strategies","lvl2":"Static Games of Complete Information"},"type":"lvl3","url":"/2022-02-08-orie4350-game-theory#mixed-strategies","position":42},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Mixed Strategies","lvl2":"Static Games of Complete Information"},"content":"","type":"content","url":"/2022-02-08-orie4350-game-theory#mixed-strategies","position":43},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Definitions","lvl3":"Mixed Strategies","lvl2":"Static Games of Complete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#definitions","position":44},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Definitions","lvl3":"Mixed Strategies","lvl2":"Static Games of Complete Information"},"content":"Consider discrete strategies set S_i = \\{s_{i1}, \\dots, s_{im} \\} for player i and each strategy has a probability to be played.\n\nWe represent a mixed strategy using \\sigma_i \\in \\Delta S_i where \\sigma_i = \\{\\sigma_i(s_{i1}), \\dots, \\sigma_i(s_{im})\\} meaning \\sigma_i is a probability distribution associated with each element in S_i. The player plays s_{ik} with probability \\sigma_i(s_{ik}).\n\n\\Delta S_i is the simplex of S_i It is the set of all probability distribution over S_i\n\nNote: Pure strategies is a special case of mixed strategies.\n\nA pure strategy s_{i} is in the support of \\sigma_i when it is played with a non-zero probability, so \\sigma_i(s_{i}) \\gt 0\n\nWith a continuous strategy set, instead of the probability distribution function \\sigma_i, we have culmulative distribution function F_i and its differentiate, the probability density function f_i, over the continuous strategy set. s_{i} \\in S_i is in the support of F_i if f_i(s_i) \\gt 0\n\nA belief for player i is a probability distribution \\pi_i\\in \\Delta S_{-i} over the strategies of his opponents. This is generally not the same as \\sigma_{-i}, which describes what the other players will actually play. One is the prob dist of one’s belief, while the other is the prob dist of what one will actually play. We should be very familiar with this distinction from discussions in previous chapters. However, note \\pi_i=\\sigma_{-i} in a Nash setting.\n\nThe expected payoff of player i if he chooses pure strategies s_i \\in S_i and the other players play the mixed strategy \\sigma_{-i} \\in \\Delta S_{-i}, then\\begin{align}\nv_i(s_i, \\sigma_{-i})\n&= \\underset{s_{-i} \\sim \\sigma_{-i}}{\\mathbb E} v_i(s_i, s_{-i})\\\\\n&= \\sum_{s_{-i} \\in S_{-i}} v_i(s_i, s_{-i}) \\; \\sigma_{-i}(s_{-i})\n\\end{align}\n\nIf player i chooses a mixed strategy \\sigma_i \\in S_i,\\begin{align}\nv_i(\\sigma_i, \\sigma_{-i})\n&= \\underset{\\substack{s_i \\sim \\sigma_i \\\\ s_{-i} \\sim \\sigma_{-i}}}{\\mathbb E} v_i(s_i, s_{-i})\\\\\n&= \\sum_{s_{i} \\in S_{i}} \\sum_{s_{-i} \\in S_{-i}} v_i(s_i, s_{-i}) \\; \\sigma_{i}(s_{i}) \\; \\sigma_{-i}(s_{-i})\n\\end{align}","type":"content","url":"/2022-02-08-orie4350-game-theory#definitions","position":45},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Mixed-Strategy Nash Equilibrium","lvl3":"Mixed Strategies","lvl2":"Static Games of Complete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#mixed-strategy-nash-equilibrium","position":46},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Mixed-Strategy Nash Equilibrium","lvl3":"Mixed Strategies","lvl2":"Static Games of Complete Information"},"content":"We now generalize the concept of Nash equilibrium to mixed strategies:\n\nThe mixed strategy profile \\sigma^* = (\\sigma_1^*, \\dots, \\sigma_n^*) is a Nash equilibrium if for every player i\\in N, \\sigma_i^* is a best response to \\sigma_{-i}^*\n\nA strategy that is not a best response will not be in the support of \\sigma^*, so before looking for Mixed-Strategy Nash Equilibrium, we should first perform rationality elimination.\n\nProposition: Given a Nash equilibrium \\sigma^* = (\\sigma_1^*, \\dots, \\sigma_n^*), s_i and s_i' are in the support of \\sigma_i^*, then v_i(s_i, \\sigma_{-i}^*) = v_i(s_i', \\sigma_{-i}^*) = v_i(\\sigma_i^*, \\sigma_{-i}^*)\n\nProof.\n\nTake arbitrary s_i, s_i' \\in S_i\n\nAFOC, v_i(s_i, \\sigma_{-i}^*) \\gt v_i(\\sigma_i^*, \\sigma_{-i}^*). We can then conclude that \\sigma_i^* is not a best response to \\sigma_{-i}^*, so not inside Nash equilibrium. Contradiction. Therefore, we can concldue that ...\n\nAFOC, v_i(s_i, \\sigma_{-i}^*) \\lt v_i(\\sigma_i^*, \\sigma_{-i}^*).\n$$\\begin{align}\n>   v_i(\\sigma_i^*, \\sigma_{-i}^*)\n>   &= \\sum_{s_i \\in support(\\sigma_i^*)} \\sigma_i^*(s_i) v_i(s_i, \\sigma^*_{-i}) \\\\\n>   &\\lt \\sum_{s_i \\in support(\\sigma_i^*)} \\sigma_i^*(s_i) v_i(\\sigma_i^*, \\sigma^*_{-i}) \\\\\n>   &= v_i(\\sigma_i^*, \\sigma^*_{-i}) \\sum_{s_i \\in support(\\sigma_i^*)} \\sigma_i^*(s_i) \\\\\n>   &= v_i(\\sigma_i^*, \\sigma^*_{-i})\n>   \\end{align}\n\n$$\n\nTherefore, v_i(s_i, \\sigma_{-i}^*) = v_i(\\sigma_i^*, \\sigma_{-i}^*) and the same holds for s_i' by symmetry.\n\nTake away: In a Nash equilibrium, a player who plays a mixed strategy does not gain any more direct payoff. The payoff of all pure strategies that she is mixing over are all the same (so playing one pure strategy is no different from playing another pure strategy and so no different from playing a mixed of these pure strategies). The player plays a mixed strategy to make sure all the other players also play a mixed strategy so they can arrive at Nash Equilibrium.\n\nIn short, player 1 plays a mixed-strategy not to directly gain more, but so that player 2 is indifferent between the pure strategies in the support of \\sigma_2^* so that player 1 can gain indirectly.\n\nWe can use this proposition that v_i(s_i, \\sigma_{-i}^*) = v_i(s_i', \\sigma_{-i}^*) to find the Nash equilibrium mixed strategy. Refer to HW4 for some examples.","type":"content","url":"/2022-02-08-orie4350-game-theory#mixed-strategy-nash-equilibrium","position":47},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Nash’s Existence Theorem","lvl3":"Mixed Strategies","lvl2":"Static Games of Complete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#nashs-existence-theorem","position":48},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Nash’s Existence Theorem","lvl3":"Mixed Strategies","lvl2":"Static Games of Complete Information"},"content":"Any n-player normal-form game with finite strategy sets S_i for all players has a (Nash) equilibrium in mixed strategies.\n\nWe will not write down a proof of this theorem, but you can find proof in lecture notes 10, 11. Refer to HW4 Tadelis 6.12(e) for an application of this theorem.","type":"content","url":"/2022-02-08-orie4350-game-theory#nashs-existence-theorem","position":49},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl2":"Dynamic Games of Complete Information"},"type":"lvl2","url":"/2022-02-08-orie4350-game-theory#dynamic-games-of-complete-information","position":50},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl2":"Dynamic Games of Complete Information"},"content":"We always use backward induction to find Nash Equilibrium of Dynamic Games.","type":"content","url":"/2022-02-08-orie4350-game-theory#dynamic-games-of-complete-information","position":51},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Introduction","lvl2":"Dynamic Games of Complete Information"},"type":"lvl3","url":"/2022-02-08-orie4350-game-theory#introduction-1","position":52},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Introduction","lvl2":"Dynamic Games of Complete Information"},"content":"","type":"content","url":"/2022-02-08-orie4350-game-theory#introduction-1","position":53},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"The Extensive-Form Game","lvl3":"Introduction","lvl2":"Dynamic Games of Complete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#the-extensive-form-game","position":54},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"The Extensive-Form Game","lvl3":"Introduction","lvl2":"Dynamic Games of Complete Information"},"content":"In this section, we introduce sequential rationality: Earlier movers will take into account the rationality of players who move later in the game. Note it is not the chronological order of play that matters, but what players know when they make their choices.\n\nA game tree describes this sequential property. There is X, a set of nodes associated with each tree. Among them, we have\n\nroot x_0 and set of leaves / terminals Z. Leaves denote the the final payoffs of each player. The game proceeds from the root to the leaf.\n\nEvery node x that is not a leaf is assigned either to Nature or to a player i with the action set A_i(x). Function i(x) specifies which player (Nature is also a player) will play at moment x.\n\nEdges going from a node correspond to A_i(x) - the possible actions of the player associated with that node.\n\nWe need to specify what players know when it’s their turn. Therefore, we partition the nodes that are labelled with player i into information sets h_i. Every player has a collection of information sets H_i associated with them. Mathematically, H_i is a partition of \\{v | v\\in X, i(v)=i\\}. Information Sets has the following properties:\n\nIf h_i is a singleton that includes only x then player i who moves at x knows that he is at x.\n\nA player cannot distinguish between nodes that are in the same information set: If x\\not=x' and both x, x' \\in h_i, then player i who moves at x does not know whether he is at x or x'\n\nWe also assume that if x\\not=x' and both x, x' \\in h_i, then A_i(x) = A_i(x')\n\nRemember we defined Game of Complete Information at the beginning of part 2. This definition is enough for a normal-form game, but not suffices our extended form of game. Therefore, we introduce:\n\nGame of Perfect Information: a game of complete information in which every information set is a singleton and there are no moves of Nature. Therefore, at every step of the game, the player knows exactly where he is by knowing what happened before he chooses.\n\nGame of Imperfect Information: a game in which some information sets contain several nodes or in which there are moves of Nature. Therefore, at some steps of the game, some players do not know where they are because they either don’t know other players’ (endogenous uncertainty) or nature’s (exogenous uncertainty) choice.","type":"content","url":"/2022-02-08-orie4350-game-theory#the-extensive-form-game","position":55},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Strategies","lvl3":"Introduction","lvl2":"Dynamic Games of Complete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#strategies","position":56},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Strategies","lvl3":"Introduction","lvl2":"Dynamic Games of Complete Information"},"content":"                    x0                    P1 chooses first\n                  /    \\\n               O /      \\ F\n                /        \\\n               x1         x2              P2 chooses second\n              /  \\       /  \\\n           o /    \\ f  o/    \\ f\n            /      \\   /      \\\n           2       0   0       1          P1 payoff\n           1       0   0       2          P2 payoff\n\nA pure strategy of player i is a strategy profile for all information sets associated with i. Intuitively, for every information set i can be in, a pure strategy directs i what action to paly. Formally, it is mapping s_i: H_i \\to A_i that to every information set h_k \\in H_i assigns an action s_i(h_k) \\in A_i. Therefore, if player i can be in information set [h_1, h_2], a pure strategy s says [s(h_1), s(h_2)], meaning i will play s(h_1) when at information set h_1 and play s(h_2) when at information set h_2. Denote S_i as the set of all pure strategies.\n\nAs before, a mixed strategy for player i is a probability distribution over his pure strategies. Therefore, the player selects a plan randomly before the game is played and then follows a particular pure strategy.\n\nA behavioral strategy \\sigma_i: H_i \\to \\Delta A_i(h_i) specifies for each information set h_i \\in H_i an independent probability distribution over A_i(h_i), so \\sigma_i(a_i(h_i)) is the probability of player i plays a_i \\in A_i(h_i) when at information set h_i. Therefore, the player chooses what to play independently at each state as the game unfolds.\n\nCompare pure/mixed strategy with behavioral strategy in the context of Battle of the Sex with player 1 chooses first, then player 2. Player 2’s pure strategy is oo, of, fo, ff, where xy means to play x after 1 plays O and plays y after 1 plays F.\n\nWe can think of mixed strategy as joint distribution: \\sigma(ff) = Pr[\\text{2 f after 1 O and 2 f after 1 F}]\n\nWe can think of behavioral strategy as conditional distribution: \\sigma^F(o) = Pr[\\text{2 o | 1 F}]\n\nBehavioral strategy and mixed strategy can be easily translated into each other: \\sigma(ff) + \\sigma(of) = \\sigma^F(f). Note this only holds with the mild assumption of perfect recall (Tadelis Definition 7.7, in the example of absent minded driver, A randomized strategy will never receive the payoff of 4 because P1 will play the same actions in both worlds, while behavioral strategy flips a coin at every node. )\n\nAlso note that if a player has multiple turns, these distributions are assumed to be independent. Therefore, behavioral strategies and mixed strategies are not completely the same thing.","type":"content","url":"/2022-02-08-orie4350-game-theory#strategies","position":57},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Nash Equilibrium","lvl3":"Introduction","lvl2":"Dynamic Games of Complete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#nash-equilibrium-1","position":58},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Nash Equilibrium","lvl3":"Introduction","lvl2":"Dynamic Games of Complete Information"},"content":"To find a Nash equilibrium, we just have to list all pure strategies of each player in the matrix and find it as we used to do. (Tadelis 7.2.3)","type":"content","url":"/2022-02-08-orie4350-game-theory#nash-equilibrium-1","position":59},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Credibility & Sequential Rationality","lvl2":"Dynamic Games of Complete Information"},"type":"lvl3","url":"/2022-02-08-orie4350-game-theory#credibility-sequential-rationality","position":60},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Credibility & Sequential Rationality","lvl2":"Dynamic Games of Complete Information"},"content":"From the sequential Battle of Sex, we have 3 Nash Equilibrium if we consider it in the normal form: (O, oo) (O, of) (F, ff). However, is (F, ff) really rational? It seems like P1 will not choose O over F because he is threatened by player 2 choosing ff. But in fact, think about whether this ff threat is valid: if 1 suddenly switches to O, ff will be an irrational choice. (the first f in ff says P2 plays f when P1 plays O, but yeah think about this. This is not a rational move. BR_2(O) = o is not f) Therefore, we say that this ff threat is incredible.\n\nHere, we introduce a new concept that helps us eliminate such incredible Nash Equilibrium.","type":"content","url":"/2022-02-08-orie4350-game-theory#credibility-sequential-rationality","position":61},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Sequential Rationality & Backward Induction","lvl3":"Credibility & Sequential Rationality","lvl2":"Dynamic Games of Complete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#sequential-rationality-backward-induction","position":62},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Sequential Rationality & Backward Induction","lvl3":"Credibility & Sequential Rationality","lvl2":"Dynamic Games of Complete Information"},"content":"Sequential Rationality: a player use strategies that are optimal at every information set in the game tree - Given strategies \\sigma_{-i} \\in \\Delta S_{-i} of i’s opponents, we say that σ_i is sequentially rational when i is playing a best response to \\sigma_{-i} in each of his information sets.\n\nIn \\{x_1\\}, BR_2(\\{x_1\\}) = o; In \\{x_2\\}, BR_2(\\{x_2\\}) = f; so the sequentially rational best response from 2 is of: Namely, if 1 plays O, 2 plays o, and if 1 plays F, 2 plays f. oo is not optimal at x_2 and ff is not optimal at x_1\n\n1 does the same reasoning, so he will play BR_1(of) = O because O gives him a reward 2, while F will give him a reward 1.\n\nBackward Induction: We start at the leaves of the game tree and work up the tree, reasoning about best responses.\n\nThis is to model the fact that since every agent knows the entire game tree, the player that chooses early can reason about what the later players will choose based on his decisions. Once he knows what the later players choose, he can chooses to play what maximizes his own payoff.\n\nBackward Induction only works when at each point a decision is made, we know exactly where we are in the tree. That is, if all information sets are singletons (contain exactly 1 node of the tree). That is, we are playing a game of perfect information.\n\nAny finite game of perfect information has a backward induction solution that is sequentially rational. This is a pure strategy Nash Equilibrium. Furthermore if no two terminal nodes prescribe the same payoffs to any player then the backward induction solution is unique.","type":"content","url":"/2022-02-08-orie4350-game-theory#sequential-rationality-backward-induction","position":63},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Subgame-Perfect Nash Equilibrium","lvl3":"Credibility & Sequential Rationality","lvl2":"Dynamic Games of Complete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#subgame-perfect-nash-equilibrium","position":64},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Subgame-Perfect Nash Equilibrium","lvl3":"Credibility & Sequential Rationality","lvl2":"Dynamic Games of Complete Information"},"content":"","type":"content","url":"/2022-02-08-orie4350-game-theory#subgame-perfect-nash-equilibrium","position":65},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Definition","lvl4":"Subgame-Perfect Nash Equilibrium","lvl3":"Credibility & Sequential Rationality","lvl2":"Dynamic Games of Complete Information"},"type":"lvl5","url":"/2022-02-08-orie4350-game-theory#definition","position":66},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Definition","lvl4":"Subgame-Perfect Nash Equilibrium","lvl3":"Credibility & Sequential Rationality","lvl2":"Dynamic Games of Complete Information"},"content":"In this section, we look at the games of imperfect information (we don’t know exactly where we are in the tree). In such games, we cannot perform backward induction, because we may encounter multiple parents when working up the tree.\n\nSubgame is a subtree of the game tree s.t. no information set contains nodes both in and not in the subtree. Note the game itself is the subgame of itself.\n\nA behavioral strategy \\sigma^* is a subgame-perfect Nash equilibrium when for every subgame G, if we only look at this subgame, \\sigma^* is a Nash equilibrium in G.\n\nThe difference between a Nash Equilibrium and a Subgame-Perfect Nash Equilibrium is that what satisfies a NE but not a SGPNE is a strategy profile (for P2) that is only BR to what P1 plays when he believes P2 will play the current strategy (this is just the definition of NE) , but not a BR to all P1’s possible strategies (this is just the definition of SGPNE).","type":"content","url":"/2022-02-08-orie4350-game-theory#definition","position":67},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Example: Voluntary Battle of Sex","lvl4":"Subgame-Perfect Nash Equilibrium","lvl3":"Credibility & Sequential Rationality","lvl2":"Dynamic Games of Complete Information"},"type":"lvl5","url":"/2022-02-08-orie4350-game-theory#example-voluntary-battle-of-sex","position":68},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Example: Voluntary Battle of Sex","lvl4":"Subgame-Perfect Nash Equilibrium","lvl3":"Credibility & Sequential Rationality","lvl2":"Dynamic Games of Complete Information"},"content":"In voluntary battle of sex, we let P1 first choose whether it plays or not. If it doesn’t play, both players just get a payoff of 1.5 If it chooses to play, assume 2 chooses simultaneously with 1, so 2 doesn’t know what 1 chose.                         x0               P1 chooses to play or not\n                       /    \\\n                  Yes /      \\ No\n                     /        \\\n                    x1        1.5         If P1 wants to play, then it chooses first\n                  /    \\      1.5\n               O /      \\ F\n                /        \\\n             -----------------\n             | x2         x3 |            P2 chooses second\n             -----------------\n              /  \\       /  \\\n           o /    \\ f  o/    \\ f\n            /      \\   /      \\\n           2       0   0       1          P1 payoff\n           1       0   0       2          P2 payoff\n\nIn this example, we have two subgames: the classic battle of sex is a strict subgame of the voluntary battle of sex and this voluntary battle of sex is a subgame of itself. To solve the voluntary battle of sex, we first look at the strict subgame classic battle of sex. We know the Pure Strategy Nash equilibrium for this subgame is (O,o) and (F,f). Now we only have to consider these two cases. These two cases each have payoff (2,1) and (1,2). If we have another mixed strategy Nash Eq (\\sigma_1, \\sigma_2), we just have to calculate its corresponding expected payoff and go up one level to x_0.\n\nFrom there we can apply backward induction: back at x_0, this is player 1’s state so he player 1 gets to decide: BR_1(O,o) = Y, \\; BR_1(F,f) = N. Therefore, a rational player should play (YO,o) or (NF, f). These two are the only Pure Nash Equilibriums that are rational for every subgame.\n\nJust to revisit our definition, a behavioral strategy profile specifies for each player, an action for each of its information set. In this game, player 1 has information set \\{x_0\\}, \\{x_1\\} and player 2 has information set \\{x_2, x_3\\}. For the strategy profile (YO,o), 1 plays Y at \\{x_0\\} and O at \\{x_1\\}, 2 plays o at \\{x_2, x_3\\}.","type":"content","url":"/2022-02-08-orie4350-game-theory#example-voluntary-battle-of-sex","position":69},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Example: Stackelberg Competition (Variant of Cournot Duopoly)","lvl4":"Subgame-Perfect Nash Equilibrium","lvl3":"Credibility & Sequential Rationality","lvl2":"Dynamic Games of Complete Information"},"type":"lvl5","url":"/2022-02-08-orie4350-game-theory#example-stackelberg-competition-variant-of-cournot-duopoly","position":70},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Example: Stackelberg Competition (Variant of Cournot Duopoly)","lvl4":"Subgame-Perfect Nash Equilibrium","lvl3":"Credibility & Sequential Rationality","lvl2":"Dynamic Games of Complete Information"},"content":"Say P1 announces his price, and P2 decides based on that. So when P2 makes the decision, he already knows (not believes) quantity set by P1. P2 has a continuous information set, each being a particular q_1 P1 announces. Based on our definition of a behavioral strategy, P2 must react to each of his information set, so a strategy profile for P2 is \\frac{90-q_1}{2}. P1 uses this fact and reason backward and reach a single value 45.\n\nIt is curious that the player who announces first gets a better option: he has a unique value for Subgame Perfect Nash Eq. We should note this is because by announcing first in a game of complete information, P1 can reason about what P2 will do and use that information to improve his own decision. However, P2 cannot reason about P1’s behavior, because he will be told what P1 plays, he doesn’t get to believe what P1 plays.","type":"content","url":"/2022-02-08-orie4350-game-theory#example-stackelberg-competition-variant-of-cournot-duopoly","position":71},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Multistage Game","lvl2":"Dynamic Games of Complete Information"},"type":"lvl3","url":"/2022-02-08-orie4350-game-theory#multistage-game","position":72},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Multistage Game","lvl2":"Dynamic Games of Complete Information"},"content":"","type":"content","url":"/2022-02-08-orie4350-game-theory#multistage-game","position":73},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Introduction","lvl3":"Multistage Game","lvl2":"Dynamic Games of Complete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#introduction-2","position":74},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Introduction","lvl3":"Multistage Game","lvl2":"Dynamic Games of Complete Information"},"content":"A multistage game has T stages. At each stage i = 1, \\dots, T, the players play a static game of complete information. (Games can be different for different stages) with the following rules:\n\nAfter playing game in stage i and before moving on to the next stage i+1, all actions played of this current stage are revealed to all players\n\nThese above rules about multistage game is common knowledge.\n\nTo calculate payoff, we will assume a discount factor \\delta, so that payoff v one period in the future is worth \\delta v now. Higher discount factor \\delta means that the players are more patient and care more about future payoffs.\n\nThe total payoff of a player when he plays a strategy for this series of games is the discounted sum of payoffs this player can get at each stage: (v_i^t denotes the player i’s payoff at the game of stage t)v_i = \\sum^T_{t=1} \\delta^{t-1}v^t_i","type":"content","url":"/2022-02-08-orie4350-game-theory#introduction-2","position":75},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Strategies & Subgame Perfect Equilibrium","lvl3":"Multistage Game","lvl2":"Dynamic Games of Complete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#strategies-subgame-perfect-equilibrium","position":76},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Strategies & Subgame Perfect Equilibrium","lvl3":"Multistage Game","lvl2":"Dynamic Games of Complete Information"},"content":"In this section, we will use the Prisoner-Revenge Game:\n\nStage1: Prisoner’s Dilemma, with only 1 Pure Strategy Nash Eq.\n\nStage2: Revenge Game, with 2 Pure Strategy Nash Eq.\n\nFor each 2nd (last) stage subgame, its Nash Eq are the same Nash Eq when you play it as a standalone game. That is because: Given a static game G, define a new static game \\tilde G that is exactly the same except the payoffs are modified in the following way: \\tilde{v_i}(s_i, s_{-i}) = a_iv_i(s_i, s_{-i}) + b_i for some a_i \\gt 0, b_i \\in \\mathbb R. Pure Nash Equilibriums of this new game \\tilde G are exactly the same as the Pure Strategy Nash Equilibrium of original game G. This is because such modification preserves the preference relation and it’s obvious that two games with payoffs having the same preference relation should have the same Nash Eq.\n\nIn this Prisoner-Revenge Game, two players each have 2 actions at stage 1, so there are a total 2\\times 2 = 4 results from playing stage 1. Each result leads to a subgame in stage 2, so there are 4 subgames in stage 2. As we just analyzed, the PSNE in any subgame of stage 2 are the same as if we play this subgame as an independent standalone game. Therefore, each subgame of stage 2 has two PSNE: (L, l) and (G, g).\n\nTo be a SGPNE, it needs to be a NE at each subgame. We first look at each 2nd-stage subgame.\n\nArbitrarily choose one of the NE for each 2nd-stage subgame. There are 4 2nd-stage subgames and each subgame has 2 NE, so there are a total 2^4 = 16 combinations of strategies to consider. For any SGPNE, the actions for these 4 2nd-stage subgame, must be one of the 16 combinations (or it is not a NE at at least one of these subgames, so this profile as a whole cannot be subgame perfect). Therefore, we only have to consider these 16 profiles for SGPNE. Such a profile has the form s_i^2 = (s_i^2(Mm), s_i^2(Mf), s_i^2(Fm), s_i^2(Ff)), where s_i^2(ab) means what player i will do in 2nd-stage if in 1st-stage, 1 plays a and 2 plays b. One pair ab is one of the 4 possible outcomes of 1st-stage game, and therefore specifies a particular subgame in 2nd-stage. Since we have want SGPNE, s^2(ab) should also be a Nash Equilibrium when looked alone, so it is always either (L,l) or (G,g). A profile s^2(ab) of all players at situation ab specifies all players’ payoffs at a particular case of the 4 outcomes in 1st-stage game. Then we just have to find the PSNE in this 1st-stage game with this new payoff specified. (Like finding a PSNE in any game, it’s possible that we can find only one, none, or multiple) If we have a mixed-strategy s^2(ab) for the 2nd-stage game, we just have to calculate each player’s expected payoff in this subgame and use that as the new payoff of situation ab in 1st-stage game.\n\nA strategy profile for the whole game has the form s_i = (s_i^1, s_i^2) = (s_i^1, s_i^2(Mm), s_i^2(Mf), s_i^2(Fm), s_i^2(Ff)). For example, if we have s_1 = (F, L, G, G, L), s_2 = (f, l, g, g, l). Player 1 first chooses to fink at 1st-stage, chooses loner at 2nd-stage if 1 plays M and 2 plays m at 1st-stage, chooses Gangers if Mf, Gangers if Fm, and loner if Ff. Similarly for player 2. Eventually, the game path is ((Ff), (Ll)). For an arbitrary strategy profile, it doesn’t have to be symmetric like this. It just happens to be in this Prisoner-Revenge Game.","type":"content","url":"/2022-02-08-orie4350-game-theory#strategies-subgame-perfect-equilibrium","position":77},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Non-Nash to Nash","lvl3":"Multistage Game","lvl2":"Dynamic Games of Complete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#non-nash-to-nash","position":78},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Non-Nash to Nash","lvl3":"Multistage Game","lvl2":"Dynamic Games of Complete Information"},"content":"Note we get to choose which Nash Eq will be played in each of the 2nd (later) stage games and we can use this to reward good behavior in the 1st (any previous) stage game. In fact, we can persuade the players to play at this first (any previous) stage what is not a Nash equilibrium when play 1st-stage as a standalone game.\n\nContinue with the Prisoner-Revenge Game. In the 1st-stage of the game, the prisoner’s dilemma, we only have one Nash Eq (F,f). We want these two players to be loyal to each other and achieve (M,m). Therefore, in the 2nd-stage, we will reward (carot) loyalty (M,m) and punish (stick) anything else by making both players join the gangs if they don’t cooperate and be lone wolves if they do. Therefore, s_1^2 = (L, G, G, G), s_2^2 = (l, g, g, g). Draw the payoff matrix of 1st-stage based on the 2nd-stage strategies:\n\n\n\nm\n\nf\n\nM\n\n4,4\n\n-1-3\\delta, 5-3\\delta\n\nF\n\n5-3\\delta, -1-3\\delta\n\n1-3\\delta, 1-3\\delta\n\nWe can see that v_2(Mm) \\ge v_2(Mf) when \\delta \\ge \\frac 1 3. The same holds for P1 by symmetry. Therefore, as long as we have a discount factor \\ge \\frac 1 3, the Nash equilibrium for this 1st-stage game will be Mm and ((M, L, G, G, G), (m, l, g, g, g)) are our Subgame Perfect Nash Equilibrium.\n\nWe basically change the players’ beliefs about what will happen later. After understanding the bad result of betraying the partners (joining gangs for revenge and hurting each other), they will decide to keep the secret.","type":"content","url":"/2022-02-08-orie4350-game-theory#non-nash-to-nash","position":79},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Repeated Games","lvl2":"Dynamic Games of Complete Information"},"type":"lvl3","url":"/2022-02-08-orie4350-game-theory#repeated-games","position":80},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Repeated Games","lvl2":"Dynamic Games of Complete Information"},"content":"G(T,\\delta) denotes the finitely repeated game in which the stage-game G is played T consecutive times, and \\delta is the common discount factor.\n\nWe crucially relied on the fact that by having multiple Nash Eq in the 2nd-stage, we can have different factors for \\delta (these factors are just different payoff in 2nd-stage game) and this gives us the power to use some Nash Eq as a carrot and use some as a stick. If we only have one Nash Eq for this static game, then the only Subgame Perfect Equilibrium will be playing this Nash Eq all the time.\n\nHowever, if we have an Infinitely Repeated Game and there is only one NE in the standalone game, we can still design strategies that enables us to achieve a non-NE. Here, we introduce the “Grim-Trigger” Strategy for the prisoner’s game, where both keep playing (M,m) until one deviates. Specifically, they\n\nPlay (M,m) as long as both players played (M,m) in all stages before this one\n\nPlay (F,f) if otherwise\n\nThis turns out to also be a Subgame Perfect Nash Equilibrium for some \\delta value. However, since it’s an infinite game, we cannot check on every subgame. Luckily, we have a new principle to check SGPNE:\n\nOne-Step Deviation Principle: A strategy profile (\\sigma_1, \\dots, \\sigma_n) is a SGPNE when there is no subgame where a player can improve his payoff using a one-step deviation.\n\nFor a player i, a One-Step Deviation from his strategy s_i is a strategy s_i' when for all information sets, only action for one of them is different from s_i.\n\nTake the above Infinitely Repeated Prisoner’s Game as an example. Say if at the middle of the game, P1 wants to betray (deviate from all these (M,m) played) Consider this round where P1 betrays: he will gain 5 immediately, but 1 for all the subsequent rounds. On the other hand, if he sticks to cooperation, he will gain 5 in all of these rounds. To keep him from betraying, we need a discount factor that is4 + 4\\delta + 4\\delta^2 + \\dots \\ge 5 + 1\\delta + 1\\delta^2 + \\dots \\\\\n4 + 4 \\frac{\\delta}{1-\\delta} \\ge 5 + 1 \\frac{\\delta}{1-\\delta} \\\\\n\\delta \\ge \\frac 1 4\n\nThe generalized version of this principle is Folk Theorem: any periodic strategy that has average payoff that is larger than the trigger strategy is subgame perfect.","type":"content","url":"/2022-02-08-orie4350-game-theory#repeated-games","position":81},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl2":"Static Games of Incomplete Information"},"type":"lvl2","url":"/2022-02-08-orie4350-game-theory#static-games-of-incomplete-information","position":82},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl2":"Static Games of Incomplete Information"},"content":"","type":"content","url":"/2022-02-08-orie4350-game-theory#static-games-of-incomplete-information","position":83},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Bayesian Games","lvl2":"Static Games of Incomplete Information"},"type":"lvl3","url":"/2022-02-08-orie4350-game-theory#bayesian-games","position":84},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Bayesian Games","lvl2":"Static Games of Incomplete Information"},"content":"","type":"content","url":"/2022-02-08-orie4350-game-theory#bayesian-games","position":85},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Representation","lvl3":"Bayesian Games","lvl2":"Static Games of Incomplete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#representation","position":86},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Representation","lvl3":"Bayesian Games","lvl2":"Static Games of Incomplete Information"},"content":"To capture the idea that a player’s characteristics may be unknown to other players, we now say that a player can be of different types. A same player of different type can have different payoff values over the same strategy profile.\n\nGames of Incomplete Information are the games that incorporate the possibility that players could be of different types.\n\nEach combination of player type gives a different game. We always have nature node first deciding which game tree we are in. We use information sets to denote that a player knows what type he is, but doesn’t know what type the other players are.","type":"content","url":"/2022-02-08-orie4350-game-theory#representation","position":87},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Normal Form","lvl4":"Representation","lvl3":"Bayesian Games","lvl2":"Static Games of Incomplete Information"},"type":"lvl5","url":"/2022-02-08-orie4350-game-theory#normal-form","position":88},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Normal Form","lvl4":"Representation","lvl3":"Bayesian Games","lvl2":"Static Games of Incomplete Information"},"content":"Normal Form of a static Bayesian Game of Incomplete Information has\n\na set of players - N = \\{1, 2, \\dots, n\\}\n\nFor each player i:\n\na action set - A_i\n\na type space - \\Theta_i, where different types of the same player gives different games\n\ntype dependent payoff function - v_i: A_1 \\times \\dots \\ A_n \\times \\Theta_i \\to \\mathbb R, where different types of the same player playing the same action gives different payoffs\n\na probability distribution over the types of the players that specifies the probability of us being on a particular game tree - \\Theta_1 \\times \\Theta_2 \\times \\dots\\ \\times \\Theta_n","type":"content","url":"/2022-02-08-orie4350-game-theory#normal-form","position":89},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Beliefs","lvl4":"Representation","lvl3":"Bayesian Games","lvl2":"Static Games of Incomplete Information"},"type":"lvl5","url":"/2022-02-08-orie4350-game-theory#beliefs","position":90},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Beliefs","lvl4":"Representation","lvl3":"Bayesian Games","lvl2":"Static Games of Incomplete Information"},"content":"And all the information above is common knowledge. So a player surely knows this probability distribution over all the types. One’s own type is a private knowledge to oneself. If a player knows his own type, he can deduce something more about the others’ types: he can deduce the posterior belief about others’ types \\phi_i = \\phi(\\theta_{-i} | \\theta_i)\n\nFormally, the game proceeds as follows:\n\nNature chooses a profile of types (\\theta_1, \\theta_2, \\dots, \\theta_n) according to the probability distribution of types\n\nEach player i learns his own type \\theta_i and deduces a belief about the probability distribution over the other player’s types, denoted by \\phi(\\theta_{-i} | \\theta_i).\n\nEach player i simultaneously chooses an action a_i \\in A_i\n\nPayoffs are realized for each player i by v_i(a_i, a_{-i}, \\theta_i)","type":"content","url":"/2022-02-08-orie4350-game-theory#beliefs","position":91},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Strategies","lvl4":"Representation","lvl3":"Bayesian Games","lvl2":"Static Games of Incomplete Information"},"type":"lvl5","url":"/2022-02-08-orie4350-game-theory#strategies-1","position":92},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Strategies","lvl4":"Representation","lvl3":"Bayesian Games","lvl2":"Static Games of Incomplete Information"},"content":"Strategies in Games of Incomplete Information are defined very similar to the strategies on information sets in Dynamic Games.\n\nA Pure Strategy for player i is a function for each of its type - s_i : \\Theta_i \\to A_i\n\nA Mixed Strategy is a probability distribution over pure strategies.","type":"content","url":"/2022-02-08-orie4350-game-theory#strategies-1","position":93},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Payoffs","lvl4":"Representation","lvl3":"Bayesian Games","lvl2":"Static Games of Incomplete Information"},"type":"lvl5","url":"/2022-02-08-orie4350-game-theory#payoffs","position":94},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"Payoffs","lvl4":"Representation","lvl3":"Bayesian Games","lvl2":"Static Games of Incomplete Information"},"content":"","type":"content","url":"/2022-02-08-orie4350-game-theory#payoffs","position":95},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"and Bayesian Nash Equilibrium","lvl4":"Representation","lvl3":"Bayesian Games","lvl2":"Static Games of Incomplete Information"},"type":"lvl5","url":"/2022-02-08-orie4350-game-theory#and-bayesian-nash-equilibrium","position":96},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"and Bayesian Nash Equilibrium","lvl4":"Representation","lvl3":"Bayesian Games","lvl2":"Static Games of Incomplete Information"},"content":"Expected payoff of a player i of type \\theta_i takes the weighted average over all possible combination of other players’ types. Note a player in the Bayesian Game always knows his own type.\\begin{align}\nv_i(s ; \\theta_i)\n&= \\mathbb E_{\\theta_{-i}} \\; v_i(s_i(\\theta_i), s_{-i}(\\theta_{-i}) ; \\theta_i)\\\\\n&= \\sum_{\\theta_{-i}} \\phi(\\theta_{-i} | \\theta_i) \\; v_i(s_i(\\theta_i), s_{-i}(\\theta_{-i}) ; \\theta_i)\n\\end{align}\n\nBayesian Nash Equilibrium is a strategy profile where every player chooses a best response for every possible type he can be. This definition is almost the same as the sequential rationality, except it was “best response for every possible information set he can be in”.\n\nFormally, s^* is a Pure-Strategy Nash Equilibrium when for every player i and every type \\theta_i \\in \\Theta_i this player can be, s^*_i(\\theta_i) is a best response to s^*_{-i}\\forall i, \\; \\forall \\theta_i \\in \\Theta_i, \\; s_i(\\theta_i) \\in BR_i(s^*_{-i})\\\\\n\\forall i, \\; \\forall \\theta_i \\in \\Theta_i, \\; \\forall s_i' \\in S_i, \\; v_i(s_i^*, s_{-i}^*; \\theta_i) \\ge v_i(s_i', s_{-i}^*; \\theta_i)","type":"content","url":"/2022-02-08-orie4350-game-theory#and-bayesian-nash-equilibrium","position":97},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Eg. Game of Chicken","lvl3":"Bayesian Games","lvl2":"Static Games of Incomplete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#eg-game-of-chicken","position":98},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Eg. Game of Chicken","lvl3":"Bayesian Games","lvl2":"Static Games of Incomplete Information"},"content":"In the game of chicken, for player 1, his strategy set is \\{CC, CD, DC, DD\\} where AB means 1 plays A when he has type L and plays B when he has type H.\n\nDo an example calculating expected payoff for P1 at situation (CD, dc):\\begin{align}\nv_1(CD, dc) &= P[\\theta_1=L, \\theta_2 = L] v_1(C,d; L) \\\\\n\t\t\t&+ P[\\theta_1=L, \\theta_2 = H] v_1(C,c; L) \\\\\n\t\t\t&+ P[\\theta_1=H, \\theta_2 = L] v_1(D,d; H) \\\\\n   \t\t\t&+ P[\\theta_1=H, \\theta_2 = H] v_1(D,c; H) \\\\\n\\end{align}","type":"content","url":"/2022-02-08-orie4350-game-theory#eg-game-of-chicken","position":99},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Eg. Study Groups","lvl3":"Bayesian Games","lvl2":"Static Games of Incomplete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#eg-study-groups","position":100},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Eg. Study Groups","lvl3":"Bayesian Games","lvl2":"Static Games of Incomplete Information"},"content":"Two students form a study group. A student can choose to work (W) or shirk (S). If at least one student works, the work will be done. If a student chooses to work, he will pay a cost c no matter what. Type of each student \\Theta_i = [0,1] determines how important the assignment is for each student. Types are uniformly i.i.d drawn. A player’s strategy is s_i: \\Theta_i \\to \\{W,S\\} We have payoffs (the former one represents action of the player in discussion)\n\nv_i(S,S; \\theta_i) = 0\n\nv_i(W,W;\\theta_i) = v_i(W,S;\\theta_i) = \\theta_i^2-c\n\nv_i(S,W;\\theta_i) = \\theta_i^2\n\nCompute the expected payoff if P1 plays W and S:\n\nv_1 (W,s_2(\\theta_2);\\theta_1) = \\theta_1^2 - c\n\nv_1 (S,s_2(\\theta_2);\\theta_1) = \\theta_1^2 \\; Pr[s_2(\\theta_2)=W] + 0 \\; Pr[s_2(\\theta_2)=S]\n\nTherefore, we choose W whenv_1(W;\\theta_1) \\gt v_1(S;\\theta_1) \\\\\n\\theta_1^2-c \\gt \\theta_1^2 \\; Pr[s_2(\\theta_2)=W]\\\\\n\\theta_1 \\gt \\sqrt{\\frac c {1 - Pr[s_2(\\theta_2)=W]}}\n\nDenote Pr[s_2(\\theta_2)=W] as p, s_1 = W \\iff \\theta_1 \\gt \\sqrt{\\frac c {1-p}}. Since \\theta_1 is drawn uniformly random from [0,1], Pr[\\theta_1 \\gt \\sqrt{\\frac c {1-p}}] = 1 - \\sqrt{\\frac c {1-p}}.\n\nBy symmetry, this also holds for P2. Pr[s_1(\\theta_1)=W] = Pr[s_2(\\theta_2)=W] = p. We therefore have p = 1 - \\sqrt{\\frac c {1-p}} and 1 - p = \\sqrt[3] c. \\theta_1 \\gt \\sqrt{\\frac c {1-p}} = \\sqrt[3] c\n\nNow we can conclude thats_i^*(\\theta_i) = \\begin{cases} W \\text{ if } \\theta_i \\gt \\sqrt[3] c \\\\ S \\text{  if otherwise} \\end{cases}","type":"content","url":"/2022-02-08-orie4350-game-theory#eg-study-groups","position":101},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Auctions","lvl2":"Static Games of Incomplete Information"},"type":"lvl3","url":"/2022-02-08-orie4350-game-theory#auctions","position":102},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Auctions","lvl2":"Static Games of Incomplete Information"},"content":"open auction: bidders observe some dynamic price process until winner emerges\n\nsealed-bid auction: players write down their bids and submit these without knowing the bids of other players. Bids are collected. Then somebody wins, somebody pays according to rules.\n\nWe focus on Sealed-Bid Auction in this section","type":"content","url":"/2022-02-08-orie4350-game-theory#auctions","position":103},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Model","lvl3":"Auctions","lvl2":"Static Games of Incomplete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#model","position":104},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Model","lvl3":"Auctions","lvl2":"Static Games of Incomplete Information"},"content":"n players\n\neach player i has a type \\theta_i, which corresponds to private value i has for this object. \\theta_i is drawn independently from some distribution \\theta_i \\sim F_i(\\cdot) where F_i is the cumulative probability function of this distribution.\n\naction is a bid b_i\n\npayoff to player i is\n\n0 if he does not win\n\n\\theta_i - p if she wins\n\nwhere p is a function of the bids to a price, representing the paying rule of this auction. For example, most of the time we let the bidder who wins the bid (that is who proposes the highest bid) pays his bids, but maybe we can let the winner to pay the second highest bid instead.","type":"content","url":"/2022-02-08-orie4350-game-theory#model","position":105},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Second-Price Auction - Strategy","lvl3":"Auctions","lvl2":"Static Games of Incomplete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#second-price-auction-strategy","position":106},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Second-Price Auction - Strategy","lvl3":"Auctions","lvl2":"Static Games of Incomplete Information"},"content":"In second-price auction, p is second highest bid. Person who wins the bid, instead of paying his own bid, pays the second highest bid. Bidding your private value is the weakly dominant strategy.\n\nObserve for any bid b_i player i chooses to pay, his payoff function v_i(b_i, b_{-i}) isv_i(b_i, b_{-i})=\n\\begin{cases}\n\\theta_i - max(b_{-i})  &\\text{if $i$ wins the bid: $b_i \\gt max(b_{-i})$}\\\\\n0& \\text{if $i$ loses the bid: $b_i \\lt max(b_{-i})$ }\n\\end{cases}\n\nObserve that if we decide to pay our private value \\theta_i, we have a weakly dominant strategy. (See this by replacing all b_i above with \\theta_i)","type":"content","url":"/2022-02-08-orie4350-game-theory#second-price-auction-strategy","position":107},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"First-Price Auction - Strategy","lvl3":"Auctions","lvl2":"Static Games of Incomplete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#first-price-auction-strategy","position":108},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"First-Price Auction - Strategy","lvl3":"Auctions","lvl2":"Static Games of Incomplete Information"},"content":"Bidding your private value is weakly dominated: you eventually just get b_i - \\theta_i = \\theta_i - \\theta_i = 0. One can get the same payoff by simply not participating in bidding at all. Next we find the Bayesian Nash Eq strategy for this problem.\n\nAssumption: types of each player are drawn independently from some distribution with CDF, cumulative distribution function F_i\n\nStrategy b_i = s_i: \\Theta_i \\to \\mathbb R^{\\ge 0} is a map from private value to bidding value.\n\nWe will assume that s_i is strictly increasing (higher private valuation leads to higher bid)\\begin{align}\n& P[\\text{player $i$ wins}] \\\\\n=\\;& P[ \\forall j \\not=i, s_i(\\theta_i) \\gt s_j(\\theta_j)]\\\\\n=\\;& P[\\forall j \\not=i, s_j(\\theta_j) \\lt t]  &&\\text{replace $s_i(\\theta_i)$ with $t$}\\\\\n=\\;& P[\\forall j \\not=i, \\theta_j \\lt s^{-1}_j(t)]  &&\\text{since $s$ is strictly increasing, its inverse func exists}\\\\\n=\\;& \\prod_{j\\not=i} P[\\theta_j \\lt s^{-1}_j(t)]  &&\\text{each type is independent}\\\\\n=\\;& \\prod_{j\\not=i} F_j(s^{-1}_j(t))  &&\\text{definition of CDF}\\\\\n\\end{align}\n\nPayoff if i wins: P[\\text{player i wins}] \\times (\\theta_i - x) = \\prod_{j\\not=i} F_j(s^{-1}_i(t))  \\times (\\theta_i - x)\n\nTo simplify this question a bit more, we make some extra assumptions: players’ types are drawn from the same CDF F and each player have the same strategy s. Therefore, the payoff is now [F(s^{-1}(t))]^{n-1} \\times (\\theta_i - x)\n\nTo find its maximum, we take the derivative of this thing. And solve for the best response,s(\\theta_i) = \\theta_i - \\frac {\\int^{\\theta_i}_{-\\infty} [F(t)]^{n-1} dt } {[F(\\theta_i)]^{n-1}}\n\nConsider the special case where \\theta_i \\sim uniform[0,1], where we are basically normalizing private value to the scale of [0,1]\\begin{align}\ns(\\theta_i)\n&= \\theta_i - \\frac {\\int^{\\theta_i}_{0} t^{n-1} dt } {\\theta_i^{n-1}} \\\\\n&= \\frac {n-1} {n} \\theta_i\n\\end{align}","type":"content","url":"/2022-02-08-orie4350-game-theory#first-price-auction-strategy","position":109},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"First-Price Auction - Revenue","lvl3":"Auctions","lvl2":"Static Games of Incomplete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#first-price-auction-revenue","position":110},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"First-Price Auction - Revenue","lvl3":"Auctions","lvl2":"Static Games of Incomplete Information"},"content":"Expected revenue for this auction is \\mathbb E[s(\\theta)] = \\mathbb E[(1 - \\frac 1 n) \\theta_{max}]. We need to focus on \\theta_{max} because according to strategy s(\\theta) = (1-\\frac 1 n)\\theta, the one with highest \\theta wins the bid and therefore pays the auction. That is, the one with highest private value eventually determine how much revenue an auction can make.\n\nThis expected value will average on the distribution of this \\theta_{max}. Look at CDF and PDF of \\theta_{max}:\\begin{align}\nG(x) &= \\mathbb P(\\theta_{max} \\le x)\\\\\n&= \\mathbb P(\\theta_{1} \\le x) \\mathbb P(\\theta_{2} \\le x) \\dots \\mathbb P(\\theta_{n} \\le x) && \\text{$\\theta$ are independent} \\\\\n&= x^n && \\text{$\\theta$ is uniformly distributed on [0,1]}\\\\\ng(x) &= \\mathbb P(\\theta_{max} = x) = nx^{n-1}\n\\end{align}\n\nWith PDF, we can calculate the expected revenue\\begin{align}\nE[(1 - \\frac 1 n) \\theta_{max}]\n&= (1 - \\frac 1 n) E[\\theta_{max}] \\\\\n&= (1 - \\frac 1 n) \\int_0^1 x \\; nx^{n-1} \\; dx \\\\\n&= \\frac {n-1} {n+1}\n\\end{align}\n\nIf we let \\theta_i \\sim uniform[0,c], this result will be \\frac {n-1} {n+1}c","type":"content","url":"/2022-02-08-orie4350-game-theory#first-price-auction-revenue","position":111},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Second-Price Auction - Revenue","lvl3":"Auctions","lvl2":"Static Games of Incomplete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#second-price-auction-revenue","position":112},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Second-Price Auction - Revenue","lvl3":"Auctions","lvl2":"Static Games of Incomplete Information"},"content":"Recall that s_i(\\theta_i) = \\theta_i, and the second highest price \\theta_i^{(2)} is paid. so the expected revenue of an auction is \\mathbb E(\\theta_i^{(2)}) Look at the CDF and PDF of \\theta_i^{(2)}\\begin{align}\nG(x) &= \\mathbb P(\\theta_i^{(2)} \\le x) \\\\\n&= \\sum_{j\\in N} \\mathbb P(\\forall i\\not=j, \\theta_i \\le x \\text{ and } \\theta_j \\gt x)\n + \\mathbb P (\\forall i \\in N, \\theta_i \\le x)\\\\\n&= n\\; x^{n-1}(1-x) + x^n\\\\\ng(x) &= n(n-1)x^{n-2} + n(1-n)x^{n-1}\n\\end{align}\n\nNow we can calculate the expected revenue\\begin{align}\n\\mathbb E(\\theta_i^{(2)})\n&= \\int_0^1 x \\; (n(n-1)x^{n-2} + n(1-n)x^{n-1}) \\; dx \\\\\n&= \\frac {n-1} {n+1}\n\\end{align}\n\n如果 private value is [0,c] 那么最后就是 \\frac {n-1} {n+1}c","type":"content","url":"/2022-02-08-orie4350-game-theory#second-price-auction-revenue","position":113},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Revenue Equivalence","lvl3":"Auctions","lvl2":"Static Games of Incomplete Information"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#revenue-equivalence","position":114},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Revenue Equivalence","lvl3":"Auctions","lvl2":"Static Games of Incomplete Information"},"content":"As we introduced all these different auctions, it is natural to ask which will yield the highest expected revenue for a seller. However, it turns out that the expected revenue a seller obtains from any of auctions we considered is the same as long as we meet some very general conditions.","type":"content","url":"/2022-02-08-orie4350-game-theory#revenue-equivalence","position":115},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl2":"Appendix"},"type":"lvl2","url":"/2022-02-08-orie4350-game-theory#appendix","position":116},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl2":"Appendix"},"content":"","type":"content","url":"/2022-02-08-orie4350-game-theory#appendix","position":117},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Abbreviation","lvl2":"Appendix"},"type":"lvl3","url":"/2022-02-08-orie4350-game-theory#abbreviation","position":118},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Abbreviation","lvl2":"Appendix"},"content":"NE = Nash Eq = Nash Equilibrium\n\nPSNE: Pure Strategy Nash Equilibrium\n\nSGPNE: Subgame Perfect Nash Equilibrium","type":"content","url":"/2022-02-08-orie4350-game-theory#abbreviation","position":119},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Test Tips","lvl2":"Appendix"},"type":"lvl3","url":"/2022-02-08-orie4350-game-theory#test-tips","position":120},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Test Tips","lvl2":"Appendix"},"content":"Find all Nash Equilibrium: pure strategy and mixed strategy.\n\n当连续的时候，很多东西 (BR, payoff, ...) 都要分类讨论一般情况和极端情况 (Cournot Duopoly 中当对手正常生产的时候，你有一个 BR；以及当对手直接拉满的时候，你有另一个 BR=0 因为你不管生产多少都亏本）","type":"content","url":"/2022-02-08-orie4350-game-theory#test-tips","position":121},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Incomplete Note: Mechanism Design","lvl2":"Appendix"},"type":"lvl3","url":"/2022-02-08-orie4350-game-theory#incomplete-note-mechanism-design","position":122},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Incomplete Note: Mechanism Design","lvl2":"Appendix"},"content":"This section is not complete and will likely never be completed.\n\nIn this section, we discuss how to design a game given a goal.\n\nAuctions can be viewed as a solution to a social problem: We have a good and players have a private valuation of the good. Society is best off if the good is assigned to players with the highest valuation.\n\nWe want to design a game (mechanism) to find this best assignment even if all players strategically (even if players try to game the mechanism)\n\nFormalize the problem:\n\nplayers have private types \\theta_i\n\nif we know these types, we know what is best for society. So there will be a function f from type to outcome, describing what is the best outcome given players’ types. f:\\theta\\to \\text{outcome} Note \\theta is a type profile here \\theta = (\\theta_1, \\theta_2, \\dots, \\theta_n)\n\nWe say a rule is implementable if we can ???\n\nPlayers know the game and play strategically (so they want to get the best outcome for themselves, not necessarily for the whole society). How can we design a mechanism so the society good is achieved?","type":"content","url":"/2022-02-08-orie4350-game-theory#incomplete-note-mechanism-design","position":123},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"How did we solve this?","lvl3":"Incomplete Note: Mechanism Design","lvl2":"Appendix"},"type":"lvl5","url":"/2022-02-08-orie4350-game-theory#how-did-we-solve-this","position":124},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl5":"How did we solve this?","lvl3":"Incomplete Note: Mechanism Design","lvl2":"Appendix"},"content":"Problem: since all people can calculate this result, it’s possible players will end up reporting a private value smaller than this to skip some payments.\n\nQuasilinear Performances Assumption: we will assume that for player i if the final (global) outcome is x and monetary payment to player i is m_i, he has a payoff v_i: (Note m_i can be negative, representing how much money i has to pay out)v_i(x, m_i, \\theta_i) = v_i(x, \\theta_i) + m_i\n\nDesign this game:\n\nchoose actions A_i for each player\n\nBased on actions, decide an outcome and payment for this game\n\ng: A_1 \\times \\dots \\times A_n \\to \\text{outcome space}\n\nm_i:A_1 \\times \\dots \\times A_n \\to \\mathbb R\n\nWe want that for the equilibrium strategy profile s^*, the outcome we get from each player choosing based on its own benefit is indeed our socially best outcome. \\forall \\theta \\in \\Theta, g(s^*(\\theta)) = f(\\theta)\n\nConsider this special type of game: let all player’s actions be their types(private values), so just assign each of their action space to be their type space. i.e. \\forall i \\in N, A_i = \\Theta_i\n\nThen g \\equiv f will give us the desired mechanism, provided that playing their true types is an equilibrium strategy profile.\n\nGiven f, define g???\n\n-- ting bu dong--\n\nGames where \\forall i\\in N, A_i = \\Theta_i are called “direct revelation mechanisms”\n\nRevelation Principle: If f is implementable (see implementable definition) in Bayesian Nash/ dominant strategies, then there exists a direct revelation mechanism.\n\nprove. If f is implementable, then there exists an equilibrium s^*  such that \\forall \\theta \\in \\Theta g(s^*(\\theta))\n\nConsider the following mechanism ??","type":"content","url":"/2022-02-08-orie4350-game-theory#how-did-we-solve-this","position":125},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Vickrey-Clarke-Groves Mechanism","lvl3":"Incomplete Note: Mechanism Design","lvl2":"Appendix"},"type":"lvl4","url":"/2022-02-08-orie4350-game-theory#vickrey-clarke-groves-mechanism","position":126},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl4":"Vickrey-Clarke-Groves Mechanism","lvl3":"Incomplete Note: Mechanism Design","lvl2":"Appendix"},"content":"We will think about implementing functions f(\\theta) = x where x maximizes \\sum_{i \\in N} v_i(x, \\theta_i) That is the outcome that gives highest sum of payoffs is there are no monetary transfers.\n\nIn this sense, this objective is sometimes called “social welfare”\n\nIf there is o payment, then lying about your type may be beautiful if f(\\theta_i, \\theta_{-i}) \\not= f(\\theta_i', \\theta_{-i}) and v_i(f(\\theta_i, \\theta_{-i}), \\theta_i) \\gt v_i(f(\\theta_i', \\theta_{-i}), \\theta_i)\n\nWe do not want this happen, so to counteract\n\nif he is reporting his true types, we want the monetary payment to him to be at least than that, so he is willing to report his true type\n\nWe can simply define m_i(\\theta_i,) to be ..., so m_i(\\theta_i',) is and we see the previous inequality holds","type":"content","url":"/2022-02-08-orie4350-game-theory#vickrey-clarke-groves-mechanism","position":127},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Mechanism 2","lvl2":"Appendix"},"type":"lvl3","url":"/2022-02-08-orie4350-game-theory#mechanism-2","position":128},{"hierarchy":{"lvl1":"ORIE4350 Game Theory","lvl3":"Mechanism 2","lvl2":"Appendix"},"content":"Goal: create a game to implement “social welfare function” f(\\theta) = argmax*x \\sum*{i \\in N} v_i(x, \\theta_i) . So it is the outcome that maximizes overall payoff (sum of payoffs).\n\nTypes are private info and we use game to extract this info from the players.\n\nWe will show that we can implement social welfare functions using the revelation mechanism in weakly dominant strategies.\n\nuse differences of payments if you lie about your type - you can lie, you can get better payoff by lying, but your monetary payment will be about how much you hurt the other players because it’s a social thing.","type":"content","url":"/2022-02-08-orie4350-game-theory#mechanism-2","position":129},{"hierarchy":{"lvl1":"Generics"},"type":"lvl1","url":"/2019-09-24-generics","position":0},{"hierarchy":{"lvl1":"Generics"},"content":"","type":"content","url":"/2019-09-24-generics","position":1},{"hierarchy":{"lvl1":"Generics","lvl2":"Wildcards"},"type":"lvl2","url":"/2019-09-24-generics#wildcards","position":2},{"hierarchy":{"lvl1":"Generics","lvl2":"Wildcards"},"content":"To make up for the lack of variance, Java has a feature called wildcards, in which question marks are used as type arguments. The type LList<?> represents an object that is an LList<T> for some type T, though precisely which type T is not known at compile time (or for that matter, even at run time).\n\nA value of type LList<T> (for any T) can be used as if it had type LList<?>, so there is a family of subtyping relationships LList<T> <: LList<?>. This means that a method can provide a caller with a list of any type without the client knowing what is really stored in the list; the client can get elements from the list but cannot change the list:LList<?> f() {\n    LList<Integer> i = new LList();\n    i.add(2);\n    i.add(3);\n    i.add(5);\n    return i;\n}\n\n// in caller\nLList<?> lst = f();\nlst.add(7); // illegal: type ? not known\u0001\nfor (Object o : lst) {\n    println(o);\n}\n\nNote that the type of the elements iterated over is not really known either, but at least we know that the type hidden by ? is a subtype of Object. So it is type-safe to declare the variable o as an Object.\n\nIf we need to know more about the type hidden by the question mark, it is possible to add an extends clause. For example, suppose we have an interface Animal with two implementing classes Elephant and Rhino. Then the type Collection<? extends Animal> is a supertype of both Collection<Elephant> and Collection<Rhino>, and we can iterate over the collection and extract Animals rather than just Objects.Collection<? extends Animal> c = new LList<Rhino>();\nfor (Animal a : c) { \n    // use a as Animal here\u0001\n}","type":"content","url":"/2019-09-24-generics#wildcards","position":3},{"hierarchy":{"lvl1":"Value Representation, Hashing, and Generics"},"type":"lvl1","url":"/2019-09-30-value-representation-hashing-and-generi","position":0},{"hierarchy":{"lvl1":"Value Representation, Hashing, and Generics"},"content":"","type":"content","url":"/2019-09-30-value-representation-hashing-and-generi","position":1},{"hierarchy":{"lvl1":"Value Representation, Hashing, and Generics","lvl2":"Subtyping"},"type":"lvl2","url":"/2019-09-30-value-representation-hashing-and-generi#subtyping","position":2},{"hierarchy":{"lvl1":"Value Representation, Hashing, and Generics","lvl2":"Subtyping"},"content":"Like other implements declarations, the declaration above that LList<T> implements Collection<T> generates a subtype relationship: in fact, a family of subtype relationships, because the subtype relationship holds regardless of what actual type T is chosen. The compiler therefore understands that the relationship LList<String> <: Collection<String> holds. What about these other possible relationships?\n\nLList<String> <: LList<Object> ?\n\nLList<String> <: Collection<Object> ?\n\nBoth of these look reasonable at first glance. But they are actually unsound, leading to possible run-time type errors. The following example shows the problem:LList<String> ls = new LList<String>();\nLList<Object> lo = ls;\nlo.add(2112);\nString s = ls.head(); // extract data from head of list\u0001\n\nThe head element of the list, which is assigned to a variable of type String, is actually an Integer! This is erroneous, so the Java compiler will not allow it. A similar situation arises with arrays, although in that case the error is unfortunately only caught at run time.String[] a = new String[1];\nObject[] b = a;\nb[0] = 2112;\nSystem.out.println(a[0]);\n\nThe idea that there can be a subtyping relationship between different instantiations of the same generic type is called variance. Variance is tricky to support in a sound way, so Java does not support variance. Other languages such as Scala do have variance.","type":"content","url":"/2019-09-30-value-representation-hashing-and-generi#subtyping","position":3},{"hierarchy":{"lvl1":"Value Representation, Hashing, and Generics","lvl2":"Wildcards"},"type":"lvl2","url":"/2019-09-30-value-representation-hashing-and-generi#wildcards","position":4},{"hierarchy":{"lvl1":"Value Representation, Hashing, and Generics","lvl2":"Wildcards"},"content":"To make up for the lack of variance, Java has a feature called wildcards, in which question marks are used as type arguments. The type LList<?> represents an object that is an LList<T> for some type T, though precisely which type T is not known at compile time (or for that matter, even at run time).\n\nA value of type LList<T> (for any T) can be used as if it had type LList<?>, so there is a family of subtyping relationships LList<T> <: LList<?>. This means that a method can provide a caller with a list of any type without the client knowing what is really stored in the list; the client can get elements from the list but cannot change the list:LList<?> f() {\n    LList<Integer> i = new LList();\n    i.add(2);\n    i.add(3);\n    i.add(5);\n    return i;\n}\n\n// in caller\u0001\nLList<?> lst = f();\nlst.add(7); // illegal: type ? not known\u0001\nfor (Object o : lst) {\n    println(o);\n}\n\nNote that the type of the elements iterated over is not really known either, but at least we know that the type hidden by ? is a subtype of Object. So it is type-safe to declare the variable o as an Object.\n\nIf we need to know more about the type hidden by the question mark, it is possible to  add an extends clause. For example, suppose we have an interface Animal with two implementing classes Elephant and Rhino. Then the type Collection<? extends Animal> is a supertype of both Collection<Elephant> and Collection<Rhino>, and we can iterate over the collection and extract Animals rather than just Objects.Collection<? extends Animal> c = new LList<Rhino>();\nfor (Animal a : c) { \n    // use a as Animal here\u0001\n}","type":"content","url":"/2019-09-30-value-representation-hashing-and-generi#wildcards","position":5},{"hierarchy":{"lvl1":"Value Representation, Hashing, and Generics","lvl2":"Limitations"},"type":"lvl2","url":"/2019-09-30-value-representation-hashing-and-generi#limitations","position":6},{"hierarchy":{"lvl1":"Value Representation, Hashing, and Generics","lvl2":"Limitations"},"content":"The way generics are actually implemented in Java is that all actual type parameters are erased at run time.  This implementation choice leads to a number of limitations of the generics mechanism in Java when in a generic context where T is a formal parameter:\n\nConstructors of T cannot be used; we cannot write new T(). The workaround for this limitation is to have an object with a factory method for creating T objects.\n\nArrays with T as elements cannot be created, either. We cannot write new T[n]  , because the type T is not known at run time and so the type T[]  cannot be installed into the object’s header. The workaround for this limitation is to use an array of type Object[] instead: T[] a = (T[]) new Object[n];\n\nThis of course creates an array that could in principle be used to store things other than T’s, but as long as we use that array through the variable a, we won’t. The compiler gives us an alarming warning when we use this trick because of the unsafe cast, but this programming idiom is fairly safe. Note that if we need to create an array of T in a context where T is known to be a subtype of some type, then the array that should be created is an array of that type, rather than of Object.\n\nSimilarly, we can’t create an array whose type includes a parameter type:HashSet<String>[] sets = new HashSet<String>[n]; // error: generic array creation\u0001\n\nThe workaround is to use a wildcard type to create the array, and dynamically cast it to the desired type:HashSet<String>[] sets = new HashSet<?>[n];\n\nWe can’t use instanceof to find out what type parameters are, because the object does not contain that information. If, for example, we create an LList<String> object, the object’s header word only records that it is an LList. So an LList<String> object that is statically typed as an Object can be tested to see if it is some kind of LList, but not whether the actual type parameter is String:Object co = new LList<String>();\n\nif (co instanceof LList<String>) ... // illegal\u0001\nif (co instanceof LList<?>)      ... // legal\u0001\nif (co instanceof LList)         ... // legal but discouraged\u0001\n\nLList<String> ls = (LList<String>) co; // legal but only partly checked\u0001\nLList<?>      ls = (LList<?>) co;      // legal\u0001\nLList<String> ls = (LList<?>) co;      // illegal\u0001\nLList<String> ls = (LList)co;          // legal but discouraged\u0001\n\nThe last four lines above illustrate how downcasts interoperate with generics. Code can cast to a type with an actual type parameter, but the type parameter is not actually checked at run time; Java takes the programmer’s word that the type parameter is correct. We can cast to a wildcard instantiation, but such a cast is not very useful if we need to use the elements at their actual type. Finally, we can cast to the raw type LList; casting to raw types is unsafe. It is essentially the same as casting to LList<?> except that Java allows a raw type to be used as if it were any particular instantiation. Raw types should be avoided when possible.","type":"content","url":"/2019-09-30-value-representation-hashing-and-generi#limitations","position":7},{"hierarchy":{"lvl1":"Value Representation, Hashing, and Generics","lvl2":"Accessing type operations"},"type":"lvl2","url":"/2019-09-30-value-representation-hashing-and-generi#accessing-type-operations","position":8},{"hierarchy":{"lvl1":"Value Representation, Hashing, and Generics","lvl2":"Accessing type operations"},"content":"What if we want to use methods of T in a generic context where T is a formal parameter? There is more than one way to do this, but in Java the most powerful approach is to provide a separate model object that knows how to perform the operations that are needed. For example, suppose we want to compare objects of type T using the compareTo method. We declare a generic interface Comparator<T>:interface Comparator<T> {\n    /** Compares x and y. Return 0 if x and y are equal, a negative number if x < y, \n     *  and a positive number if x > y.\n     */\n    int compareTo(T x, T y);\n}\n\nNow, a generic method for sorting an array takes an extra comparator parameter:/** Sort the array a in ascending order using cmp to define the ordering on the\n * elements. */\n<T> sort(T[] a, Comparator<T> cmp) {\n    ...\n    if (cmp.compareTo(a[i], a[j]) > 0) {\n        ...\n    }\n    ...\n}\n\nA class can then implement the comparator interface and be used to make the right comparator operation available to the generic code.class SCmp implements Comparator<String> {\n    @Override\n    public int compareTo(String x, String y) {\n        return x.compareTo(y);\n    }\n}\n\nString[] a = {\"z\", \"Y\", \"x\"};\nsort(a, new SCmp());\n\nNotice that here we are using String’s own compareTo operation as a model for the comparator, but we don’t have to. For example, we could have used the compareToIgnoreCase method to sort strings while ignoring the difference between upper and lower case. It turns out that we can also use Java 8’s new lambda expressions to implement the interface even more compactly. Here is how we would sort the array using a lambda expression while also ignoring case:sort(a, (x,y) -> x.compareToIgnoreCase(y));\n\nThe lambda expression (x,y) -> x.compareToIgnoreCase(y) is actually just a very convenient syntactic sugar for declaring a class like the one above and instantiating it with new.\n\nGeneric classes may need to access parameter type operations too. The typical approach is to accept the model object in constructors, then to store it in an instance variable for later use by other methods:class SortedList<T> implements Collection<T> {\n    Comparator<T> comparator;\n\n    SortedList(Comparator<T> cmp) {\n        comparator = cmp; // save model object\u0001\n        ...\n    }\n\n    boolean add(T x) {\n        ...\n        if (comparator.compareTo(x, y)) { // use model object\u0001\n            ...\n        }\n        ...\n    }\n}","type":"content","url":"/2019-09-30-value-representation-hashing-and-generi#accessing-type-operations","position":9},{"hierarchy":{"lvl1":"Parsing"},"type":"lvl1","url":"/2019-10-08-parsing","position":0},{"hierarchy":{"lvl1":"Parsing"},"content":"// E → T + E | T\nvoid parseE() throws SyntaxError {\n    parseT();\n    if (peek(\"+\")) {\n        advance();\n        parseE();\n    }\n}\n\nSimilarly, the method parseT looks for “×” to decide which production to use:// T → F × T | F\nvoid parseT() throws SyntaxError {\n    parseF();\n    if (peek(\"×\")) {\n        advance();\n        parseT();\n    }\n}\n\nAnd parseF() can decide using the first symbol it sees, assuming we have an appropriate method isNumber():// F → n | ( E )\nvoid parseF() throws SyntaxError {\n    if (isNumber(peek())) {\n        advance();\n    } else {\n        consume(\"(\");\n        parseE();\n        consume(\")\");\n    }\n}","type":"content","url":"/2019-10-08-parsing","position":1},{"hierarchy":{"lvl1":"Designing and documenting interfaces and implementations"},"type":"lvl1","url":"/2019-10-17-designing-and-documenting-interfaces-an","position":0},{"hierarchy":{"lvl1":"Designing and documenting interfaces and implementations"},"content":"From Lecture: \n\nDesigning and documenting interfaces and implementations","type":"content","url":"/2019-10-17-designing-and-documenting-interfaces-an","position":1},{"hierarchy":{"lvl1":"Designing and documenting interfaces and implementations","lvl2":"An Example of Writing Interface"},"type":"lvl2","url":"/2019-10-17-designing-and-documenting-interfaces-an#an-example-of-writing-interface","position":2},{"hierarchy":{"lvl1":"Designing and documenting interfaces and implementations","lvl2":"An Example of Writing Interface"},"content":"","type":"content","url":"/2019-10-17-designing-and-documenting-interfaces-an#an-example-of-writing-interface","position":3},{"hierarchy":{"lvl1":"Designing and documenting interfaces and implementations","lvl3":"1. Overview: what concerns?","lvl2":"An Example of Writing Interface"},"type":"lvl3","url":"/2019-10-17-designing-and-documenting-interfaces-an#id-1-overview-what-concerns","position":4},{"hierarchy":{"lvl1":"Designing and documenting interfaces and implementations","lvl3":"1. Overview: what concerns?","lvl2":"An Example of Writing Interface"},"content":"/** An n*n mutable 2048 puzzle. */\nclass Puzzle{}more","type":"content","url":"/2019-10-17-designing-and-documenting-interfaces-an#id-1-overview-what-concerns","position":5},{"hierarchy":{"lvl1":"Designing and documenting interfaces and implementations","lvl3":"2. Choose operations","lvl2":"An Example of Writing Interface"},"type":"lvl3","url":"/2019-10-17-designing-and-documenting-interfaces-an#id-2-choose-operations","position":6},{"hierarchy":{"lvl1":"Designing and documenting interfaces and implementations","lvl3":"2. Choose operations","lvl2":"An Example of Writing Interface"},"content":"creators create objects\n\nconstructors\n\nfactory methods (usually static, loose coupling; don’t expose the choice of which class is being constructed)\tpublic Puzzle(int n){}\n    public static Puzzle create(int n){}\n\nobservers(queries):\n\nprimary purpose: report the state of object\n\nno side effect (doesn’t change anything)\n\nmutators (command):\n\nprimary purpose: side effect on the state of the object\n\nonly makes sense when the object is mutable\n\nshould maintain class invariants\n\nCommand-Query Separation: You have to decide whether this class belongs to command or query, don’t write both of these things in a single method\n\nProblem with getters:\n\nrepresentation exposure in Mutable Objects (sometimes you directly return the reference to this object and now doing modification to the returned object and unintendedly change the original object)","type":"content","url":"/2019-10-17-designing-and-documenting-interfaces-an#id-2-choose-operations","position":7},{"hierarchy":{"lvl1":"Designing and documenting interfaces and implementations","lvl3":"3. Write Specifications:","lvl2":"An Example of Writing Interface"},"type":"lvl3","url":"/2019-10-17-designing-and-documenting-interfaces-an#id-3-write-specifications","position":8},{"hierarchy":{"lvl1":"Designing and documenting interfaces and implementations","lvl3":"3. Write Specifications:","lvl2":"An Example of Writing Interface"},"content":"Returns/Creates: postcondition\n\nRequires/Checks: precondition\n\nEffects/Modifies: side effects\n\nExamples: (as needed)\n\nExceptions:\n\nin return clause: when there is a client error (not programmer error)\n- in checks clause: when precondition violated/** Returns: the puzzle size n*/\nint size();\n\n/** Effects: adds a random tile to the board*/\nvoid addRandomTile();\n\n/** Effects: adds a random tile to the board\n\tReturns: true if there was room*/\nboolean addRandomTile();\n\n/** Effects: adds a random tile to the board\n\tThrows: BoardFull if there is no room*/\nvoid addRandomTile() throws BoardFull;\n\n/** Effects: shifts the tiles in direction d*/\nvoid shiftTile(Direction d);","type":"content","url":"/2019-10-17-designing-and-documenting-interfaces-an#id-3-write-specifications","position":9},{"hierarchy":{"lvl1":"Designing and documenting interfaces and implementations","lvl3":"4. Documenting Impls:","lvl2":"An Example of Writing Interface"},"type":"lvl3","url":"/2019-10-17-designing-and-documenting-interfaces-an#id-4-documenting-impls","position":10},{"hierarchy":{"lvl1":"Designing and documenting interfaces and implementations","lvl3":"4. Documenting Impls:","lvl2":"An Example of Writing Interface"},"content":"Audience: maintainers, not clients\n\nGoals: keep impl details out of the spec (abstraction barrier)\n\nRepresents: an abstraction function: concrete fields -> client view/** A rational number */\nclass Rational {\n    int num,dem;\n    //Represents: the rational number num/dem\n}\n\nClass Invariant:\n\nSpec for private/protected methods\n\nAlgorithms Explanations: (If sometimes the spec is not enough to understand the method(specific algorithm), you should write algorithm explanations inside the method(This would be something client doesn’t have to know but maintainers may want to know so you don’t write them in the spec))\n\nWrite paragraphs instead of interleave comments/** revtrieves the greatest common demoninator of x and y*/\nint gcd(x,y){\n    /**This method uses Euclid's to find the ...    */\n}","type":"content","url":"/2019-10-17-designing-and-documenting-interfaces-an#id-4-documenting-impls","position":11},{"hierarchy":{"lvl1":"Design Pattern"},"type":"lvl1","url":"/2019-10-24-design-pattern","position":0},{"hierarchy":{"lvl1":"Design Pattern"},"content":"Say A is the producer, B is the consumer","type":"content","url":"/2019-10-24-design-pattern","position":1},{"hierarchy":{"lvl1":"Design Pattern","lvl2":"Iterator Pattern"},"type":"lvl2","url":"/2019-10-24-design-pattern#iterator-pattern","position":2},{"hierarchy":{"lvl1":"Design Pattern","lvl2":"Iterator Pattern"},"content":"Pull pattern: B pull values  from AA implements Iterable<T>{\n    Iterable<T> iterator();\n}\n\nB uses Iterator methods{\n    boolean hasNext();\n    T next();\n}\n\nUsing iterator is easy\n\nImplementing it is difficult\n\niterator should remember its position (or you don’t really know whether it hasNext)\n\nunderlying data structure can’t be mutated(what if the Next is deleted?)\n\nneed version number on every node of the data structure\n\nIf the version number is changed, it will throw ConcurrentMod(ification)Exception\n\nUsed when you have different data type of sources to be pulled from, so you don’t want to pull directly the hashTable or arrayList they use to iterate those values. You want something more generic. Here comes the iterator. Java has its own iterator method. You can create iterator from ArrayList or hashTable.\n\n​","type":"content","url":"/2019-10-24-design-pattern#iterator-pattern","position":3},{"hierarchy":{"lvl1":"Design Pattern","lvl2":"Observer Pattern"},"type":"lvl2","url":"/2019-10-24-design-pattern#observer-pattern","position":4},{"hierarchy":{"lvl1":"Design Pattern","lvl2":"Observer Pattern"},"content":"Push Pattern: A pushes values to Binterface Observer<T>{\n\tvoid notify(T elem);\n}\n\ninterface Obeservable<T>{\n\tvoid register Observer(Observer<T> obs);\n}\n","type":"content","url":"/2019-10-24-design-pattern#observer-pattern","position":5},{"hierarchy":{"lvl1":"Design Pattern","lvl2":"Streams"},"type":"lvl2","url":"/2019-10-24-design-pattern#streams","position":6},{"hierarchy":{"lvl1":"Design Pattern","lvl2":"Streams"},"content":"“pull” with transformersList<T> l;\nl.stream(); //read in a strem of T\nl.filter(predicate); //filter out all predicates in T\nl.map(function); //get a Stream<Integer> if function: Function<T,Integer>\n\nWhen you want to pull somethingfindFirst returns Optimal<T>\n\n\nOptional<T> of(T)//converts T to Optional <T>\nOptional<T> ofNullible(T) // if T is empty, it returns an empty Optional<T>","type":"content","url":"/2019-10-24-design-pattern#streams","position":7},{"hierarchy":{"lvl1":"Building GUI"},"type":"lvl1","url":"/2019-10-29-event-handlers","position":0},{"hierarchy":{"lvl1":"Building GUI"},"content":"","type":"content","url":"/2019-10-29-event-handlers","position":1},{"hierarchy":{"lvl1":"Building GUI","lvl2":"Event Handlers"},"type":"lvl2","url":"/2019-10-29-event-handlers#event-handlers","position":2},{"hierarchy":{"lvl1":"Building GUI","lvl2":"Event Handlers"},"content":"interface EventHandler<T>{\n    void handle(T event);\n}\nclass MyHandler implemens Evethandler<ActionEvent>{\n    void handle (ActionEvent e){\n        print(\"click\");\n    }\n}","type":"content","url":"/2019-10-29-event-handlers#event-handlers","position":3},{"hierarchy":{"lvl1":"Concurrency"},"type":"lvl1","url":"/2019-11-07-concurrency","position":0},{"hierarchy":{"lvl1":"Concurrency"},"content":"From Lecture: \n\nConcurrency","type":"content","url":"/2019-11-07-concurrency","position":1},{"hierarchy":{"lvl1":"Concurrency","lvl2":"Concurrency and Parallelism"},"type":"lvl2","url":"/2019-11-07-concurrency#concurrency-and-parallelism","position":2},{"hierarchy":{"lvl1":"Concurrency","lvl2":"Concurrency and Parallelism"},"content":"Concurrency: multiple threads (java level, can be executed by one core or multiple core)\n\nparallelism: multiple cores (hardware level. can execute one thread or multiple thread)moreimport java.lang.Thread;\n\nclass Thread{\n    /** starts a new thread executing run() */\n    void start();\n    \n    /** Effect: anything; but default does nothing*/\n    void run();\n}\n\nrun() 里面就是写的这个thread到底应该干什么","type":"content","url":"/2019-11-07-concurrency#concurrency-and-parallelism","position":3},{"hierarchy":{"lvl1":"Concurrency","lvl2":"Threads Interference"},"type":"lvl2","url":"/2019-11-07-concurrency#threads-interference","position":4},{"hierarchy":{"lvl1":"Concurrency","lvl2":"Threads Interference"},"content":"Best approach: most objects owned by 1 thread\n\nRead-Only sharing ok\n\nRead/Write sharing | Write/Write sharing dangerousclass Account {\n  int balance;\n  void withdraw(int n) {\n   int b = balance - n; // R1\u0001\n   balance = b;         // W1\u0001\n  }\n  void deposit(int n) {\n   int b = balance + n; // R2\u0001\n   balance = b;         // W2\u0001\n  }\n}\n\ne.g. initial balance: $100, T1 executes witdraw(50), T2 executes deposit(50)\n\n(R1, W1, R2, W2) or (R2, W2, R1, W1), the final balance is indeed $100.\n\n(R1, R2, W2, W1) destroys $50 -> $50\n\n(R2, R1, W1, W2) creates ​$50 -> $150\n\nWe therefore want deposit, withdraw method to be (or at least act to be) atomic (indivisible), as if this sentence cannot be influenced by other threads (reading and writing are happening simultaneously) .","type":"content","url":"/2019-11-07-concurrency#threads-interference","position":5},{"hierarchy":{"lvl1":"Concurrency","lvl2":"Mutex"},"type":"lvl2","url":"/2019-11-07-concurrency#mutex","position":6},{"hierarchy":{"lvl1":"Concurrency","lvl2":"Mutex"},"content":"(mutual exclusion locks)\n\nThreads can acquire them and release them. At most one thread can hold a mutex at a time. While a mutex is being held by a thread, all other threads that try to acquire it will be blocked until it is released, at which point just one waiting thread will manage to acquire it.\n\nevery write and read to a shared mutable variable, mutex must be held/** Effect: blocks current thread until mutex is held by another thread\n\t\t\tthen acquires mutex */\nacquire();\n\n/** Effect: release mutex */\nrelease();\nsynchronized(Object o){\n    //acquire o's mutex\n    ... // do some operations\n    //release o's mutex\n}\n\nTake deposit & withdraw as an example:void withdraw(int n) {\n   //do something\n   synchronized(this) {\n      //acquire o's mutex\n      balance -= n;\n      //release o's mutex\n   }\n   //do something else\n}\n\nBecause the pattern of wrapping entire method bodies in synchronized(this) is so common, Java has syntactic sugar for it:synchronized void withdraw(int n) {\n   balance -= n;\n}\n\nWhenever mutex is not held, that variable guarded by the mutex will change unpredictably and the invariant will not hold\n\nas long as we called synchronized someMethodName() or synchronized(this). Everything in this object/class this will be held mutex (?). So we want to write small methods and small classes. As mentioned previously:\n\nBest approach: most objects owned by 1 thread","type":"content","url":"/2019-11-07-concurrency#mutex","position":7},{"hierarchy":{"lvl1":"Concurrency","lvl2":"Conclusion"},"type":"lvl2","url":"/2019-11-07-concurrency#conclusion","position":8},{"hierarchy":{"lvl1":"Concurrency","lvl2":"Conclusion"},"content":"avoid unnecessary concurrency: slows down the program\n\nlimit sharing of mutable state: we don’t have to care about concurrency if its immutable(read only)\n\nguard all accesses to shared mutable state with mutexes: in case of thread interference","type":"content","url":"/2019-11-07-concurrency#conclusion","position":9},{"hierarchy":{"lvl1":"Synchronization"},"type":"lvl1","url":"/2019-11-12-synchronization","position":0},{"hierarchy":{"lvl1":"Synchronization"},"content":"From Lecture: \n\nSynchronization","type":"content","url":"/2019-11-12-synchronization","position":1},{"hierarchy":{"lvl1":"Synchronization","lvl2":"Monitor"},"type":"lvl2","url":"/2019-11-12-synchronization#monitor","position":2},{"hierarchy":{"lvl1":"Synchronization","lvl2":"Monitor"},"content":"idea: object state guarded by its mutex\n\nmonitor is just a class, whose all public methods “synchronize,” which means you can’t access state without holding mutex\n\nPrincipal: let short methods hold the mutex in case it doesn’t affect performancemoreT1: add(elem)\nT2: size(); contains(elem);\n\nwhen elem is added, it may not be seen in size. So add should be written as synchronized add (T elem) {}","type":"content","url":"/2019-11-12-synchronization#monitor","position":3},{"hierarchy":{"lvl1":"Synchronization","lvl3":"Locks & Deadlock","lvl2":"Monitor"},"type":"lvl3","url":"/2019-11-12-synchronization#locks-deadlock","position":4},{"hierarchy":{"lvl1":"Synchronization","lvl3":"Locks & Deadlock","lvl2":"Monitor"},"content":"Lock is achieved by a mutex\n\ndeadlocks happen when all threads end up being blocked by a mutex","type":"content","url":"/2019-11-12-synchronization#locks-deadlock","position":5},{"hierarchy":{"lvl1":"Synchronization","lvl4":"e.g.","lvl3":"Locks & Deadlock","lvl2":"Monitor"},"type":"lvl4","url":"/2019-11-12-synchronization#e-g","position":6},{"hierarchy":{"lvl1":"Synchronization","lvl4":"e.g.","lvl3":"Locks & Deadlock","lvl2":"Monitor"},"content":"class a{\n    synchronized f(){ b.g();}\n}\nclass b{\n    synchronized g() { a.f() }\n}T1: a.f() ---> b.g()\nT2: b.g() ---> a.f()\n\nSay we first go into T1, we acquire a; Then before going into b.g() in T1, we acquire b.g() in T2. Then we get into a deadlock cause we can’t get b.g() in a.f() nor vice versa.\n\nThat’s because we wrote a lock acquisition order in cycle","type":"content","url":"/2019-11-12-synchronization#e-g","position":7},{"hierarchy":{"lvl1":"Synchronization","lvl4":"Solution:","lvl3":"Locks & Deadlock","lvl2":"Monitor"},"type":"lvl4","url":"/2019-11-12-synchronization#solution","position":8},{"hierarchy":{"lvl1":"Synchronization","lvl4":"Solution:","lvl3":"Locks & Deadlock","lvl2":"Monitor"},"content":"no cycle\n\nwrite an order in which to acquire mutex\n\ne.g. a<b: can’t acquire a after b (which makes b.g() illegal)\n\nAnd then write a spec /** Requires: lock level < a */\n\nacquire mutex in the same order (T1: A,B; T2: A,B instead of T2: B,A)","type":"content","url":"/2019-11-12-synchronization#solution","position":9},{"hierarchy":{"lvl1":"Synchronization","lvl2":"Barriers"},"type":"lvl2","url":"/2019-11-12-synchronization#barriers","position":10},{"hierarchy":{"lvl1":"Synchronization","lvl2":"Barriers"},"content":"in scientific computation, there are many computations embarrassingly parallel\n\nspan N threads and wait until they are all done\n\nupdates before barrier will be seen by all threads after the barrierb = new CyclicBarrier(N); // N is the number of threads(without R/W sharing)\n// after each thread finishes executing, it will wait(), until all threads are done\n// (the barrier blocks everything until N await() are called)","type":"content","url":"/2019-11-12-synchronization#barriers","position":11},{"hierarchy":{"lvl1":"Synchronization","lvl2":"Blocking Abstractions"},"type":"lvl2","url":"/2019-11-12-synchronization#blocking-abstractions","position":12},{"hierarchy":{"lvl1":"Synchronization","lvl2":"Blocking Abstractions"},"content":"How to build your own threads-blocking abstractions?","type":"content","url":"/2019-11-12-synchronization#blocking-abstractions","position":13},{"hierarchy":{"lvl1":"Synchronization","lvl3":"e.g.","lvl2":"Blocking Abstractions"},"type":"lvl3","url":"/2019-11-12-synchronization#e-g-1","position":14},{"hierarchy":{"lvl1":"Synchronization","lvl3":"e.g.","lvl2":"Blocking Abstractions"},"content":"/** computes two threads seperately and then add them together*/\nclass WorkerPair extends Runnable\n{\n    int done=0; // # threads done (0~2)\n    Object result;\n    \n    WorkerPair(){\n        new Thread(this).start();\n        new Thread(this).start();\n    }\n    \n    public void run(){\n    \trealDoWork(); //TODO implement this\n    \tsynchronized(this){ // Why not put a synchronized around run()? because then only one thread will really realDoWork();\n    \t\tdone++;\n    \t\tresult = something;\n    \t}\n    }\n    \n    Object getResult(){\n    \twhile(done<2){...}\n    \treturn result;\n    }\n    \n}","type":"content","url":"/2019-11-12-synchronization#e-g-1","position":15},{"hierarchy":{"lvl1":"Synchronization","lvl3":"Condition Variables","lvl2":"Blocking Abstractions"},"type":"lvl3","url":"/2019-11-12-synchronization#condition-variables","position":16},{"hierarchy":{"lvl1":"Synchronization","lvl3":"Condition Variables","lvl2":"Blocking Abstractions"},"content":"allow you to block things until some condition is true\n\na condition variable is always associated with a mutex\n\nevery Java object has its own condition variable, which ties to its own mutex\n\nA notifyAll() is sent whenever any of the conditions may become true; threads awakened by notifyAll() then test to see if their particular condition has become true; otherwise, they go back to sleep./** Effect: blocks thread and releases mutex, wait for some condition to become true\n(note: why should wait() release mutex? Becuase when the program enters this waiting thread, this thread first acquires mutex, so when it begins to wait, it has to release it so other threads can do their work and finally wake it up after done)\n\tRequires: mutex is held*/\nvoid wait();\n\n/** Effect: return all wait() method (Unblocks/Wake up all threads that are waiting)\n\tRequires: mutex held*/\nvoid notifyAll();\n\nTo block until condition is true:public void run(){\n    realDoWork();\n    synchronized(this){ \n    \tdone++;\n    \tresult = something;\n        notifyAll();\n    }\n}\n\nsynchronized Object getResult(){\n    while(done<2) wait();\n    return result;\n}\n\nwhile (!condition) wait(); is the most usual way to write a condition variable\n\nwait(): wait for some condition to be true, which is determined by other threads\n\nnotifyAll() called after whatever some of the operations already done can probably wake some method up","type":"content","url":"/2019-11-12-synchronization#condition-variables","position":17},{"hierarchy":{"lvl1":"Graph Traversal"},"type":"lvl1","url":"/2019-11-19-graph-traversal","position":0},{"hierarchy":{"lvl1":"Graph Traversal"},"content":"From Lecture: \n\nGraph traversals","type":"content","url":"/2019-11-19-graph-traversal","position":1},{"hierarchy":{"lvl1":"Graph Traversal","lvl2":"Tricolor Algorithm"},"type":"lvl2","url":"/2019-11-19-graph-traversal#tricolor-algorithm","position":2},{"hierarchy":{"lvl1":"Graph Traversal","lvl2":"Tricolor Algorithm"},"content":"It is a general model of many graph traversal algorithms.\n\nInvariant: there is no black->white edgemore1. color all nodes white\n2. color roots gray\n3. while(some gray node g){\n\tcolor g's white sucessors grey\n\tif (no more white successors){\n\t\tcolor g black;\n\t}\n}\n\nAt the end of this algorithm, there will be no grey nodes in the graph. All reachable nodes are black, while all unreachable nodes are white.\n\nProperties unique to different tricolor algorithm:\n\nwhich successor in which order to color grey?\n\nwhether and when to color g black","type":"content","url":"/2019-11-19-graph-traversal#tricolor-algorithm","position":3},{"hierarchy":{"lvl1":"Graph Traversal","lvl2":"BFS"},"type":"lvl2","url":"/2019-11-19-graph-traversal#bfs","position":4},{"hierarchy":{"lvl1":"Graph Traversal","lvl2":"BFS"},"content":"Gray Frontier: FIFO queue\n\nv.distance = length of shortest path from root to v OR infinity if no path yet\n\nv.color==black means v.dist!=infinity  & v is not in queue\n\nset v.distance = infinity for all v\n\nset root.distance = 0, queue.push(root) (root.color=gray)\n\nwhile(!frontier.empty()){\n\tNode v=frontier.pop();\n\tfor (Node v>g) // v is g's successor\n        if(v.dist==infinity){ // v.color==white\n            v.distance = g.distance+1;\n            frontier.push(v);\n        }\n    //g is now black: g.color = black\n}\n","type":"content","url":"/2019-11-19-graph-traversal#bfs","position":5},{"hierarchy":{"lvl1":"Graph Traversal","lvl2":"DFS"},"type":"lvl2","url":"/2019-11-19-graph-traversal#dfs","position":6},{"hierarchy":{"lvl1":"Graph Traversal","lvl2":"DFS"},"content":"use LIFO stack\n\nOR use recursion","type":"content","url":"/2019-11-19-graph-traversal#dfs","position":7},{"hierarchy":{"lvl1":"Graph Traversal","lvl3":"Recursion:","lvl2":"DFS"},"type":"lvl3","url":"/2019-11-19-graph-traversal#recursion","position":8},{"hierarchy":{"lvl1":"Graph Traversal","lvl3":"Recursion:","lvl2":"DFS"},"content":"/** Effect: mark all nodes black that\n\tare reachable from v on all white paths.\n\tRequires: v is gray. */\nvoid visit(Node g) {\n    for (Node v>g){\n        if(v.color==white){\n            v.color = gray;\n            visit(v);\n        }\n    }\n    g.color = black;\n}\n\ncycle detection: grey -> grey\n\nTherefore, if we reach a gray successor in DFS, we found a cycle\n\nDFS therefore gives linear-time cycle detection\n\ntopological sort:\n\nDFS runs in a postorder traversal way: Node finishes after all descendants/dependents finished\n\ntoposort = reversal postorder traversal","type":"content","url":"/2019-11-19-graph-traversal#recursion","position":9},{"hierarchy":{"lvl1":"Graph (Recitation)"},"type":"lvl1","url":"/2019-11-20-graph-recitation","position":0},{"hierarchy":{"lvl1":"Graph (Recitation)"},"content":"From Recitation: \n\nEuler paths, planar graphs, and Hamiltonian paths","type":"content","url":"/2019-11-20-graph-recitation","position":1},{"hierarchy":{"lvl1":"Graph (Recitation)","lvl2":"Edge Classification"},"type":"lvl2","url":"/2019-11-20-graph-recitation#edge-classification","position":2},{"hierarchy":{"lvl1":"Graph (Recitation)","lvl2":"Edge Classification"},"content":"We have an edge e=(v,child), where v is always grey\n\nTree edge:if (child.color==white)\n\te = TreeEdge;more\n\nBack edge:if (child.color==grey)\n    e = BackEdge;\n\nForward edge:if (child.color==black && v.timeStamp<child.timeStamp)\n    e = ForwardEdge;\n\nCross edge:if (child.color==black && v.timeStamp > child.timeStamp)\n    e = ForwardEdge;","type":"content","url":"/2019-11-20-graph-recitation#edge-classification","position":3},{"hierarchy":{"lvl1":"shortest path algorithm"},"type":"lvl1","url":"/2019-11-21-shortest-path-algorithm","position":0},{"hierarchy":{"lvl1":"shortest path algorithm"},"content":"","type":"content","url":"/2019-11-21-shortest-path-algorithm","position":1},{"hierarchy":{"lvl1":"shortest path algorithm","lvl2":"Dijkstra"},"type":"lvl2","url":"/2019-11-21-shortest-path-algorithm#dijkstra","position":2},{"hierarchy":{"lvl1":"shortest path algorithm","lvl2":"Dijkstra"},"content":"white: dist == infinity\n\ngray: dist < infinity, in queue\n\nblack: dist < infinity, not in queueroot.dist = 0; //root is the source\nfrontier.push(root); //root is now grey\nwhile(!frontier.empty()){\n    Node g = frontier.pop();\n    for (g -> v){\n        if(v.dist==infinity){ //found a new white node\n            v.dist = g.dist + d;\n            frontier.push(v);\n        }\n        else{ // v is grey\n            v.dist = min(v.dist, g.dist+d);\n        }\n    }\n    //g is now black\n}\n\nv1 and v2 is on the best path iff v1.dist+d=v2.dist\n\nTherefore, if we want to get the shortest path, we can use this algorithm to walk straight back to the graph from destination.","type":"content","url":"/2019-11-21-shortest-path-algorithm#dijkstra","position":3},{"hierarchy":{"lvl1":"shortest path algorithm","lvl3":"Performance","lvl2":"Dijkstra"},"type":"lvl3","url":"/2019-11-21-shortest-path-algorithm#performance","position":4},{"hierarchy":{"lvl1":"shortest path algorithm","lvl3":"Performance","lvl2":"Dijkstra"},"content":"Total: O((V+E)lgV)\n\nouter loop: O(V lgV)\n\nonce per vertex, O(V) iterations\n\nfinding min element in the queue once: O(lg V) - BST or heap\n\ninneer loop: O(E lgV)\n\nO(E) iterations\n\ntime to push/update priority queue: O(lg V)","type":"content","url":"/2019-11-21-shortest-path-algorithm#performance","position":5},{"hierarchy":{"lvl1":"shortest path algorithm","lvl3":"Correctness","lvl2":"Dijkstra"},"type":"lvl3","url":"/2019-11-21-shortest-path-algorithm#correctness","position":6},{"hierarchy":{"lvl1":"shortest path algorithm","lvl3":"Correctness","lvl2":"Dijkstra"},"content":"","type":"content","url":"/2019-11-21-shortest-path-algorithm#correctness","position":7},{"hierarchy":{"lvl1":"shortest path algorithm","lvl4":"Loop Invariant:","lvl3":"Correctness","lvl2":"Dijkstra"},"type":"lvl4","url":"/2019-11-21-shortest-path-algorithm#loop-invariant","position":8},{"hierarchy":{"lvl1":"shortest path algorithm","lvl4":"Loop Invariant:","lvl3":"Correctness","lvl2":"Dijkstra"},"content":"For every black node b, gray g: b.dist<=g.dist\n\nv.dist is length of shortest interior path to v (or infinity if none) (interior path means it stays in the black region, which means it only uses black nodes)","type":"content","url":"/2019-11-21-shortest-path-algorithm#loop-invariant","position":9},{"hierarchy":{"lvl1":"shortest path algorithm","lvl4":"Establishment:","lvl3":"Correctness","lvl2":"Dijkstra"},"type":"lvl4","url":"/2019-11-21-shortest-path-algorithm#establishment","position":10},{"hierarchy":{"lvl1":"shortest path algorithm","lvl4":"Establishment:","lvl3":"Correctness","lvl2":"Dijkstra"},"content":"no black nodes","type":"content","url":"/2019-11-21-shortest-path-algorithm#establishment","position":11},{"hierarchy":{"lvl1":"shortest path algorithm","lvl4":"PostCondition:","lvl3":"Correctness","lvl2":"Dijkstra"},"type":"lvl4","url":"/2019-11-21-shortest-path-algorithm#postcondition","position":12},{"hierarchy":{"lvl1":"shortest path algorithm","lvl4":"PostCondition:","lvl3":"Correctness","lvl2":"Dijkstra"},"content":"no gray nodes\n\nall the nodes are black -> All paths are interior path -> shortest: interior path is actual answer","type":"content","url":"/2019-11-21-shortest-path-algorithm#postcondition","position":13},{"hierarchy":{"lvl1":"shortest path algorithm","lvl4":"Preservation:","lvl3":"Correctness","lvl2":"Dijkstra"},"type":"lvl4","url":"/2019-11-21-shortest-path-algorithm#preservation","position":14},{"hierarchy":{"lvl1":"shortest path algorithm","lvl4":"Preservation:","lvl3":"Correctness","lvl2":"Dijkstra"},"content":"不写了，👴 后面看不懂了。","type":"content","url":"/2019-11-21-shortest-path-algorithm#preservation","position":15},{"hierarchy":{"lvl1":"shortest path algorithm","lvl2":"Application:"},"type":"lvl2","url":"/2019-11-21-shortest-path-algorithm#application","position":16},{"hierarchy":{"lvl1":"shortest path algorithm","lvl2":"Application:"},"content":"if you have something like condition probability which has to be multiplied to produce the correct result. But so you want to know the shortest probability there. You can just take the log of probability.\n\nFor example, suppose that we had a state machine that transitioned to new states with some given probability on each outgoing edge. The probability of taking a particular path through the graph would then be the product of the probabilities on edges along that path. We can then answer questions such as, “What is the most likely path that the system will follow in order to arrive at a given state?” by solving a shortest path problem in a weighted graph in which the weights are the negative logarithms of the probabilities, since (−log a) + (−log b) = −log ab.","type":"content","url":"/2019-11-21-shortest-path-algorithm#application","position":17},{"hierarchy":{"lvl1":"Udacity: Android Basics"},"type":"lvl1","url":"/2019-11-23-android-basics-user-interface","position":0},{"hierarchy":{"lvl1":"Udacity: Android Basics"},"content":"","type":"content","url":"/2019-11-23-android-basics-user-interface","position":1},{"hierarchy":{"lvl1":"Udacity: Android Basics","lvl2":"User Interface"},"type":"lvl2","url":"/2019-11-23-android-basics-user-interface#user-interface","position":2},{"hierarchy":{"lvl1":"Udacity: Android Basics","lvl2":"User Interface"},"content":"","type":"content","url":"/2019-11-23-android-basics-user-interface#user-interface","position":3},{"hierarchy":{"lvl1":"Udacity: Android Basics","lvl3":"Images & Texts","lvl2":"User Interface"},"type":"lvl3","url":"/2019-11-23-android-basics-user-interface#images-texts","position":4},{"hierarchy":{"lvl1":"Udacity: Android Basics","lvl3":"Images & Texts","lvl2":"User Interface"},"content":"wrap content: android:layout_height=\"wrap_content\"\n\ndp: density-independent pixel\n\ntext size: android:textsize=\"20sp\"\n\nsp: scalar-independent pixel\n\nbackground and text color: android:background=\"#66ffcc\" android:textColor=\"66ccff\"more","type":"content","url":"/2019-11-23-android-basics-user-interface#images-texts","position":5},{"hierarchy":{"lvl1":"Udacity: Android Basics","lvl3":"View Groups","lvl2":"User Interface"},"type":"lvl3","url":"/2019-11-23-android-basics-user-interface#view-groups","position":6},{"hierarchy":{"lvl1":"Udacity: Android Basics","lvl3":"View Groups","lvl2":"User Interface"},"content":"","type":"content","url":"/2019-11-23-android-basics-user-interface#view-groups","position":7},{"hierarchy":{"lvl1":"Udacity: Android Basics","lvl4":"Linear Layout","lvl3":"View Groups","lvl2":"User Interface"},"type":"lvl4","url":"/2019-11-23-android-basics-user-interface#linear-layout","position":8},{"hierarchy":{"lvl1":"Udacity: Android Basics","lvl4":"Linear Layout","lvl3":"View Groups","lvl2":"User Interface"},"content":"linear layout: similar to Hbox and Vbox in javaFX, one linear layout can contain many other linear layouts\n\nweight values: give different weights to layouts to determine what proportion of the space they should take in the screen","type":"content","url":"/2019-11-23-android-basics-user-interface#linear-layout","position":9},{"hierarchy":{"lvl1":"Udacity: Android Basics","lvl4":"Relative Layout","lvl3":"View Groups","lvl2":"User Interface"},"type":"lvl4","url":"/2019-11-23-android-basics-user-interface#relative-layout","position":10},{"hierarchy":{"lvl1":"Udacity: Android Basics","lvl4":"Relative Layout","lvl3":"View Groups","lvl2":"User Interface"},"content":"relative layout to parent:android:layout_align_Parent(Top/Bottom/Left/Right) = \"true/false\"; android:layout_center(Horizontal/Vertical)=\"true\"\n\nassigning ID: android:id = \"@+id/ben_textview\" (used “+” because we added a new ID to the library)\n\nreferring to ID: android:id = \"@id/ben_textview\" (no longer need the “+” here)\n\nrelative layout to others: android:layout_to(Right/Left)Of=\"@id/pic\"","type":"content","url":"/2019-11-23-android-basics-user-interface#relative-layout","position":11},{"hierarchy":{"lvl1":"Udacity: Android Basics","lvl2":"Multiscreen Apps"},"type":"lvl2","url":"/2019-11-23-android-basics-user-interface#multiscreen-apps","position":12},{"hierarchy":{"lvl1":"Udacity: Android Basics","lvl2":"Multiscreen Apps"},"content":"","type":"content","url":"/2019-11-23-android-basics-user-interface#multiscreen-apps","position":13},{"hierarchy":{"lvl1":"Udacity: Android Basics","lvl3":"Add texts to the Layout","lvl2":"Multiscreen Apps"},"type":"lvl3","url":"/2019-11-23-android-basics-user-interface#add-texts-to-the-layout","position":14},{"hierarchy":{"lvl1":"Udacity: Android Basics","lvl3":"Add texts to the Layout","lvl2":"Multiscreen Apps"},"content":"","type":"content","url":"/2019-11-23-android-basics-user-interface#add-texts-to-the-layout","position":15},{"hierarchy":{"lvl1":"Udacity: Android Basics","lvl4":"with List","lvl3":"Add texts to the Layout","lvl2":"Multiscreen Apps"},"type":"lvl4","url":"/2019-11-23-android-basics-user-interface#with-list","position":16},{"hierarchy":{"lvl1":"Udacity: Android Basics","lvl4":"with List","lvl3":"Add texts to the Layout","lvl2":"Multiscreen Apps"},"content":"ArrayList<String> words;\n\n//findViewById only returns a View, we should downcast it to a LinearLayout\nLinearLayout rootView = (LinearLayout) findViewById(R.id.rootView);\n\nfor(String word:words){\n    //TextView takes into a context, in this case is \"this\"\n    TextView wordView = new TextView(this);\n    wordView.setText(word);\n    \n    //add wordView to the this View of List(LinearLayout)\n    rootView.addView(wordView);\n}\n\nHowever, a LinearLayout is not scrollable and can easily go beyond the screen. Therefore, we should use an ArrayAdapter","type":"content","url":"/2019-11-23-android-basics-user-interface#with-list","position":17},{"hierarchy":{"lvl1":"Udacity: Android Basics","lvl4":"with ArrayAdapter","lvl3":"Add texts to the Layout","lvl2":"Multiscreen Apps"},"type":"lvl4","url":"/2019-11-23-android-basics-user-interface#with-arrayadapter","position":18},{"hierarchy":{"lvl1":"Udacity: Android Basics","lvl4":"with ArrayAdapter","lvl3":"Add texts to the Layout","lvl2":"Multiscreen Apps"},"content":"ArrayAdapter<Word> itemsAdapter = new WordAdapter(this, words);\nListView listView = (ListView) findViewById(R.id.list);\nlistView.setAdapter(itemsAdapter);\n\nWe have to create two more classes: a Word class, and a customized WordAdapter that extends ArrayAdapter<Word>. We also have to create another XML file to customize the display of one item in the list.","type":"content","url":"/2019-11-23-android-basics-user-interface#with-arrayadapter","position":19},{"hierarchy":{"lvl1":"Priority Queue and Heap"},"type":"lvl1","url":"/2019-11-26-priority-queue-and-heap","position":0},{"hierarchy":{"lvl1":"Priority Queue and Heap"},"content":"","type":"content","url":"/2019-11-26-priority-queue-and-heap","position":1},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl2":"Priority Queue"},"type":"lvl2","url":"/2019-11-26-priority-queue-and-heap#priority-queue","position":2},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl2":"Priority Queue"},"content":"interface PriorityQueue<E> {\n    \n    void increasePriority(E x, int priority)\n        \n    //constructor \n\tPriorityQueue (ElemOps<E> ops);\n}","type":"content","url":"/2019-11-26-priority-queue-and-heap#priority-queue","position":3},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl2":"Binary Heap"},"type":"lvl2","url":"/2019-11-26-priority-queue-and-heap#binary-heap","position":4},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl2":"Binary Heap"},"content":"","type":"content","url":"/2019-11-26-priority-queue-and-heap#binary-heap","position":5},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl3":"Invariant","lvl2":"Binary Heap"},"type":"lvl3","url":"/2019-11-26-priority-queue-and-heap#invariant","position":6},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl3":"Invariant","lvl2":"Binary Heap"},"content":"Order: n.priority<=n.child.priority  (小根堆)\n\nshape:\n\nAll leaves are at depth h or h-1\n\nAll h-depth are on the left\n\nThis heap is in fact a complete binary tree","type":"content","url":"/2019-11-26-priority-queue-and-heap#invariant","position":7},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl3":"Implementation","lvl2":"Binary Heap"},"type":"lvl3","url":"/2019-11-26-priority-queue-and-heap#implementation","position":8},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl3":"Implementation","lvl2":"Binary Heap"},"content":"","type":"content","url":"/2019-11-26-priority-queue-and-heap#implementation","position":9},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl4":"Swim","lvl3":"Implementation","lvl2":"Binary Heap"},"type":"lvl4","url":"/2019-11-26-priority-queue-and-heap#swim","position":10},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl4":"Swim","lvl3":"Implementation","lvl2":"Binary Heap"},"content":"if parents have bigger priority, this node should swim.","type":"content","url":"/2019-11-26-priority-queue-and-heap#swim","position":11},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl4":"Sink","lvl3":"Implementation","lvl2":"Binary Heap"},"type":"lvl4","url":"/2019-11-26-priority-queue-and-heap#sink","position":12},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl4":"Sink","lvl3":"Implementation","lvl2":"Binary Heap"},"content":"if any of the child has higher priority (smaller value), we swap this node with the one with a higher priority (the smaller one). And we only sink when there is actually something to sink (we are at the second deepest level, one level above the leaves)\n\n但是要注意，有的时候你想要sink，但是你的孩子实际上是未被初始化的一些有着随机优先值的结点，这时如果你做交换了，那就全完了。所以应当判断这个孩子结点原本在不在堆中","type":"content","url":"/2019-11-26-priority-queue-and-heap#sink","position":13},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl4":"Add","lvl3":"Implementation","lvl2":"Binary Heap"},"type":"lvl4","url":"/2019-11-26-priority-queue-and-heap#add","position":14},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl4":"Add","lvl3":"Implementation","lvl2":"Binary Heap"},"content":"We should preserve both the invariant when adding one element.\n\nshape: Ifwe add it at the end of the heap, shape invariant is preserved\n\norder: This invariant can be broken (though it’s at the end, it may not be the node with lowest priority) . So this newly added element should swim.","type":"content","url":"/2019-11-26-priority-queue-and-heap#add","position":15},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl4":"Extract","lvl3":"Implementation","lvl2":"Binary Heap"},"type":"lvl4","url":"/2019-11-26-priority-queue-and-heap#extract","position":16},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl4":"Extract","lvl3":"Implementation","lvl2":"Binary Heap"},"content":"extract the head of the heap. But we need to replace it with something to preserve the invariant.\n\nshape: The last element in the array is a good candidate. We move it to the root of the tree. This reestablishes the shape invariant\n\norder: This invariant may now be broken. We fix the heap invariant by sinking the root\n\n要注意我们做任何删除操作的时候，要确保这个元素已经不在数组中了(1. 长度-- 2. 原本的index设为LOWEST_PRIORITY 确保它永远在堆底待着)","type":"content","url":"/2019-11-26-priority-queue-and-heap#extract","position":17},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl4":"Change in Priority","lvl3":"Implementation","lvl2":"Binary Heap"},"type":"lvl4","url":"/2019-11-26-priority-queue-and-heap#change-in-priority","position":18},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl4":"Change in Priority","lvl3":"Implementation","lvl2":"Binary Heap"},"content":"shape: maintained\n\norder: we may need to bubble the element up if we increase the priority (which for a min-queue means decreasing the value) or down if we decrease the priority.\n\nYou need to know the node’s index to perform swimming and sinking. But if the node is an object, how would you know its index? We can store its index inside the object.","type":"content","url":"/2019-11-26-priority-queue-and-heap#change-in-priority","position":19},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl4":"Heapify","lvl3":"Implementation","lvl2":"Binary Heap"},"type":"lvl4","url":"/2019-11-26-priority-queue-and-heap#heapify","position":20},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl4":"Heapify","lvl3":"Implementation","lvl2":"Binary Heap"},"content":"Here is a very efficient way to turn an arbitrary array into a heap.\n\nHeapifying can be done by bubbling every element down, starting from the last element in the second last layer (叶子的前一层，叶子那一层没有可以sink的东西，或者说，叶子那一层的Heap Invariant是绝对被保存的，第一个Heap Invariant不被保存的节点是倒数第二层的第一个节点) in the array representation and working backward.\n\nThe total time required to do this is linear. At most half the elements need to be bubbled down one step (倒数第二层，叶子前一层), at most a quarter of the elements need to be bubbled down two steps (倒数第三层，叶子前两层), and so on.\n\nTime Complexity: n/2 + 2n/4 + 3n/8 + 4n/16 + ... + k^n/2k + ... = 2n.\n\nThis method tells us to start from the end and bubble down. However, we can also start from the front and bubble up. The only difference is that up is slower than down in total (think about how many nodes have to go through how many layers to get to the correct position for each kind of heapify).\n\nWe can also try bubbling down from the front or bubbling up from the end. These two methods are incorrect. Because when we bubble up from the front, we can guarantee that the nodes we have looped through and its parent can form a heap. Similarly, when we bubble down from the end, we can guarantee that the nodes we have looped through and its children can form a heap. However, for bubbling down from the front and bubbling up from the end, no such qualities can be guaranteed.","type":"content","url":"/2019-11-26-priority-queue-and-heap#heapify","position":21},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl4":"Resizable Array","lvl3":"Implementation","lvl2":"Binary Heap"},"type":"lvl4","url":"/2019-11-26-priority-queue-and-heap#resizable-array","position":22},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl4":"Resizable Array","lvl3":"Implementation","lvl2":"Binary Heap"},"content":"You can store your heap in a resizable array with a load factor as we’ve seen in the HashTables.","type":"content","url":"/2019-11-26-priority-queue-and-heap#resizable-array","position":23},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl4":"Code in C++","lvl3":"Implementation","lvl2":"Binary Heap"},"type":"lvl4","url":"/2019-11-26-priority-queue-and-heap#code-in-c","position":24},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl4":"Code in C++","lvl3":"Implementation","lvl2":"Binary Heap"},"content":"这份代码有问题，洛谷的数据太弱了没测出来。具体是测试某节点的孩子是否存在的代码错误：堆并不是一个每一层都满的二叉树。正确实现见 \n\n清华DSA 2-7#include<iostream>\n#include<cstdio>\n#include<cmath>\n#include<chrono>\n#include<time.h>\nusing namespace std;\nconst int M=1000000,INF=10e9; int n=0,a[M];\nvoid swim(int); void sink(int);void add(int); int extract(); int head(); void print(); int height(int);\nint main()\n{\n    /* 堆排序小测试\n    int num; cin>>num;\n    for(int i=0;i<num;i++) {int now; cin>>now; add(now);}\n    for(int i=0;i<num;i++) cout<<\"extracted\"<<extract()<<endl<<endl;*/\n\n    /* 洛谷P3378 模板 堆\n    int result[M/2],ans=0; int num; cin>>num;\n    for(int i=0;i<num;i++){\n        int command; cin>>command;\n        switch(command){\n            case 1:\n                int adding; cin>>adding; add(adding);\n                break;\n            case 2:\n                result[ans++]=head(); break;\n            case 3:\n                extract(); break;\n        }\n    }\n    for(int i=0;i<ans;i++) cout<<result[i]<<endl;*/\n\n    /* heapify 时间测试\n    理论上来说sink比swim要快(需要的操作少)\n    但是大概因为我的sink的实现方法太烂，所以竟然 swim 快一点\n    int max = 100000;n=max;\n    for(int i=0;i<max;i++) a[i] = max-i;\n\n    auto t0 = std::chrono::system_clock::now();\n\n    for (int i = (n/2); i > 0; i--) sink(i);\n    //for(int i=2;i<=n;i++) {swim(i);}\n\n    auto t1 = std::chrono::system_clock::now();\n    using milliseconds = std::chrono::duration<double, std::milli>;\n    milliseconds ms = t1 - t0;\n    std::cout << \" time taken by my code: \" << ms.count() << '\\n';*/\n\n    return 0;\n\n}\nvoid swim(int i)\n{\n    while(a[i/2]>=a[i]&&i>1){\n        //注意边界的选择：i<=1的话上面已经没东西了，就没有再上游的必要了\n        swap(a[i/2],a[i]);\n        i = i/2;\n    }\n}\nvoid sink(int i)\n{\n    int l=i*2, r=i*2+1;\n    while((a[i]>=a[l]||a[i]>=a[r])&&i<pow(2,height(n))){//叶子的上一层才需要下沉，叶子那一层没有东西可以沉\n        if(a[l]<=a[r]&&l<=n){\n            //l<=n判断这个孩子结点确实在堆中，而不是一个堆外但在数组中的结点\n            //有问题，这个地方如果不把每个删掉的元素设为INF的话，r会一直存在在数组中；\n            //考虑2 1 (0) 的情况，其中0是已经被删掉的元素，但是因为a[r]=0<a[l]=1所以2不会sink\n            //如果将数组初始化成INF的话，就不需要在不在堆内l<=n这个条件判断了\n            //但是内存占用会极大提高，因为整个长为M的数组都被初始化了\n            swap(a[i],a[l]);\n            i = l; l=i*2; r=i*2+1; continue;\n        }\n        else if (a[l]>a[r]&&r<=n){\n            swap(a[i],a[r]);\n            i = r; l=i*2; r=i*2+1; continue;\n        }\n        else return;\n    }\n}\nvoid add(int i)\n{\n    a[++n] = i;\n    swim(n);\n}\nint extract()\n{\n    int result = a[1];\n    swap(a[1],a[n]);\n    a[n--]=INF;\n    sink(1);\n    return result;\n}\nint head(){return a[1];}\n\n//  returns the height of a complete binary tree with n nodes\nint height(int x){\n     if(x==0) return 1;\n     int digit=0; while(x>0) {x=x>>1; digit++;} return digit;\n}\n\nvoid print(){for(int i=1;i<=n;i++) cout<<a[i]<<\" \"; cout<<endl;}\n","type":"content","url":"/2019-11-26-priority-queue-and-heap#code-in-c","position":25},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl4":"0-1 Based","lvl3":"Implementation","lvl2":"Binary Heap"},"type":"lvl4","url":"/2019-11-26-priority-queue-and-heap#id-0-1-based","position":26},{"hierarchy":{"lvl1":"Priority Queue and Heap","lvl4":"0-1 Based","lvl3":"Implementation","lvl2":"Binary Heap"},"content":"上面是用Binary heap设计一个 Max-oriented Priority Queue， 数组是1-based。  假如遇到面试官问怎么heapify怎么办？  下面我们就对上面代码进行少许改动，变为0-based，可以直接对数组进行max -  heapify。\n\nheapify()方法: 可以看出我们的heapify方法基本没有变化，除了把N / 2变成了数组的长度 nums.length / 2\n\nsink()方法 : 这里我们要注意一下边界条件。 先设置len = nums.length，这里len就相当于之前的N， 然后再进行比较的时候，我们要把每次的 j 都减1，从1-based改变为 0-based，其他代码都不需要改变public static void heapify(int[] nums) {\n        if (nums == null) {\n            return;\n        }\n        for (int k = nums.length / 2; k >= 1; k--) {\n            sink(nums, k);\n        }\n    }\n    \n    private static void sink(int[] nums, int k) {\n        int len = nums.length;\n        while (2 * k <= len) {\n            int j = 2 * k;\n            if (j < len && nums[j - 1] < nums[j]) {\n                j++;\n            }\n            if (nums[k - 1] > nums[j - 1]) {\n                break;\n            }\n            swap(nums, k - 1, j - 1);\n            k = j;\n        }\n    }","type":"content","url":"/2019-11-26-priority-queue-and-heap#id-0-1-based","position":27},{"hierarchy":{"lvl1":"Problem Analysis"},"type":"lvl1","url":"/2019-12-05-problem-analysis","position":0},{"hierarchy":{"lvl1":"Problem Analysis"},"content":"","type":"content","url":"/2019-12-05-problem-analysis","position":1},{"hierarchy":{"lvl1":"Problem Analysis","lvl2":"Computable Problem"},"type":"lvl2","url":"/2019-12-05-problem-analysis#computable-problem","position":2},{"hierarchy":{"lvl1":"Problem Analysis","lvl2":"Computable Problem"},"content":"","type":"content","url":"/2019-12-05-problem-analysis#computable-problem","position":3},{"hierarchy":{"lvl1":"Problem Analysis","lvl3":"Time Complexity","lvl2":"Computable Problem"},"type":"lvl3","url":"/2019-12-05-problem-analysis#time-complexity","position":4},{"hierarchy":{"lvl1":"Problem Analysis","lvl3":"Time Complexity","lvl2":"Computable Problem"},"content":"","type":"content","url":"/2019-12-05-problem-analysis#time-complexity","position":5},{"hierarchy":{"lvl1":"Problem Analysis","lvl4":"Tractable Problems (P)","lvl3":"Time Complexity","lvl2":"Computable Problem"},"type":"lvl4","url":"/2019-12-05-problem-analysis#tractable-problems-p","position":6},{"hierarchy":{"lvl1":"Problem Analysis","lvl4":"Tractable Problems (P)","lvl3":"Time Complexity","lvl2":"Computable Problem"},"content":"Polynomial algorithms: O(n^k) for some k (k\\leq4)","type":"content","url":"/2019-12-05-problem-analysis#tractable-problems-p","position":7},{"hierarchy":{"lvl1":"Problem Analysis","lvl4":"Exponential Time (EXPTIME)","lvl3":"Time Complexity","lvl2":"Computable Problem"},"type":"lvl4","url":"/2019-12-05-problem-analysis#exponential-time-exptime","position":8},{"hierarchy":{"lvl1":"Problem Analysis","lvl4":"Exponential Time (EXPTIME)","lvl3":"Time Complexity","lvl2":"Computable Problem"},"content":"Exponential Time algorithm: O((2^k)^n)","type":"content","url":"/2019-12-05-problem-analysis#exponential-time-exptime","position":9},{"hierarchy":{"lvl1":"Problem Analysis","lvl4":"Nondeterministic Polynomial Problems (NP)","lvl3":"Time Complexity","lvl2":"Computable Problem"},"type":"lvl4","url":"/2019-12-05-problem-analysis#nondeterministic-polynomial-problems-np","position":10},{"hierarchy":{"lvl1":"Problem Analysis","lvl4":"Nondeterministic Polynomial Problems (NP)","lvl3":"Time Complexity","lvl2":"Computable Problem"},"content":"e.g.: Hamilton cycles, SAT (Boolean satisfiability), Graph coloring\n\nHow would you solve a very hard problem? If you have enough resources (space & time), you can just randomly generate some results and check the result’s correctness.\n\nTherefore, the key is that you have to check the correctness in deterministic time (polynomial time). In fact, all of them can be solved by exponential-time algorithms that essentially try all exponentially many possible solutions, but this approach is infeasible\n\nNP-Complete: All NP problems can be expressed using the three NP examples above. These three problems are said to be NP-Complete. In fact, SAT is the most frequently used one. There are many people working on how to solve well-formed SAT problems in a reasonable amount of time. e.g. You can transform a Hamilton cycle problem into an SAT problem and use those SAT algorithms to solve it","type":"content","url":"/2019-12-05-problem-analysis#nondeterministic-polynomial-problems-np","position":11},{"hierarchy":{"lvl1":"Problem Analysis","lvl3":"Space Complexity","lvl2":"Computable Problem"},"type":"lvl3","url":"/2019-12-05-problem-analysis#space-complexity","position":12},{"hierarchy":{"lvl1":"Problem Analysis","lvl3":"Space Complexity","lvl2":"Computable Problem"},"content":"L: logarithmic space\n\nPSPACE: polynomial space\n\nNPSPACE: nondeterministic polynomial space","type":"content","url":"/2019-12-05-problem-analysis#space-complexity","position":13},{"hierarchy":{"lvl1":"Problem Analysis","lvl3":"Inclusion Relationships","lvl2":"Computable Problem"},"type":"lvl3","url":"/2019-12-05-problem-analysis#inclusion-relationships","position":14},{"hierarchy":{"lvl1":"Problem Analysis","lvl3":"Inclusion Relationships","lvl2":"Computable Problem"},"content":"L\\sube P\\sube NP \\sube PSPACE = NPSPACE \\sube EXPTIME","type":"content","url":"/2019-12-05-problem-analysis#inclusion-relationships","position":15},{"hierarchy":{"lvl1":"Problem Analysis","lvl2":"Unknow Problems"},"type":"lvl2","url":"/2019-12-05-problem-analysis#unknow-problems","position":16},{"hierarchy":{"lvl1":"Problem Analysis","lvl2":"Unknow Problems"},"content":"O(Factoring)\\in P ?\n\nNP = EXPTIME ?\n\nP = NP ?\n\nL = P ?\n\nbecause L≠PSPACE, we know that at least one of the inequalities L≠P, P≠NP, and NP≠PSPACE must hold, but we don’t know which.","type":"content","url":"/2019-12-05-problem-analysis#unknow-problems","position":17},{"hierarchy":{"lvl1":"Problem Analysis","lvl2":"Incomputable Problems (Halting Problem)"},"type":"lvl2","url":"/2019-12-05-problem-analysis#incomputable-problems-halting-problem","position":18},{"hierarchy":{"lvl1":"Problem Analysis","lvl2":"Incomputable Problems (Halting Problem)"},"content":"","type":"content","url":"/2019-12-05-problem-analysis#incomputable-problems-halting-problem","position":19},{"hierarchy":{"lvl1":"Problem Analysis","lvl3":"interpret","lvl2":"Incomputable Problems (Halting Problem)"},"type":"lvl3","url":"/2019-12-05-problem-analysis#interpret","position":20},{"hierarchy":{"lvl1":"Problem Analysis","lvl3":"interpret","lvl2":"Incomputable Problems (Halting Problem)"},"content":"claim:\n\nwe can convert a program to an AST\n\nwe can build an interpreter for the AST\n\nWe have a simple program p that returns only a boolean value.\n\nIf we can’t determine whether such simple programs terminate, then of course we have no hope of determining whether more complex programs do./** @return result of p on input i\n\tor does not terminate if p couldn't terminate on input i\n*/\nboolean interpret (Program p, Object i)\n    \nclass p {\n    boolean main(Object i){...}\n}\n\nmethod interpretis correct \\iff \\forall(p,i) p.main(i)==interpret(p,i)","type":"content","url":"/2019-12-05-problem-analysis#interpret","position":21},{"hierarchy":{"lvl1":"Problem Analysis","lvl3":"terminate","lvl2":"Incomputable Problems (Halting Problem)"},"type":"lvl3","url":"/2019-12-05-problem-analysis#terminate","position":22},{"hierarchy":{"lvl1":"Problem Analysis","lvl3":"terminate","lvl2":"Incomputable Problems (Halting Problem)"},"content":"/** Return whether p would halt on i */\nboolean terminates(Program p, Object i)","type":"content","url":"/2019-12-05-problem-analysis#terminate","position":23},{"hierarchy":{"lvl1":"Problem Analysis","lvl3":"Autological & Heterological","lvl2":"Incomputable Problems (Halting Problem)"},"type":"lvl3","url":"/2019-12-05-problem-analysis#autological-heterological","position":24},{"hierarchy":{"lvl1":"Problem Analysis","lvl3":"Autological & Heterological","lvl2":"Incomputable Problems (Halting Problem)"},"content":"A program is either autological or heterological:\n\nautological: program is autological if it returns true when provided its own AST (return value for other inputs not defined)\n\nheterological: either returns false on itself as input or doesn’t terminate","type":"content","url":"/2019-12-05-problem-analysis#autological-heterological","position":25},{"hierarchy":{"lvl1":"Problem Analysis","lvl3":"Self-Reference","lvl2":"Incomputable Problems (Halting Problem)"},"type":"lvl3","url":"/2019-12-05-problem-analysis#self-reference","position":26},{"hierarchy":{"lvl1":"Problem Analysis","lvl3":"Self-Reference","lvl2":"Incomputable Problems (Halting Problem)"},"content":"You cannot build both interpret and terminates for a program.class H{\n    \n    /** returns whether P is heterological (either doesn't terminate or return false) */\n    boolean main(Program p){\n        if(terminates(p,p))\n            return !interpret(p,p); //terminate but returns false, so p is heterological\n        else\n            return true; // doesn't terminate, so p is heterological\n    }\n}\n\ncan’t directly return !interpret(p,p) because we are not sure whether P will terminate. Therefore, we have to first check whether it terminates or not.\n\nNow we want to pass H itself to H.main\n\nnote: H can always terminate no matter what p is (because if p terminates, it calls interpret, which always returns a value if p terminates; if p doesn’t terminate, it just returns a boolean value of true) Therefore, if we pass H itself, it goes into the if (terminates) {} block, which returns ! interpret(h,h) = !h.main(h) Therefore, calling h.main(h) actually returns !h.main(h) But how can h returns the negation of what it’s supposed to return? Therefore, there doesn’t exist a program like h. \\\n\nTherefore, we cannot build both interpret and terminate. Since we can and did build an interpreter, terminate is what we can’t really build. That’s the why this problem is called a “halting problem”.","type":"content","url":"/2019-12-05-problem-analysis#self-reference","position":27},{"hierarchy":{"lvl1":"Problem Analysis","lvl3":"Implications","lvl2":"Incomputable Problems (Halting Problem)"},"type":"lvl3","url":"/2019-12-05-problem-analysis#implications","position":28},{"hierarchy":{"lvl1":"Problem Analysis","lvl3":"Implications","lvl2":"Incomputable Problems (Halting Problem)"},"content":"","type":"content","url":"/2019-12-05-problem-analysis#implications","position":29},{"hierarchy":{"lvl1":"Problem Analysis","lvl4":"Theoretical Implications","lvl3":"Implications","lvl2":"Incomputable Problems (Halting Problem)"},"type":"lvl4","url":"/2019-12-05-problem-analysis#theoretical-implications","position":30},{"hierarchy":{"lvl1":"Problem Analysis","lvl4":"Theoretical Implications","lvl3":"Implications","lvl2":"Incomputable Problems (Halting Problem)"},"content":"Every language is either:\n\ntoo expressive to analyze termination precisely (incomplete)\n\nnot expressive to build on interpreter (inconsistent)\n\nmakes a program like H illegal (impossible to be both comprehensive and consistent)","type":"content","url":"/2019-12-05-problem-analysis#theoretical-implications","position":31},{"hierarchy":{"lvl1":"Problem Analysis","lvl4":"practical implications","lvl3":"Implications","lvl2":"Incomputable Problems (Halting Problem)"},"type":"lvl4","url":"/2019-12-05-problem-analysis#practical-implications","position":32},{"hierarchy":{"lvl1":"Problem Analysis","lvl4":"practical implications","lvl3":"Implications","lvl2":"Incomputable Problems (Halting Problem)"},"content":"e.g. Can we write a IntelliJ plugin to check NullPointerException? if NPE is decidable, consider the codes belowh;\nx = null;\nx.hashcode(); // It should give you an error at this line at compile-time, which means the program can determine whether the program will actually run to this line, which means the program can determine whether h terminates \n\nReduction: if algorithm for problem A can be used to solve B (with suitable efficiency), A must be at least as hard as B. (In the example of above A: NPE checker B: halting problem)","type":"content","url":"/2019-12-05-problem-analysis#practical-implications","position":33},{"hierarchy":{"lvl1":"Functions"},"type":"lvl1","url":"/2020-01-28-functions","position":0},{"hierarchy":{"lvl1":"Functions"},"content":"From Textbook: \n\nFunctions","type":"content","url":"/2020-01-28-functions","position":1},{"hierarchy":{"lvl1":"Functions","lvl2":"Functions"},"type":"lvl2","url":"/2020-01-28-functions#functions","position":2},{"hierarchy":{"lvl1":"Functions","lvl2":"Functions"},"content":"Definition: let f x1 x2 ... xn = e (f is the function name; xi is input, and there can be multiple inputs; e is the output)\n\nWe can think of t1 -> t2 -> u as the type of a function that takes two inputs, the first of type t1 and the second of type t2, and returns an output of type u. Likewise for a function that takes n arguments.\n\nA function is already a value (that’s how you assign the value “function” to a variable name), so there is nothing to be evaluated when we evaluate its dynamic semantic.more","type":"content","url":"/2020-01-28-functions#functions","position":3},{"hierarchy":{"lvl1":"Functions","lvl3":"Anonymous Function","lvl2":"Functions"},"type":"lvl3","url":"/2020-01-28-functions#anonymous-function","position":4},{"hierarchy":{"lvl1":"Functions","lvl3":"Anonymous Function","lvl2":"Functions"},"content":"Definition: fun x -> x+1 (fun is a keyword indicating an anonymous function)\n\nAnonymous functions are also called lambda expressions, a term that comes out of the lambda calculus, which is a mathematical model of computation in the same sense that Turing machines are a model of computation. In the lambda calculus, fun x -> e would be written λx.e. The λ denotes an anonymous function.","type":"content","url":"/2020-01-28-functions#anonymous-function","position":5},{"hierarchy":{"lvl1":"Functions","lvl3":"Function Application","lvl2":"Functions"},"type":"lvl3","url":"/2020-01-28-functions#function-application","position":6},{"hierarchy":{"lvl1":"Functions","lvl3":"Function Application","lvl2":"Functions"},"content":"Normal way:(fun x -> e3) ((fun x -> e2) e1);;\n\nPipeline: e1 |> fun x->e2 |> fun x->e3;;\n\nThey are semantically the same as “let expressions” : let x = e1 in let x = e2 in e3  (In fact, the let expression is just a syntactic sugar of function application)\n\nFunction application is left-associative: g f x = (g f) x, while function types are right-associative: g -> f -> x = g -> (f -> x)","type":"content","url":"/2020-01-28-functions#function-application","position":7},{"hierarchy":{"lvl1":"Functions","lvl3":"Polymorphic Functions","lvl2":"Functions"},"type":"lvl3","url":"/2020-01-28-functions#polymorphic-functions","position":8},{"hierarchy":{"lvl1":"Functions","lvl3":"Polymorphic Functions","lvl2":"Functions"},"content":"The 'a is a type variable: it stands for an unknown type, just like a regular variable stands for an unknown value.","type":"content","url":"/2020-01-28-functions#polymorphic-functions","position":9},{"hierarchy":{"lvl1":"Functions","lvl3":"Labeled Arguments","lvl2":"Functions"},"type":"lvl3","url":"/2020-01-28-functions#labeled-arguments","position":10},{"hierarchy":{"lvl1":"Functions","lvl3":"Labeled Arguments","lvl2":"Functions"},"content":"OCaml supports labeled arguments to functions. You can declare this kind of function using the following syntax:# let f ~name1:arg1 ~name2:arg2 = arg1 + arg2;;\nval f : name1:int -> name2:int -> int = <fun>\n\nThis function can be called by passing the labeled arguments in either order:f ~name2:3 ~name1:4;;\n\nA sugar of declaring function with labeled arguments islet f ~name1 ~name2 = name1 + name2","type":"content","url":"/2020-01-28-functions#labeled-arguments","position":11},{"hierarchy":{"lvl1":"Functions","lvl3":"Partial Application","lvl2":"Functions"},"type":"lvl3","url":"/2020-01-28-functions#partial-application","position":12},{"hierarchy":{"lvl1":"Functions","lvl3":"Partial Application","lvl2":"Functions"},"content":"A function of two variables: let add x y = x + y\n\nA composite function: let addx x = fun y -> x + ylet add x y = x+y\nlet add x = fun y -> x+y\nlet add = fun x -> (fun y -> x+y)\n\nThe top two are just syntactic sugar for the last statement. Now, think about what does the last line mean? Does the fun y -> x+y actually knows that there exists an x? The answer is yes. That’s because the statement fun y -> x+y is in the scope of x’s declaration.\n\nFor the codes below, the outermost function actually takes in a value of type t1 and produces a function that is of type t2 -> (t3 -> t4)\n\nAnd the type of such a functiont1 -> t2 -> t3 -> t4\n\nreally means the same ast1 -> (t2 -> (t3 -> t4))\n\nThat is, function types are right associative: there are implicit parentheses around function types, from right to left. The intuition here is that a function takes a single argument and returns a new function that expects the remaining arguments.\n\nBelow is an example of Partial Application: The bottom two are syntactic sugars of the first statementlet comp = fun f g -> fun x -> g(f x);;\nlet compa f g = fun x -> g(f x);;\nlet compb f g x = g(f x);;\n\nApplying comp to other functions:utop # let inc x = x+1;;\nval inc : int -> int = <fun>\n────────────────────────────────────────────────────────\nutop # let inc2 = comp inc inc;;\nval inc2 : int -> int = <fun>\n────────────────────────────────────────────────────────\nutop # inc 1;;\n- : int = 2\n────────────────────────────────────────────────────────\nutop # inc2 1;;\n- : int = 3\n\nA useful application of Partial Application is precomputation: When we want to use a process multiple times, we can just write a function that takes in other function and do that job. g. predefine comp so that when we want to composite two functions, we only need to apply it to the function comp instead of writing out the composite function on ourselves every time.","type":"content","url":"/2020-01-28-functions#partial-application","position":13},{"hierarchy":{"lvl1":"Functions","lvl3":"Unit Function","lvl2":"Functions"},"type":"lvl3","url":"/2020-01-28-functions#unit-function","position":14},{"hierarchy":{"lvl1":"Functions","lvl3":"Unit Function","lvl2":"Functions"},"content":"There is only one value of this type, which is written () and is also pronounced “unit”. So unit is like bool, except there is one fewer value of type unit than there is of bool. Unit is therefore used when you need to take an argument or return a value, but there’s no interesting value to pass or return.","type":"content","url":"/2020-01-28-functions#unit-function","position":15},{"hierarchy":{"lvl1":"Functions","lvl3":"Type Inference","lvl2":"Functions"},"type":"lvl3","url":"/2020-01-28-functions#type-inference","position":16},{"hierarchy":{"lvl1":"Functions","lvl3":"Type Inference","lvl2":"Functions"},"content":"How to determine the type of a very complicated function?\n\nAdd right-associative parameters; Rewrite the function as a more understandable let expression\n\nFind out which variables have to take in a value (then it must be a function), which doesn’t (then it can be anything)\n\nDetermine the type of each variable from the last statement, and write their types from left to right in the sequence they were taken in.\n\nTake fun f g -> fun x -> g(f x) as an example:\n\nlet h = fun f g -> ( fun x -> g(f x) )\n\nx doesn’t take in a value, so x is a variable of type a’\n\nf takes in x, so f must be a function of type a’ -> b’\n\ng takes in the output of f, so g must be a function of type b’ -> c’\n\nThe type of this function is\tf\t\t\t  g\t\t\tx\t(output:g(f x))\n(a' -> b') -> (b' -> c') -> a' -> c'\n\nTake fun f g -> fun x -> (g f) x as another example:\n\nlet h = fun f g -> (fun x -> (g f) x)\n\nx doesn’t take in a value, so x is a variable of type a’\n\nf doesn’t take in a value, so f is a variable of type b’\n\ng takes in f, so g must be a function whose input is of type b’; plus its output takes in another variable x, so its output is also a function, which takes in a type a’. Therefore, g is of type b' -> (a' -> c')\n\nThe type of this function isf \t\t\tg\t\t\t  x  (output:(g f) x)\nb' -> (b' -> a' -> c') -> a' -> c'","type":"content","url":"/2020-01-28-functions#type-inference","position":17},{"hierarchy":{"lvl1":"Standard Data Types"},"type":"lvl1","url":"/2020-01-30-standard-data-types","position":0},{"hierarchy":{"lvl1":"Standard Data Types"},"content":"From Textbook: \n\nStandard Data Types","type":"content","url":"/2020-01-30-standard-data-types","position":1},{"hierarchy":{"lvl1":"Standard Data Types","lvl2":"Lists"},"type":"lvl2","url":"/2020-01-30-standard-data-types#lists","position":2},{"hierarchy":{"lvl1":"Standard Data Types","lvl2":"Lists"},"content":"","type":"content","url":"/2020-01-30-standard-data-types#lists","position":3},{"hierarchy":{"lvl1":"Standard Data Types","lvl3":"Building Lists","lvl2":"Lists"},"type":"lvl3","url":"/2020-01-30-standard-data-types#building-lists","position":4},{"hierarchy":{"lvl1":"Standard Data Types","lvl3":"Building Lists","lvl2":"Lists"},"content":"The empty list is written [] and is pronounced “nil”, a name that comes from Lisp. Given a list lst and element elt, we can prepend elt to lst by writing elt::lst. The double-colon operator is pronounced “cons”more\n\ncons always prepend things, so cons is actually right-associative. The following code has the same effect.1::2::3::[];;\n1::(2::(3::[]));;\n\nAll the elements of a list must have the same type. The word list itself here is not a type. For example, given int, it produces the type int list. You could think of type constructors as being like functions that operate on types, instead of functions that operate on values. (We mentioned this idea of thinking constructor as a function on type in CS2112)","type":"content","url":"/2020-01-30-standard-data-types#building-lists","position":5},{"hierarchy":{"lvl1":"Standard Data Types","lvl3":"Accessing Lists","lvl2":"Lists"},"type":"lvl3","url":"/2020-01-30-standard-data-types#accessing-lists","position":6},{"hierarchy":{"lvl1":"Standard Data Types","lvl3":"Accessing Lists","lvl2":"Lists"},"content":"The following code computes the sum of a list.let rec sum lst = \n  match lst with\n  | [] -> 0\n  | h::t -> h + sum t\n\nThe following code computes the length of a list. _, the underscore character is used when we want to indicate the presence of some value in a pattern without actually giving it a name.let rec length lst = \n  match lst with\n  | [] -> 0\n  | _::t -> 1 + length t\n  \n\nThe following code appends one list onto the beginning of another list.let rec append lst1 lst2 = \n  match lst1 with\n  | [] -> lst2\n  | h::t -> h::(append t lst2)\n\nNote: every natural number is either 0 or is 1 greater than some other natural number n, and so a proof by induction has a base case for 0 and an inductive case for n+1. Likewise all our functions have a base case for the empty list and a recursive case for the list that has one more element than another list. This similarity is no accident. There is a deep relationship between induction and recursion; we’ll explore that relationship in more detail later in the course.","type":"content","url":"/2020-01-30-standard-data-types#accessing-lists","position":7},{"hierarchy":{"lvl1":"Standard Data Types","lvl3":"Mutating Lists","lvl2":"Lists"},"type":"lvl3","url":"/2020-01-30-standard-data-types#mutating-lists","position":8},{"hierarchy":{"lvl1":"Standard Data Types","lvl3":"Mutating Lists","lvl2":"Lists"},"content":"Values in OCaml are immutable. The following code increments the head by 1.let inc_first lst =\n  match lst with\n  | [] -> []\n  | h::t -> (h+1)::t\n\nThis code looks extremely similar with C or Java operating on pointers. The implementation of list in OCaml works in the way that it shares the tail list t between the old list and the new list, such that the amount of memory in use does not increase (beyond the one extra piece of memory needed to store h+1). The reason that it’s quite safe for the compiler to implement sharing is exactly that list elements are immutable.","type":"content","url":"/2020-01-30-standard-data-types#mutating-lists","position":9},{"hierarchy":{"lvl1":"Standard Data Types","lvl3":"Pattern Matching with Lists","lvl2":"Lists"},"type":"lvl3","url":"/2020-01-30-standard-data-types#pattern-matching-with-lists","position":10},{"hierarchy":{"lvl1":"Standard Data Types","lvl3":"Pattern Matching with Lists","lvl2":"Lists"},"content":"","type":"content","url":"/2020-01-30-standard-data-types#pattern-matching-with-lists","position":11},{"hierarchy":{"lvl1":"Standard Data Types","lvl4":"Basics","lvl3":"Pattern Matching with Lists","lvl2":"Lists"},"type":"lvl4","url":"/2020-01-30-standard-data-types#basics","position":12},{"hierarchy":{"lvl1":"Standard Data Types","lvl4":"Basics","lvl3":"Pattern Matching with Lists","lvl2":"Lists"},"content":"Each of the clauses pi -> ei is called a branch or a case of the pattern match. The p’s here are a new syntactic form called a pattern.\n\na variable name, e.g. x\n\nthe underscore character _, which is called the wildcard (we don’t care what it is)\n\nthe empty list []\n\np1::p2\n\n[p1; ...; pn]let length_is lst n =\n  match length lst with\n  | n -> true\n  | _ -> false\n\nThe code above always returns true, because suppose that the length of lst is 5. Then the pattern match becomes: match 5 with n -> true | _ -> false. And n matches 5. A variable pattern matches any value and here produces the binding n->5. The correct codes are written below.let length_is lst n =\n  match length lst with\n  | m -> if m=n then true else false\n  | _ -> false\n  \nlet length_is lst n =\n  match length lst with\n  | m -> m=n\n  | _ -> false\n\nlet length_is lst n =\n  length lst = n\n\nHowever, this doesn’t mean patterns are not the variable values as in switch statement. Yes they are general “patterns”. But you can match them to specific values. e.g.match 5 with\n| 6 -> true\n| _ -> false;;\n- : bool = false\n\nmatch 5 with\n| 5 -> true\n| _ -> false;;\n- : bool = true","type":"content","url":"/2020-01-30-standard-data-types#basics","position":13},{"hierarchy":{"lvl1":"Standard Data Types","lvl4":"Advanced Pattern Matching","lvl3":"Pattern Matching with Lists","lvl2":"Lists"},"type":"lvl4","url":"/2020-01-30-standard-data-types#advanced-pattern-matching","position":14},{"hierarchy":{"lvl1":"Standard Data Types","lvl4":"Advanced Pattern Matching","lvl3":"Pattern Matching with Lists","lvl2":"Lists"},"content":"p1 | ... | pn: an “or” pattern; matching against it succeeds if a match succeeds against any of the individual patterns pi, which are tried in order from left to right. All the patterns must bind the same variables.\n\n(p : t): a pattern with an explicit type annotation.\n\nc: here, c means any constant, such as integer literals, string literals, and booleans.\n\n'ch1'..'ch2': here, ch means a character literal. For example, 'A'..'Z' matches any uppercase letter.\n\np when e: matches p but only if e evaluates to true.","type":"content","url":"/2020-01-30-standard-data-types#advanced-pattern-matching","position":15},{"hierarchy":{"lvl1":"Standard Data Types","lvl2":"Tuples and Records"},"type":"lvl2","url":"/2020-01-30-standard-data-types#tuples-and-records","position":16},{"hierarchy":{"lvl1":"Standard Data Types","lvl2":"Tuples and Records"},"content":"both represent heterogeneous types of values, both sizes are fixed","type":"content","url":"/2020-01-30-standard-data-types#tuples-and-records","position":17},{"hierarchy":{"lvl1":"Standard Data Types","lvl3":"Records","lvl2":"Tuples and Records"},"type":"lvl3","url":"/2020-01-30-standard-data-types#records","position":18},{"hierarchy":{"lvl1":"Standard Data Types","lvl3":"Records","lvl2":"Tuples and Records"},"content":"Works like struct in C++. Each field is identified by names.","type":"content","url":"/2020-01-30-standard-data-types#records","position":19},{"hierarchy":{"lvl1":"Standard Data Types","lvl4":"Definition","lvl3":"Records","lvl2":"Tuples and Records"},"type":"lvl4","url":"/2020-01-30-standard-data-types#definition","position":20},{"hierarchy":{"lvl1":"Standard Data Types","lvl4":"Definition","lvl3":"Records","lvl2":"Tuples and Records"},"content":"type student ={name: string; gpa : float; year :int;} (*defining a type*)\nlet rbg = {name = \"R B\"; gpa = 4.0; year = 1954;} (*declare an instance of that type*)\nlet s = rbg.name (*accessing field in the record*)","type":"content","url":"/2020-01-30-standard-data-types#definition","position":21},{"hierarchy":{"lvl1":"Standard Data Types","lvl4":"Patter Matching","lvl3":"Records","lvl2":"Tuples and Records"},"type":"lvl4","url":"/2020-01-30-standard-data-types#patter-matching","position":22},{"hierarchy":{"lvl1":"Standard Data Types","lvl4":"Patter Matching","lvl3":"Records","lvl2":"Tuples and Records"},"content":"match rbg with \n| {name=n; gpa=g; year=y} -> y\n\n(*syntactic sugar of codes above*)\nmatch rgb with \n| {name;gpa;year} -> name","type":"content","url":"/2020-01-30-standard-data-types#patter-matching","position":23},{"hierarchy":{"lvl1":"Standard Data Types","lvl3":"Tuples","lvl2":"Tuples and Records"},"type":"lvl3","url":"/2020-01-30-standard-data-types#tuples","position":24},{"hierarchy":{"lvl1":"Standard Data Types","lvl3":"Tuples","lvl2":"Tuples and Records"},"content":"Tuples are identified by position, instead of naming the components.","type":"content","url":"/2020-01-30-standard-data-types#tuples","position":25},{"hierarchy":{"lvl1":"Standard Data Types","lvl4":"Definition","lvl3":"Tuples","lvl2":"Tuples and Records"},"type":"lvl4","url":"/2020-01-30-standard-data-types#definition-1","position":26},{"hierarchy":{"lvl1":"Standard Data Types","lvl4":"Definition","lvl3":"Tuples","lvl2":"Tuples and Records"},"content":"let t = (10,\"am\") (*t has type: int * string*) \ntype time = int * string \nlet t:time = (10,\"am\") (*t has type: time*) \nfst t;; snd t;; (*predefined functions to access the first and second element of a tuple*)","type":"content","url":"/2020-01-30-standard-data-types#definition-1","position":27},{"hierarchy":{"lvl1":"Standard Data Types","lvl4":"Pattern Matching","lvl3":"Tuples","lvl2":"Tuples and Records"},"type":"lvl4","url":"/2020-01-30-standard-data-types#pattern-matching","position":28},{"hierarchy":{"lvl1":"Standard Data Types","lvl4":"Pattern Matching","lvl3":"Tuples","lvl2":"Tuples and Records"},"content":"if we use a pattern in a let expression (or definition), we are really just doing pattern matching with a single clause.let tick t =\nlet (t,s) = t in (t+1,s) (*tick : int * 'a -> int * 'a = <fun>*)\n\nlet tick (t:time):time =\nlet (t,s) = t in (t+1,s) (*tick : time -> time = <fun>*)\n\nlet tick (t:time):time = \nmatch t with\n| (t,s) -> (t+1,s)","type":"content","url":"/2020-01-30-standard-data-types#pattern-matching","position":29},{"hierarchy":{"lvl1":"Standard Data Types","lvl3":"Pattern Matching in a Nutshell","lvl2":"Tuples and Records"},"type":"lvl3","url":"/2020-01-30-standard-data-types#pattern-matching-in-a-nutshell","position":30},{"hierarchy":{"lvl1":"Standard Data Types","lvl3":"Pattern Matching in a Nutshell","lvl2":"Tuples and Records"},"content":"(* Pokemon types *)\ntype ptype = \n  TNormal | TFire | TWater\n\n(* A record to represent Pokemon *)\ntype mon = {name: string; hp : int; ptype: ptype}\n\n(*********************************************\n * Several ways to get a Pokemon's hit points:\n *********************************************)\n\n(* OK *)\nlet get_hp m =\n  match m with\n  | {name=n; hp=h; ptype=t} -> h\n\n(* better *)\nlet get_hp m =\n  match m with\n  | {name=_; hp=h; ptype=_} -> h\n\n(* better *)\nlet get_hp m =\n  match m with\n  | {name; hp; ptype} -> hp\n\n(* better *)\nlet get_hp m =\n  match m with\n  | {hp} -> hp\n\n(* best *)\nlet get_hp m = m.hp\n\n(**************************************************\n * Several ways to get the 3rd component of a tuple\n **************************************************)\n\n(* OK *)\nlet thrd t =\n  match t with\n  | (x,y,z) -> z\n\n(* good *)\nlet thrd t = \n  let (x,y,z) = t in z\n\n(* better *)\nlet thrd t =\n  let (_,_,z) = t in z\n\n(* best *)\nlet thrd (_,_,z) = z\n\n(*************************************\n * How to get the components of a pair\n *************************************)\n\nlet fst (x,_) = x\nlet snd (_,y) = y\n\n\n(************************\n * take tuple as a whole\n ************************)\nlet rep_ok ((n,lst) as v) =\n  if List.length lst = n then v\n  else failwith \"RI violated\"","type":"content","url":"/2020-01-30-standard-data-types#pattern-matching-in-a-nutshell","position":31},{"hierarchy":{"lvl1":"Advanced Data Types"},"type":"lvl1","url":"/2020-02-04-advanced-data-types","position":0},{"hierarchy":{"lvl1":"Advanced Data Types"},"content":"From Textbook: \n\nAdvanced Data Types","type":"content","url":"/2020-02-04-advanced-data-types","position":1},{"hierarchy":{"lvl1":"Advanced Data Types","lvl2":"Algebraic Data Types"},"type":"lvl2","url":"/2020-02-04-advanced-data-types#algebraic-data-types","position":2},{"hierarchy":{"lvl1":"Advanced Data Types","lvl2":"Algebraic Data Types"},"content":"以前我们的 variants 比较像 enum，但是现在我们更像一个abstract classmore(*definition*)\ntype t = C1 | C2 of t2| ... | Cn (*of tn*)\n(* t: 'a * 'a * ... *)\n\n(*expression*)\nC e\n---or---\nC\n(* e: (e1:'a, e2:'a, ...) *)\n\n(*pattern matching*)\nC p\n(* p: (e1:'a, e2:'a, ...) *)\n\nexamples:type point = float * float \n\ntype shape =\n  | Point  of point\n  | Circle of point * float (* center and radius *)\n  | Rect   of point * point (* lower-left and \n                               upper-right corners *)\n\nlet area = function\n  | Point _ -> 0.0\n  | Circle (_,r) -> pi *. (r ** 2.0)\n  | Rect ((x1,y1),(x2,y2)) ->\n      let w = x2 -. x1 in\n      let h = y2 -. y1 in\n        w *. h\n\ntype string_or_int =\n| String of string\n| Int of int\n\ntype string_or_int_list = string_or_int list\n\nlet rec sum : string_or_int list -> int = function\n  | [] -> 0\n  | (String s)::t -> int_of_string s + sum t\n  | (Int i)::t -> i + sum t\n\nlet three = sum [String \"1\"; Int 2]\n\nWhen do we need [],(),{} | of?type node = {value:int; next:mylist}\nand mylist = Nil | Node of node\n\n[]: list\n\n(): Constructor of a tuple\n\n{}: Constructor of a record\n\n|: delineate different variants inside a type\n\nof: defining the construction of an algebraic type","type":"content","url":"/2020-02-04-advanced-data-types#algebraic-data-types","position":3},{"hierarchy":{"lvl1":"Advanced Data Types","lvl3":"Recursive Variants","lvl2":"Algebraic Data Types"},"type":"lvl3","url":"/2020-02-04-advanced-data-types#recursive-variants","position":4},{"hierarchy":{"lvl1":"Advanced Data Types","lvl3":"Recursive Variants","lvl2":"Algebraic Data Types"},"content":"type intlist = Nil | Cons of int * intlist\n\ntype 'a tree = \n  | Leaf \n  | Node of 'a node\nand 'a node = { \n  value: 'a; \n  left:  'a tree; \n  right: 'a tree\n}","type":"content","url":"/2020-02-04-advanced-data-types#recursive-variants","position":5},{"hierarchy":{"lvl1":"Advanced Data Types","lvl3":"Parametrized Variants","lvl2":"Algebraic Data Types"},"type":"lvl3","url":"/2020-02-04-advanced-data-types#parametrized-variants","position":6},{"hierarchy":{"lvl1":"Advanced Data Types","lvl3":"Parametrized Variants","lvl2":"Algebraic Data Types"},"content":"No matter what kind of types we define, either a variant, a record, or a tuple. We need the type parameter 'a or ('a,'b) when we define it.(* [Option] makes it safer to return nothing*)\ntype 'a option = None | Some of 'a\n\ntype 'a mylist = Nil | Cons of 'a * 'a mylist\ntype ('a,'b) pair = {first: 'a; second: 'b}\ntype ('a,'b) test = 'a * 'b\n\nSimilarly, when you want to declare a variable that has a parametrized type, you also need to give the type parameter.type 'a tree = Leaf of 'a | Node of ('a * 'a tree * 'a tree)\nlet x:'a tree = Leaf 5\nlet x:int tree = Leaf 5\n\nIf you do let x:tree = Leaf 5, the compiler won’t know what type you are talking about.","type":"content","url":"/2020-02-04-advanced-data-types#parametrized-variants","position":7},{"hierarchy":{"lvl1":"Advanced Data Types","lvl3":"Polymorphic Variants","lvl2":"Algebraic Data Types"},"type":"lvl3","url":"/2020-02-04-advanced-data-types#polymorphic-variants","position":8},{"hierarchy":{"lvl1":"Advanced Data Types","lvl3":"Polymorphic Variants","lvl2":"Algebraic Data Types"},"content":"They would be better off with the name “anonymous variants,” because you want to use them when these variants are only used in this specific function and not anywhere else.\n\nThe constructor of polymorphic variants start with a \" ` \"(* note: no type definition *)\n\nlet f = function\n  | 0 -> `Infinity\n  | 1 -> `Finite 1\n  | n -> `Finite (-n)\n  \nval f : int -> [> `Finite of int | `Infinity ]\n\nlet lst = [`Pos 5; `Zero; `Neg (~-4); `Pos 3];;\nval lst : [> `Neg of int | `Pos of int | `Zero ] list =\n  [`Pos 5; `Zero; `Neg (-4); `Pos 3]","type":"content","url":"/2020-02-04-advanced-data-types#polymorphic-variants","position":9},{"hierarchy":{"lvl1":"Advanced Data Types","lvl3":"Pattern Matching","lvl2":"Algebraic Data Types"},"type":"lvl3","url":"/2020-02-04-advanced-data-types#pattern-matching","position":10},{"hierarchy":{"lvl1":"Advanced Data Types","lvl3":"Pattern Matching","lvl2":"Algebraic Data Types"},"content":"type 'a tree = \n  | Leaf \n  | Node of 'a node\n\nand 'a node = { \n  value: 'a; \n  left:  'a tree; \n  right: 'a tree\n}\n\n(* [mem x t] returns [true] if and only if [x] is a value at some\n * node in tree [t]. \n *)\nlet rec mem x = function\n  | Leaf -> false\n  | Node {value; left; right} -> value = x || mem x left || mem x right","type":"content","url":"/2020-02-04-advanced-data-types#pattern-matching","position":11},{"hierarchy":{"lvl1":"Advanced Data Types","lvl2":"Exceptions"},"type":"lvl2","url":"/2020-02-04-advanced-data-types#exceptions","position":12},{"hierarchy":{"lvl1":"Advanced Data Types","lvl2":"Exceptions"},"content":"","type":"content","url":"/2020-02-04-advanced-data-types#exceptions","position":13},{"hierarchy":{"lvl1":"Advanced Data Types","lvl3":"The Basics","lvl2":"Exceptions"},"type":"lvl3","url":"/2020-02-04-advanced-data-types#the-basics","position":14},{"hierarchy":{"lvl1":"Advanced Data Types","lvl3":"The Basics","lvl2":"Exceptions"},"content":"(*Definition: it is just a special kind of \"type\"*)\nexception E of t\n\n\n(*Call an Exception*)\nraise e\n\n(*syntactic sugar*)\nfailwith \"Not Good\"\nraise (Failure (\"Not Good\"))","type":"content","url":"/2020-02-04-advanced-data-types#the-basics","position":15},{"hierarchy":{"lvl1":"Advanced Data Types","lvl3":"Pattern Matching","lvl2":"Exceptions"},"type":"lvl3","url":"/2020-02-04-advanced-data-types#pattern-matching-1","position":16},{"hierarchy":{"lvl1":"Advanced Data Types","lvl3":"Pattern Matching","lvl2":"Exceptions"},"content":"The following code says: try evaluating e. If it produces an exception packet, use the exception patterns from the original match expression to handle that packet. If it doesn’t produce an exception packet but instead produces a normal value, use the non-exception patterns from the original match expression to match that value.match \n  try e with\n    | q1 -> e1\n    | ...\n    | qn -> en\nwith\n  | r1 -> e1\n  | ...\n  | rm -> em","type":"content","url":"/2020-02-04-advanced-data-types#pattern-matching-1","position":17},{"hierarchy":{"lvl1":"Higher-Order Functions"},"type":"lvl1","url":"/2020-02-06-higher-order-functions","position":0},{"hierarchy":{"lvl1":"Higher-Order Functions"},"content":"From Textbook: \n\nHigher Order Programming","type":"content","url":"/2020-02-06-higher-order-functions","position":1},{"hierarchy":{"lvl1":"Higher-Order Functions","lvl2":"Introduction"},"type":"lvl2","url":"/2020-02-06-higher-order-functions#introduction","position":2},{"hierarchy":{"lvl1":"Higher-Order Functions","lvl2":"Introduction"},"content":"higher-order: functions as values, you can pass functions as arguments into other functions, functions at the same level as other variables\n\nlower-order: languages like C, functions as something higher than other variables\n\nPipeline is a higher-order function.let pipeline x f = f x\nlet (|>) = pipeline\nlet x = 5 |> double  (* 10 *)more","type":"content","url":"/2020-02-06-higher-order-functions#introduction","position":3},{"hierarchy":{"lvl1":"Higher-Order Functions","lvl2":"Map (Transforms Elements)"},"type":"lvl2","url":"/2020-02-06-higher-order-functions#map-transforms-elements","position":4},{"hierarchy":{"lvl1":"Higher-Order Functions","lvl2":"Map (Transforms Elements)"},"content":"it maps each element of the list through a function(* [map f [x1; x2; ...; xn]] is [f x1; f x2; ...; f xn] *)\nlet rec map f = function\n  | [] -> []\n  | h::t -> (f h)::(map f t)\n\nlet add1 = map (fun x-> x+1);\nlet add1' = map ((+)1);\n\nlet concat3110 = map (fun x -> x^\"3110\")","type":"content","url":"/2020-02-06-higher-order-functions#map-transforms-elements","position":5},{"hierarchy":{"lvl1":"Higher-Order Functions","lvl2":"Filter (Eliminates Elements)"},"type":"lvl2","url":"/2020-02-06-higher-order-functions#filter-eliminates-elements","position":6},{"hierarchy":{"lvl1":"Higher-Order Functions","lvl2":"Filter (Eliminates Elements)"},"content":"List.filter <predicate> <list>it picks all elements which meet predicate p to form a new list.(* [filter p l] is the list of elements of [l] that satisfy the predicate [p]. \n * The order of the elements in the input list is preserved. *)\nlet rec filter f = function\n  | [] -> []\n  | h::t -> if f h then h::(filter f t) else filter f t","type":"content","url":"/2020-02-06-higher-order-functions#filter-eliminates-elements","position":7},{"hierarchy":{"lvl1":"Higher-Order Functions","lvl2":"Fold (Combines Elements)"},"type":"lvl2","url":"/2020-02-06-higher-order-functions#fold-combines-elements","position":8},{"hierarchy":{"lvl1":"Higher-Order Functions","lvl2":"Fold (Combines Elements)"},"content":"","type":"content","url":"/2020-02-06-higher-order-functions#fold-combines-elements","position":9},{"hierarchy":{"lvl1":"Higher-Order Functions","lvl3":"Fold Right","lvl2":"Fold (Combines Elements)"},"type":"lvl3","url":"/2020-02-06-higher-order-functions#fold-right","position":10},{"hierarchy":{"lvl1":"Higher-Order Functions","lvl3":"Fold Right","lvl2":"Fold (Combines Elements)"},"content":"Can we abstract the following two functions as a single function?let rec sum = function\n  | [] -> 0\n  | h::t -> h + (sum t)\n\nlet rec concat = function\n  | [] -> \"\"\n  | h::t -> h ^ (concat t)\n\nFirst, we abstract the initial valuelet rec sum' init = function\n  | [] -> init\n  | h::t -> h + sum' init t\n\nlet sum = sum' 0\n\nlet rec concat' init = function\n  | [] -> init\n  | h::t -> h ^ concat' init t\n\nlet concat = concat' \"\"\n\nWe find out the only thing these two functions have in difference is the operator. So the next step, we factor out the operator.let rec combine init op = function\n| [] -> init\n| h::t -> op h (combine init op t);;\n\nThe intuition for why this function is called fold_right is that the way it works is to “fold in” elements of the list from the right to the left, combining each new element using the operator. For example, fold_right (+) [a;b;c] 0 results in evaluation of the expression a+(b+(c+0)). The parentheses associate from the right-most subexpression to the left.\n\nOne way to think of fold_right would be that the [] value in the list gets replaced by init, and each :: constructor gets replaced by op. For example, [a;b;c] is just syntactic sugar for a::(b::(c::[])). So if we replace [] with 0 and :: with (+), we get a+(b+(c+0)).","type":"content","url":"/2020-02-06-higher-order-functions#fold-right","position":11},{"hierarchy":{"lvl1":"Higher-Order Functions","lvl3":"Fold Left","lvl2":"Fold (Combines Elements)"},"type":"lvl3","url":"/2020-02-06-higher-order-functions#fold-left","position":12},{"hierarchy":{"lvl1":"Higher-Order Functions","lvl3":"Fold Left","lvl2":"Fold (Combines Elements)"},"content":"let rec fold_left op acc = function\n  | []   -> acc\n  | h :: t -> fold_left op (op acc h) t\n\nThe idea is that fold_left (+) 0 [a;b;c] results in evaluation of ((0+a)+b)+c. The parentheses associate from the left-most subexpression to the right. So fold_left is “folding in” elements of the list from the left to the right, combining each new element using the operator.","type":"content","url":"/2020-02-06-higher-order-functions#fold-left","position":13},{"hierarchy":{"lvl1":"Higher-Order Functions","lvl3":"Fold Left vs. Fold Right","lvl2":"Fold (Combines Elements)"},"type":"lvl3","url":"/2020-02-06-higher-order-functions#fold-left-vs-fold-right","position":14},{"hierarchy":{"lvl1":"Higher-Order Functions","lvl3":"Fold Left vs. Fold Right","lvl2":"Fold (Combines Elements)"},"content":"Why is there a difference of the order the operand takes in arguments (op acc h; op: 'a -> 'b -> 'a as in fold_left; op h (combine init op t); op: 'a -> 'b -> 'b  as in fold_right)? And why is there a difference of the order these two functions take in argument (fold_left op acc lst; fold_right op lst init)?\n\nfold_left f init [v1; v2;...; vn] is f (... (f (f init v1) v2)...) vn whereas fold_right f [v1; v2;...; vn] init is f v1 (f v2 (...(f vn init)...)) (-- Nate Foster)\n\norder of evaluation:\n\nfold_left evaluates from left to right\n\nfold_right evaluates from right to left\n\ntail-recursive:\n\nbecause of the way these two functions evaluate\n\nfold_left is tail-recursive. We can add that value to acc, the group of elements completed evaluation, after evaluating the current element.\n\nfold_right is not recursive. Because it cannot evaluate the nth element before evaluating the (n+1)th element. And the evaluation of nth element depends on (n+1)th element. This pattern violates the definition of tail-recursive\n\nThen is there a tail-recursive version of fold_right? You can first reverse the list and then fold_left.","type":"content","url":"/2020-02-06-higher-order-functions#fold-left-vs-fold-right","position":15},{"hierarchy":{"lvl1":"Higher-Order Functions","lvl3":"Fold Application","lvl2":"Fold (Combines Elements)"},"type":"lvl3","url":"/2020-02-06-higher-order-functions#fold-application","position":16},{"hierarchy":{"lvl1":"Higher-Order Functions","lvl3":"Fold Application","lvl2":"Fold (Combines Elements)"},"content":"let length l = List.fold_left (fun a _ -> a+1) 0 l\nlet rev l = List.fold_left (fun a x -> x::a) [] l\nlet map f l = List.fold_right (fun x a -> (f x)::a) l []\nlet filter f l = List.fold_right (fun x a -> if f x then x::a else a) l []\n\n(*test whether a list is full of true*)\nlet lst_and_fold =\n    List.fold_left (fun acc elt -> acc && elt) true","type":"content","url":"/2020-02-06-higher-order-functions#fold-application","position":17},{"hierarchy":{"lvl1":"Higher-Order Functions","lvl3":"Generalized Fold","lvl2":"Fold (Combines Elements)"},"type":"lvl3","url":"/2020-02-06-higher-order-functions#generalized-fold","position":18},{"hierarchy":{"lvl1":"Higher-Order Functions","lvl3":"Generalized Fold","lvl2":"Fold (Combines Elements)"},"content":"let rec foldtree init op = function\n  | Leaf -> init\n  | Node (v,l,r) -> op v (foldtree init op l) (foldtree init op r)\n  \nlet size t = foldtree 0 (fun _ l r -> 1 + l + r) t\nlet depth t = foldtree 0 (fun _ l r -> 1 + max l r) t\nlet preorder t = foldtree [] (fun x l r -> [x] @ l @ r) t","type":"content","url":"/2020-02-06-higher-order-functions#generalized-fold","position":19},{"hierarchy":{"lvl1":"Modules"},"type":"lvl1","url":"/2020-02-11-modules","position":0},{"hierarchy":{"lvl1":"Modules"},"content":"From Textbook: \n\nOCaml Modules","type":"content","url":"/2020-02-11-modules","position":1},{"hierarchy":{"lvl1":"Modules","lvl2":"Structures"},"type":"lvl2","url":"/2020-02-11-modules#structures","position":2},{"hierarchy":{"lvl1":"Modules","lvl2":"Structures"},"content":"","type":"content","url":"/2020-02-11-modules#structures","position":3},{"hierarchy":{"lvl1":"Modules","lvl3":"Semantics","lvl2":"Structures"},"type":"lvl3","url":"/2020-02-11-modules#semantics","position":4},{"hierarchy":{"lvl1":"Modules","lvl3":"Semantics","lvl2":"Structures"},"content":"The first letter of a module’s name should be capitalized.module ModuleName = struct \n  (* definitions *)\nendmore\n\nYou can access the variables / methods in a module by Module.methodlet x = ModuleName.empty;;\nx.peek;;","type":"content","url":"/2020-02-11-modules#semantics","position":5},{"hierarchy":{"lvl1":"Modules","lvl2":"Signatures"},"type":"lvl2","url":"/2020-02-11-modules#signatures","position":6},{"hierarchy":{"lvl1":"Modules","lvl2":"Signatures"},"content":"","type":"content","url":"/2020-02-11-modules#signatures","position":7},{"hierarchy":{"lvl1":"Modules","lvl3":"Semantics","lvl2":"Signatures"},"type":"lvl3","url":"/2020-02-11-modules#semantics-1","position":8},{"hierarchy":{"lvl1":"Modules","lvl3":"Semantics","lvl2":"Signatures"},"content":"Signature is a collections of declarations; not evaluated, just type checked.module type ModuleTypeName = sig \n  (* declarations *)\nend\n\nmodule type Stack = sig\n  type 'a stack\n  val empty    : 'a stack\n  val is_empty : 'a stack -> bool\n  val push     : 'a -> 'a stack -> 'a stack\n  val peek     : 'a stack -> 'a\n  val pop      : 'a stack -> 'a stack\nend\n\nA structure matches a signature if the structure provides definitions for all the names specified in the signature (and possibly more), and these definitions meet the type requirements given in the signature.\n\nIf you don’t seal X, as long as the fields in module correspond with those in signature, these two match. when we seal X, we a create a linkage between module with the signature but we don’t have to do that. It’s safer if you explicitly want X to be a type of X, but we add an extra layer of abstraction to X.","type":"content","url":"/2020-02-11-modules#semantics-1","position":9},{"hierarchy":{"lvl1":"Modules","lvl3":"Abstraction","lvl2":"Signatures"},"type":"lvl3","url":"/2020-02-11-modules#abstraction","position":10},{"hierarchy":{"lvl1":"Modules","lvl3":"Abstraction","lvl2":"Signatures"},"content":"You can also specify that this module as a type of some signature by providing a module type annotation  : Stack. After adding this, everything inside that module will be come abstract and hidden from view.module type Arith = sig\n  type t\n  val zero  : t\n  val one   : t\n  val (+)   : t -> t -> t\n  val ( * ) : t -> t -> t\n  val (~-)  : t -> t\nend\n\nmodule Ints : Arith = struct\n  type t    = int\n  let zero  = 0\n  let one   = 1\n  let (+)   = Stdlib.(+)\n  let ( * ) = Stdlib.( * )\n  let (~-)  = Stdlib.(~-)\nend\n\nOutside of the module Ints, the expression Ints.(one + one) is perfectly fine, but Ints.(1 + 1) is not, because t is abstract: outside the module no one is permitted to know that t = int. In fact, the toplevel can’t even give us good output about what the sum of one and one is!# Ints.(one + one);;\n- : Ints.t = <abstr>","type":"content","url":"/2020-02-11-modules#abstraction","position":11},{"hierarchy":{"lvl1":"Modules","lvl3":"Sharing Constraint","lvl2":"Signatures"},"type":"lvl3","url":"/2020-02-11-modules#sharing-constraint","position":12},{"hierarchy":{"lvl1":"Modules","lvl3":"Sharing Constraint","lvl2":"Signatures"},"content":"OCaml lets you write sharing constraints that refine a signature by specifying equations that must hold on the abstract types in that signature. If T is a module type containing an abstract type t, then T with type t = int is a new module type that is the same as T, except that t is known to be int. For example, we could write:module Ints : (Arith with type t = int) = struct\n  (* all of Ints as before *)\nend\n\nNow both Ints.(one + one) and Ints.(1 + 1) are legal.","type":"content","url":"/2020-02-11-modules#sharing-constraint","position":13},{"hierarchy":{"lvl1":"Modules","lvl3":"Modules and the Top Level","lvl2":"Signatures"},"type":"lvl3","url":"/2020-02-11-modules#modules-and-the-top-level","position":14},{"hierarchy":{"lvl1":"Modules","lvl3":"Modules and the Top Level","lvl2":"Signatures"},"content":"Well, apparently you cannot remember everything about how to import a library into OCaml and you don’t have to. So just refer to \n\nthis site","type":"content","url":"/2020-02-11-modules#modules-and-the-top-level","position":15},{"hierarchy":{"lvl1":"Code Reuse with Modules"},"type":"lvl1","url":"/2020-02-13-code-reuse-with-modules","position":0},{"hierarchy":{"lvl1":"Code Reuse with Modules"},"content":"From Textbook: \n\nCode Reuse with Modules","type":"content","url":"/2020-02-13-code-reuse-with-modules","position":1},{"hierarchy":{"lvl1":"Code Reuse with Modules","lvl2":"Includes"},"type":"lvl2","url":"/2020-02-13-code-reuse-with-modules#includes","position":2},{"hierarchy":{"lvl1":"Code Reuse with Modules","lvl2":"Includes"},"content":"Def: includes enables a structure to include all the values defined by another structure, or a signature to include all the names declared by another signature.more","type":"content","url":"/2020-02-13-code-reuse-with-modules#includes","position":3},{"hierarchy":{"lvl1":"Code Reuse with Modules","lvl3":"Syntax","lvl2":"Includes"},"type":"lvl3","url":"/2020-02-13-code-reuse-with-modules#syntax","position":4},{"hierarchy":{"lvl1":"Code Reuse with Modules","lvl3":"Syntax","lvl2":"Includes"},"content":"module type SetExtended = sig\n  include Set\n  (*all other definitions specific to SetExtended*)\n  val of_list : 'a list -> 'a t\nend\n\nmodule ListSetDupsExtended = struct\n  include ListSetDups\n  (*all other definitions specific to SetExtended*)\n  let of_list lst = List.fold_right add lst empty\nend","type":"content","url":"/2020-02-13-code-reuse-with-modules#syntax","position":5},{"hierarchy":{"lvl1":"Code Reuse with Modules","lvl3":"Encapsulation","lvl2":"Includes"},"type":"lvl3","url":"/2020-02-13-code-reuse-with-modules#encapsulation","position":6},{"hierarchy":{"lvl1":"Code Reuse with Modules","lvl3":"Encapsulation","lvl2":"Includes"},"content":"module ListSetDupsImpl = struct\n  type 'a t   = 'a list\n  let empty   = []\n  let mem     = List.mem\n  let add x s = x::s\n  let elts s  = List.sort_uniq Stdlib.compare s\nend\n\nmodule ListSetDups : Set = ListSetDupsImpl\n\nmodule ListSetDupsExtended = struct\n  include ListSetDupsImpl\n  let of_list lst = lst\nend\n\nThe important change is that ListSetDupsImpl is not sealed, so its type 'a t is not abstract. Plus, OCaml compiler can infer it is an implementation of Set .When we include it in ListSetDupsExtended, we can therefore exploit the fact that it’s a synonym for 'a list.\n\nThe clients should use ListSetDups, but when we use List to implement other things, we should use ListSetDupsImpl instead.","type":"content","url":"/2020-02-13-code-reuse-with-modules#encapsulation","position":7},{"hierarchy":{"lvl1":"Code Reuse with Modules","lvl3":"Includes vs. Open","lvl2":"Includes"},"type":"lvl3","url":"/2020-02-13-code-reuse-with-modules#includes-vs-open","position":8},{"hierarchy":{"lvl1":"Code Reuse with Modules","lvl3":"Includes vs. Open","lvl2":"Includes"},"content":"module M = struct\n  let x = 0\nend\n\nmodule N = struct\n  include M\n  let y = x + 1\n  let z = 1\nend\n\nmodule O = struct\n  open M\n  let y = x + 1\n  let z = 1\nend\n\n\nmodule M : sig val x : int end\nmodule N : sig val x : int val y : int val z : int end\nmodule O : sig val y : int val z : int end\n\nN has both an x and y, whereas O has only a y. The reason is that include M causes all the definitions of M to also be included in N, so the definition of x from M is present in N. But open M only made those definitions available in the scope of O, aka. a part of the implementation; it doesn’t actually make them part of the structure, aka. the client cannot see them. So O does not contain a definition of x, even though x is in scope during the evaluation of O’s definition of y.","type":"content","url":"/2020-02-13-code-reuse-with-modules#includes-vs-open","position":9},{"hierarchy":{"lvl1":"Code Reuse with Modules","lvl2":"Functors"},"type":"lvl2","url":"/2020-02-13-code-reuse-with-modules#functors","position":10},{"hierarchy":{"lvl1":"Code Reuse with Modules","lvl2":"Functors"},"content":"Def: a functor is simply a “function” from structures to structures. It is a parametrized module.","type":"content","url":"/2020-02-13-code-reuse-with-modules#functors","position":11},{"hierarchy":{"lvl1":"Code Reuse with Modules","lvl3":"Syntax","lvl2":"Functors"},"type":"lvl3","url":"/2020-02-13-code-reuse-with-modules#syntax-1","position":12},{"hierarchy":{"lvl1":"Code Reuse with Modules","lvl3":"Syntax","lvl2":"Functors"},"content":"module F (M : S) = struct\n  ...\nend\n\n(*annonymous functors*)\nmodule F = functor (M : S) -> struct\n  ...\nend\n\n(*functors parametriezed with multiple modules*)\nmodule F (M1 : S1) ... (Mn : Sn) = struct\n  ...\nend\n\n(* above are the desugared version of the codes above*)\nmodule F = functor (M1 : S1) -> ... -> functor (Mn : Sn) -> struct\n  ...\nendmodule ANewModule = F(OldModule)\nmodule ListSetNoDupsExtended = ExtendSet(ListSetNoDups)","type":"content","url":"/2020-02-13-code-reuse-with-modules#syntax-1","position":13},{"hierarchy":{"lvl1":"Code Reuse with Modules","lvl3":"Application","lvl2":"Functors"},"type":"lvl3","url":"/2020-02-13-code-reuse-with-modules#application","position":14},{"hierarchy":{"lvl1":"Code Reuse with Modules","lvl3":"Application","lvl2":"Functors"},"content":"","type":"content","url":"/2020-02-13-code-reuse-with-modules#application","position":15},{"hierarchy":{"lvl1":"Code Reuse with Modules","lvl4":"Extension","lvl3":"Application","lvl2":"Functors"},"type":"lvl4","url":"/2020-02-13-code-reuse-with-modules#extension","position":16},{"hierarchy":{"lvl1":"Code Reuse with Modules","lvl4":"Extension","lvl3":"Application","lvl2":"Functors"},"content":"module ExtendSet(S:Set) = struct\n  include S\n\n  let add_all lst set =\n    let add' s x = S.add x s in\n    List.fold_left add' set lst\nend","type":"content","url":"/2020-02-13-code-reuse-with-modules#extension","position":17},{"hierarchy":{"lvl1":"Code Reuse with Modules","lvl4":"Other than Extension: Testing","lvl3":"Application","lvl2":"Functors"},"type":"lvl4","url":"/2020-02-13-code-reuse-with-modules#other-than-extension-testing","position":18},{"hierarchy":{"lvl1":"Code Reuse with Modules","lvl4":"Other than Extension: Testing","lvl3":"Application","lvl2":"Functors"},"content":"module SackTester (S: StackSig) = struct\n\tlet _ = assert (S.(empty |> push 1 |> peek) = 1)\nend\n\nmodule MyStackTester = StackTester(MyStack)\nmodule ListStackTester = StackTester(ListStack)\n\nThe only difference is that because the latter example is about extension, we need to include everything from its parent module.","type":"content","url":"/2020-02-13-code-reuse-with-modules#other-than-extension-testing","position":19},{"hierarchy":{"lvl1":"Specifications"},"type":"lvl1","url":"/2020-02-18-specifications","position":0},{"hierarchy":{"lvl1":"Specifications"},"content":"From Textbook: \n\nSpecifications and Abstractions","type":"content","url":"/2020-02-18-specifications","position":1},{"hierarchy":{"lvl1":"Specifications","lvl2":"Specification of Functions"},"type":"lvl2","url":"/2020-02-18-specifications#specification-of-functions","position":2},{"hierarchy":{"lvl1":"Specifications","lvl2":"Specification of Functions"},"content":"(** [f x] is ...\n\tExample: ...\n\tRequires: ...\n\tRaises: ... \n*)\nlet f x  = more\n\nReturns: Don’t write Returns: ..., instead, just use [f x] is ...\n\nRequires: specific conditions on input Requires: [x >=0]\n\nRaises: what the program will do if a bad input is given Raises: xxx Exception if [x<0]\n\nExamples: give an example input and output of the function to better explain what it does","type":"content","url":"/2020-02-18-specifications#specification-of-functions","position":3},{"hierarchy":{"lvl1":"Specifications","lvl2":"Specification of Modules"},"type":"lvl2","url":"/2020-02-18-specifications#specification-of-modules","position":4},{"hierarchy":{"lvl1":"Specifications","lvl2":"Specification of Modules"},"content":"What to document in a module:\n\nfunctions not specified in the interface\n\nabstraction function\n\nrepresentation invariant(* Implementation of sets as lists without duplicates.\n * Includes rep_ok checks. *)\nmodule ListSetNoDupsRepOk : Set = struct\n  (* Abstraction function:  the list [a1; ...; an] represents the \n   * set {a1, ..., an}.  [] represents the empty set {}.\n   *\n   * Representation invariant: the list contains no duplicates.\n   *)\n  type 'a set = 'a list\n\n  let rep_ok (l : 'a set) : 'a set =\n    List.fold_right\n      (fun x t -> assert (not (List.mem x t)); x :: t)\n      l []\n\n  let empty = []\n  let mem x l = List.mem x (rep_ok l)\n  let add x l = rep_ok (if mem x (rep_ok l) then l else x :: l)\n  let rem x l = rep_ok (List.filter ((<>) x) (rep_ok l))\n  let size l = List.length (rep_ok l)\n  let union l1 l2 =  \n    rep_ok (List.fold_left\n          (fun a x -> if mem x l2 then a else x :: a) \n          (rep_ok l2) (rep_ok l1))\n  let inter l1 l2 = rep_ok (List.filter (fun h -> mem h l2) (rep_ok l1))\nend\n\nnote that the above code keeps representation invariant, abstraction function, and other spec about implementation details inside the module definition, because when we generate the docs of this module, everything outside the definition of the module will be come “public” specs for clients and everything inside will become “private” spec for maintainers.","type":"content","url":"/2020-02-18-specifications#specification-of-modules","position":5},{"hierarchy":{"lvl1":"Specifications","lvl3":"Abstraction Function","lvl2":"Specification of Modules"},"type":"lvl3","url":"/2020-02-18-specifications#abstraction-function","position":6},{"hierarchy":{"lvl1":"Specifications","lvl3":"Abstraction Function","lvl2":"Specification of Modules"},"content":"Abstraction function maps valid concrete values to abstract values(** AF: ... *)\n\nmodule ListSetDups : Set = struct\n  (* AF: the list [a1; ...; an] represents the\n   * smallest set containing all the elements a1, ..., an.\n   * The list may contain duplicates.\n   * [] represents the empty set.\n   *)\n  type 'a set = 'a list\n  ...\n  \nmodule ListSetNoDups : Set = struct\n  (* AF: the list [a1; ...; an] represents the set\n   * {a1, ..., an}.  [] represents the empty set.\n   *)\n  type 'a set = 'a list\n  ...","type":"content","url":"/2020-02-18-specifications#abstraction-function","position":7},{"hierarchy":{"lvl1":"Specifications","lvl3":"Representation Invariant","lvl2":"Specification of Modules"},"type":"lvl3","url":"/2020-02-18-specifications#representation-invariant","position":8},{"hierarchy":{"lvl1":"Specifications","lvl3":"Representation Invariant","lvl2":"Specification of Modules"},"content":"Representation Invariant distinguishes valid concrete values and from invalid concrete values and is the implicit part of all the precondition and postcondition.(** RI: ... *)\n\nmodule ListSetNoDupsRepOk : Set = struct\n  (* RI: the list contains no duplicates.\n   *)\n  type 'a set = 'a list\n  ...","type":"content","url":"/2020-02-18-specifications#representation-invariant","position":9},{"hierarchy":{"lvl1":"Specifications","lvl2":"Common Mistakes"},"type":"lvl2","url":"/2020-02-18-specifications#common-mistakes","position":10},{"hierarchy":{"lvl1":"Specifications","lvl2":"Common Mistakes"},"content":"Some common mistakes include not stating enough in preconditions, failing to identify when exceptions will be thrown, failing to specify behavior at boundary cases, writing operational specifications instead of definitional and stating too much in postconditions.","type":"content","url":"/2020-02-18-specifications#common-mistakes","position":11},{"hierarchy":{"lvl1":"Specifications","lvl3":"Long Variable Name","lvl2":"Common Mistakes"},"type":"lvl3","url":"/2020-02-18-specifications#long-variable-name","position":12},{"hierarchy":{"lvl1":"Specifications","lvl3":"Long Variable Name","lvl2":"Common Mistakes"},"content":"let number_of_zeros_in_the_list =\n   fold_left (fun (accumulator:int) (list_element:int) ->\n          accumulator + (if list_element=0 then 1 else 0)) 0 the_list\nin ...\n\nCode using such long names is verbose and hard to read. Instead of trying to embed a complete description of a variable in its name, use a short and suggestive name (e.g., zeroes), and if necessary, add a comment at its declaration explaining the purpose of the variable.","type":"content","url":"/2020-02-18-specifications#long-variable-name","position":13},{"hierarchy":{"lvl1":"Mutability"},"type":"lvl1","url":"/2020-03-03-mutability","position":0},{"hierarchy":{"lvl1":"Mutability"},"content":"From Textbook: \n\nMutabilitymore","type":"content","url":"/2020-03-03-mutability","position":1},{"hierarchy":{"lvl1":"Mutability","lvl2":"Refs"},"type":"lvl2","url":"/2020-03-03-mutability#refs","position":2},{"hierarchy":{"lvl1":"Mutability","lvl2":"Refs"},"content":"","type":"content","url":"/2020-03-03-mutability#refs","position":3},{"hierarchy":{"lvl1":"Mutability","lvl3":"Syntax","lvl2":"Refs"},"type":"lvl3","url":"/2020-03-03-mutability#syntax","position":4},{"hierarchy":{"lvl1":"Mutability","lvl3":"Syntax","lvl2":"Refs"},"content":"ref x\n!x\nx := e\n\nref x: creates a reference using the ref keyword (let x = ref 0 creates a location in memory whose contents are initialized to 0)\n\n!x: dereferences x and returns the contents of the memory location\n\nx := e: is an assignment. It mutates the contents x to be 1. Note that x itself still points to the same location (i.e., address) in memory. Variables really are immutable in that way. What changes is the contents of that memory location. Memory is mutable; variable bindings are not.","type":"content","url":"/2020-03-03-mutability#syntax","position":5},{"hierarchy":{"lvl1":"Mutability","lvl3":"Counter","lvl2":"Refs"},"type":"lvl3","url":"/2020-03-03-mutability#counter","position":6},{"hierarchy":{"lvl1":"Mutability","lvl3":"Counter","lvl2":"Refs"},"content":"let next_val = \n  let counter = ref 0 \n  in fun () ->\n    incr counter;\n    !counter\n\nlet next_val_broken = fun () ->\n  let counter = ref 0\n  in incr counter;\n     !counter\n     \nnext_val_broken () = 1 ... 1 ... 1\nnext_val () = 1 ... 2 ... 3 ...\n\nLook at how these two functions’ results differ. In the first example, counter is just a temporary variable later bound to the fun() -> ... Therefore, counter will not be initiated every time we call next_val . It is evaluated only when the first time it is defined.","type":"content","url":"/2020-03-03-mutability#counter","position":7},{"hierarchy":{"lvl1":"Mutability","lvl2":"Sequencing"},"type":"lvl2","url":"/2020-03-03-mutability#sequencing","position":8},{"hierarchy":{"lvl1":"Mutability","lvl2":"Sequencing"},"content":"e1; e2; ... ; en\n\nThe sentence above evaluates each one of ei in order from left to right, returning only vn.\n\nIf the values vi previous expression ei evaluates to are not of type unit, the compiler will give you a warning because it makes no sense to throw away those values. To get rid of the warning (if you’re sure that’s what you need to do), there’s a function ignore : 'a -> unit in the standard library. Using it, ignore(2+3); 7 will compile without a warning. Of course, you could code up ignore yourself: let ignore _ = ().\n\nThis is a syntactic sugar of let _ = e1 in e2","type":"content","url":"/2020-03-03-mutability#sequencing","position":9},{"hierarchy":{"lvl1":"Mutability","lvl2":"Equality"},"type":"lvl2","url":"/2020-03-03-mutability#equality","position":10},{"hierarchy":{"lvl1":"Mutability","lvl2":"Equality"},"content":"Physical Equality (==,!=): same address\n\nStructural Equality (=,<>): same content","type":"content","url":"/2020-03-03-mutability#equality","position":11},{"hierarchy":{"lvl1":"Mutability","lvl2":"Mutable Fields (in Record)"},"type":"lvl2","url":"/2020-03-03-mutability#mutable-fields-in-record","position":12},{"hierarchy":{"lvl1":"Mutability","lvl2":"Mutable Fields (in Record)"},"content":"","type":"content","url":"/2020-03-03-mutability#mutable-fields-in-record","position":13},{"hierarchy":{"lvl1":"Mutability","lvl3":"Syntax","lvl2":"Mutable Fields (in Record)"},"type":"lvl3","url":"/2020-03-03-mutability#syntax-1","position":14},{"hierarchy":{"lvl1":"Mutability","lvl3":"Syntax","lvl2":"Mutable Fields (in Record)"},"content":"(* declare a record with mutable field *)\ntype point = {x:int; y:int; mutable c:string}\nlet p = {x=0; y=0; c=\"red\"}\n\n(*mutate the field of that record*)\np.c <- \"white\"\n\np.x <- 3\n# Error: The record field x is not mutable","type":"content","url":"/2020-03-03-mutability#syntax-1","position":15},{"hierarchy":{"lvl1":"Mutability","lvl3":"Mutable Stack","lvl2":"Mutable Fields (in Record)"},"type":"lvl3","url":"/2020-03-03-mutability#mutable-stack","position":16},{"hierarchy":{"lvl1":"Mutability","lvl3":"Mutable Stack","lvl2":"Mutable Fields (in Record)"},"content":"","type":"content","url":"/2020-03-03-mutability#mutable-stack","position":17},{"hierarchy":{"lvl1":"Mutability","lvl2":"Arrays and Loops"},"type":"lvl2","url":"/2020-03-03-mutability#arrays-and-loops","position":18},{"hierarchy":{"lvl1":"Mutability","lvl2":"Arrays and Loops"},"content":"","type":"content","url":"/2020-03-03-mutability#arrays-and-loops","position":19},{"hierarchy":{"lvl1":"Mutability","lvl3":"Arrays","lvl2":"Arrays and Loops"},"type":"lvl3","url":"/2020-03-03-mutability#arrays","position":20},{"hierarchy":{"lvl1":"Mutability","lvl3":"Arrays","lvl2":"Arrays and Loops"},"content":"Array creation: [|e0; e1; ...; en|]\n\nArray indexing: e1.(e2)\n\nArray assignment: e1.(e2) <- e3","type":"content","url":"/2020-03-03-mutability#arrays","position":21},{"hierarchy":{"lvl1":"Mutability","lvl3":"Loops","lvl2":"Arrays and Loops"},"type":"lvl3","url":"/2020-03-03-mutability#loops","position":22},{"hierarchy":{"lvl1":"Mutability","lvl3":"Loops","lvl2":"Arrays and Loops"},"content":"while e1 do e2 done\n\nlet i = ref 0\nwhile !i<=2 do print_endline(string_of_int v.(!i)); incr i done\n\nfor x=e1 to e2 do e3 done\nfor x=e1 downto e2 do e3 done\n\nfor i=1 to 2 do print_endline(string_of_int v.(i)) done\nfor i=2 downto 0 do print_endline(string_of_int v.(i)) done\n\nwhile loops terminate when e1 becomes false\n\nfor loops execute once for each integer from e1 to e2\n\nfor..to loops evaluate starting at e1 and incrementing x each iteration; for..downto loops evaluate starting at e1 and decrementing x each iteration","type":"content","url":"/2020-03-03-mutability#loops","position":23},{"hierarchy":{"lvl1":"Red Black Tree"},"type":"lvl1","url":"/2020-04-06-red-black-tree","position":0},{"hierarchy":{"lvl1":"Red Black Tree"},"content":"From Textbook: \n\nRed-Black Treesmore","type":"content","url":"/2020-04-06-red-black-tree","position":1},{"hierarchy":{"lvl1":"Red Black Tree","lvl2":"Invariant"},"type":"lvl2","url":"/2020-04-06-red-black-tree#invariant","position":2},{"hierarchy":{"lvl1":"Red Black Tree","lvl2":"Invariant"},"content":"Every node is colored either red or black\n\nLeaves and root colored black\n\nLocal Invariant: no red node has a direct red child (only applies to its direct children and doesn’t apply to grand children or grand-grandchildren; in other words, red nodes cannot be repeated)\n\nGlobal Invariant: Every path from the root to a leaf has the same number of black nodes\n\nThe invariant implies that: length of longest path is at most twice as long as as the shortest path. (extreme case: path with red node in between each black node vs. path with only black nodes: B-R-B-R-B-R-B vs. B-B-B-B)","type":"content","url":"/2020-04-06-red-black-tree#invariant","position":3},{"hierarchy":{"lvl1":"Red Black Tree","lvl2":"Implementation"},"type":"lvl2","url":"/2020-04-06-red-black-tree#implementation","position":4},{"hierarchy":{"lvl1":"Red Black Tree","lvl2":"Implementation"},"content":"红黑树的 OCaml 实现着实震惊了我，让我惊讶 OCaml 原来是这么强大的一个语言\n\nNote where in the code we applied balance. It is the father of the node that breaks the invariant and is also the deepest black node where an invariant is violated.let balance = function\n  | Black, z, Node (Red, y, Node (Red, x, a, b), c), d\n  | Black, z, Node (Red, x, a, Node (Red, y, b, c)), d\n  | Black, x, a, Node (Red, z, Node (Red, y, b, c), d)\n  | Black, x, a, Node (Red, y, b, Node (Red, z, c, d)) ->\n      Node (Red, y, Node (Black, x, a, b), Node (Black, z, c, d))\n  | a, b, c, d -> Node (a, b, c, d)\n\nlet insert x s =\n  let rec ins = function\n    | Leaf -> Node (Red, x, Leaf, Leaf)\n    | Node (color, y, a, b) as s ->\n      if x < y then balance (color, y, ins a, b)\n      else if x > y then balance (color, y, a, ins b)\n      else s in\n  match ins s with\n    | Node (_, y, a, b) -> Node (Black, y, a, b)\n    | Leaf -> (* guaranteed to be nonempty *)\n        failwith \"RBT insert failed with ins returning leaf\"","type":"content","url":"/2020-04-06-red-black-tree#implementation","position":5},{"hierarchy":{"lvl1":"Interpreter"},"type":"lvl1","url":"/2020-04-09-interpreter","position":0},{"hierarchy":{"lvl1":"Interpreter"},"content":"more","type":"content","url":"/2020-04-09-interpreter","position":1},{"hierarchy":{"lvl1":"Interpreter","lvl2":"Compilers and Interpreters"},"type":"lvl2","url":"/2020-04-09-interpreter#compilers-and-interpreters","position":2},{"hierarchy":{"lvl1":"Interpreter","lvl2":"Compilers and Interpreters"},"content":"","type":"content","url":"/2020-04-09-interpreter#compilers-and-interpreters","position":3},{"hierarchy":{"lvl1":"Interpreter","lvl3":"Compilers","lvl2":"Compilers and Interpreters"},"type":"lvl3","url":"/2020-04-09-interpreter#compilers","position":4},{"hierarchy":{"lvl1":"Interpreter","lvl3":"Compilers","lvl2":"Compilers and Interpreters"},"content":"translation: source program -> target program\n\nbetter performance: target program is faster","type":"content","url":"/2020-04-09-interpreter#compilers","position":5},{"hierarchy":{"lvl1":"Interpreter","lvl3":"Interpreters","lvl2":"Compilers and Interpreters"},"type":"lvl3","url":"/2020-04-09-interpreter#interpreters","position":6},{"hierarchy":{"lvl1":"Interpreter","lvl3":"Interpreters","lvl2":"Compilers and Interpreters"},"content":"execute the program in the most direct way\n\neasier to implement","type":"content","url":"/2020-04-09-interpreter#interpreters","position":7},{"hierarchy":{"lvl1":"Interpreter","lvl3":"Virtual Machine","lvl2":"Compilers and Interpreters"},"type":"lvl3","url":"/2020-04-09-interpreter#virtual-machine","position":8},{"hierarchy":{"lvl1":"Interpreter","lvl3":"Virtual Machine","lvl2":"Compilers and Interpreters"},"content":"program written for virtual machines can be run cross-platform, as long as virtual machine is installed\n\nsecurity: can limit what the program can do","type":"content","url":"/2020-04-09-interpreter#virtual-machine","position":9},{"hierarchy":{"lvl1":"Interpreter","lvl2":"Architecture of Compiler and Interpreter"},"type":"lvl2","url":"/2020-04-09-interpreter#architecture-of-compiler-and-interpreter","position":10},{"hierarchy":{"lvl1":"Interpreter","lvl2":"Architecture of Compiler and Interpreter"},"content":"Front End: source code -> AST -> (IR) intermediate representation\n\nLexical Analysis with lexer\n\nSyntactic Analysis with parser\n\nSemantic Analysis\n\nBack End:\n\nInterpreter: executes AST or IR\n\nCompiler: translates IR into machine code","type":"content","url":"/2020-04-09-interpreter#architecture-of-compiler-and-interpreter","position":11},{"hierarchy":{"lvl1":"The Substitution Model"},"type":"lvl1","url":"/2020-04-14-the-substitution-model","position":0},{"hierarchy":{"lvl1":"The Substitution Model"},"content":"","type":"content","url":"/2020-04-14-the-substitution-model","position":1},{"hierarchy":{"lvl1":"The Substitution Model","lvl2":"Introduction"},"type":"lvl2","url":"/2020-04-14-the-substitution-model#introduction","position":2},{"hierarchy":{"lvl1":"The Substitution Model","lvl2":"Introduction"},"content":"When we evaluate let expressions like let x = 42 in x+1, we want to find a way to formally denote that it evaluates to 43 because x in x+1 is substituted with 42. More generally, let x = v1 in e2 --> e2 with v1 substituted for x. That’s where substitution comes into play. We want to express the right side of the arrow with a new notation e2 {v1/x}, which means e2 with all occurrences of x substituted for v1.","type":"content","url":"/2020-04-14-the-substitution-model#introduction","position":3},{"hierarchy":{"lvl1":"The Substitution Model","lvl2":"Substitution in Let Expressions"},"type":"lvl2","url":"/2020-04-14-the-substitution-model#substitution-in-let-expressions","position":4},{"hierarchy":{"lvl1":"The Substitution Model","lvl2":"Substitution in Let Expressions"},"content":"Suppose we have the following codelet x = e in \n\tlet x = e1 in e2\n\nlet x = e in \n\tlet y = e1 in e2\n\nAfter one step of evaluation, we have(let x = e1 in e2){e/x}  =  let x = e1{e/x} in e2\n(let y = e1 in e2){e/x}  =  let y = e1{e/x} in e2{e/x}\n\nBoth of those cases substitute e for x inside the binding expression e1. That’s to ensure that expressions like let x = 42 in let y = x in y would evaluate correctly: x needs to be in scope inside the binding y = x, so we have to do a substitution there regardless of the name being bound.\n\nBut the first case does not do a substitution inside e2, whereas the second case does. That’s so we stop substituting when we reach a shadowed name. Consider let x = 5 in let x = 6 in x. We know it would evaluate to 6 in OCaml because of shadowing. Here’s how it would evaluate with our definitions of SimPL:    let x = 5 in let x = 6 in x\n--> (let x = 6 in x){5/x}\n  = let x = 6{5/x} in x      ***\n  = let x = 6 in x\n--> x{6/x}\n  = 6\n\nHowever, if we use the second case to evaluate it, it will evaluate to 5, which is incorrect.","type":"content","url":"/2020-04-14-the-substitution-model#substitution-in-let-expressions","position":5},{"hierarchy":{"lvl1":"The Substitution Model","lvl2":"Substitution in Functions"},"type":"lvl2","url":"/2020-04-14-the-substitution-model#substitution-in-functions","position":6},{"hierarchy":{"lvl1":"The Substitution Model","lvl2":"Substitution in Functions"},"content":"","type":"content","url":"/2020-04-14-the-substitution-model#substitution-in-functions","position":7},{"hierarchy":{"lvl1":"The Substitution Model","lvl3":"Substitution Rule of Function Application","lvl2":"Substitution in Functions"},"type":"lvl3","url":"/2020-04-14-the-substitution-model#substitution-rule-of-function-application","position":8},{"hierarchy":{"lvl1":"The Substitution Model","lvl3":"Substitution Rule of Function Application","lvl2":"Substitution in Functions"},"content":"x{e/x} = e\ny{e/x} = y\n(e1 e2){e/x} = e1{e/x} e2{e/x}","type":"content","url":"/2020-04-14-the-substitution-model#substitution-rule-of-function-application","position":9},{"hierarchy":{"lvl1":"The Substitution Model","lvl4":"Call by Value","lvl3":"Substitution Rule of Function Application","lvl2":"Substitution in Functions"},"type":"lvl4","url":"/2020-04-14-the-substitution-model#call-by-value","position":10},{"hierarchy":{"lvl1":"The Substitution Model","lvl4":"Call by Value","lvl3":"Substitution Rule of Function Application","lvl2":"Substitution in Functions"},"content":"e1 e2 ==> v\n  if e1 ==> fun x -> e\n  and e2 ==> v2\n  and e{v2/x} ==> v\n\nIt requires the value of e2 to be evaluated to a value before apply function e1.","type":"content","url":"/2020-04-14-the-substitution-model#call-by-value","position":11},{"hierarchy":{"lvl1":"The Substitution Model","lvl4":"Call by Name","lvl3":"Substitution Rule of Function Application","lvl2":"Substitution in Functions"},"type":"lvl4","url":"/2020-04-14-the-substitution-model#call-by-name","position":12},{"hierarchy":{"lvl1":"The Substitution Model","lvl4":"Call by Name","lvl3":"Substitution Rule of Function Application","lvl2":"Substitution in Functions"},"content":"e1 e2 ==> v\n  if e1 ==> fun x -> e\n  and e{e2/x} ==> v\n\nThis can lead to greater efficiency if the value of e2 is never needed.","type":"content","url":"/2020-04-14-the-substitution-model#call-by-name","position":13},{"hierarchy":{"lvl1":"The Substitution Model","lvl3":"Substitution Rule of Function Definition","lvl2":"Substitution in Functions"},"type":"lvl3","url":"/2020-04-14-the-substitution-model#substitution-rule-of-function-definition","position":14},{"hierarchy":{"lvl1":"The Substitution Model","lvl3":"Substitution Rule of Function Definition","lvl2":"Substitution in Functions"},"content":"","type":"content","url":"/2020-04-14-the-substitution-model#substitution-rule-of-function-definition","position":15},{"hierarchy":{"lvl1":"The Substitution Model","lvl4":"Problem We Encountered (only in Call by Name case)","lvl3":"Substitution Rule of Function Definition","lvl2":"Substitution in Functions"},"type":"lvl4","url":"/2020-04-14-the-substitution-model#problem-we-encountered-only-in-call-by-name-case","position":16},{"hierarchy":{"lvl1":"The Substitution Model","lvl4":"Problem We Encountered (only in Call by Name case)","lvl3":"Substitution Rule of Function Definition","lvl2":"Substitution in Functions"},"content":"(fun x -> e'){e/x} = fun x -> e'\n(fun y -> e'){e/x} = fun y -> e'{e/x}\n\nThese rules are not correct. Consider this following case:(fun y -> x){z/x} = fun y -> x{z/x} = fun y -> z\n(fun z -> x){z/x} = fun z -> x{z/x} = fun z -> z\n\nNote that according to Principle of Name Irrelevance, these two functions are exactly the same function. However, after substitution, the second one became the identity function. Clearly, something went wrong here.\n\nWhat causes this problem is variable capturing. In the second function, the expression we are trying to substitute x with (z in {z/x}) is captured by the function argument (z in fun z -> ...) . Namely, the substitute and the argument, which shouldn’t relate to each other at all, become the same thing in this situation.","type":"content","url":"/2020-04-14-the-substitution-model#problem-we-encountered-only-in-call-by-name-case","position":17},{"hierarchy":{"lvl1":"The Substitution Model","lvl4":"Capture-Avoiding Substitution","lvl3":"Substitution Rule of Function Definition","lvl2":"Substitution in Functions"},"type":"lvl4","url":"/2020-04-14-the-substitution-model#capture-avoiding-substitution","position":18},{"hierarchy":{"lvl1":"The Substitution Model","lvl4":"Capture-Avoiding Substitution","lvl3":"Substitution Rule of Function Definition","lvl2":"Substitution in Functions"},"content":"(fun x -> e'){e/x} = fun x -> e'\n(fun y -> e'){e/x} = fun y -> e'{e/x}  if y is not in FV(e)\n\nFV(e) means free variables of e and represent the variables that are not bound in e. Or you can think of FV as the variables that can be substituted in e. We only step when y is not in FV(e) because it means that y is already bound to something uniquely, so the problem that two different expressions denoted in the same variable bound to a same thing will not happen.FV(x) = {x}\nFV(e1 e2) = FV(e1) + FV(e2)\nFV(fun x -> e) = FV(e) - {x}\n\nThis definition prevents the substitution (fun z -> x){z/x} from occurring, because z is in FV(z).\n\nHowever, this new stepping rule prevents us from stepping when y is in FV(e). In this case, we just have to change y to a different name and the function stays the same according to the Principle of Name Irrelevance. By changing its name, we mean replace every occurrence of y by, say, y1. Notice we say “replace”, not “substitution” as a rule we defined for our model. Absolutely anywhere we see y, we replace it by something else.","type":"content","url":"/2020-04-14-the-substitution-model#capture-avoiding-substitution","position":19},{"hierarchy":{"lvl1":"The Substitution Model","lvl4":"Implementation of Capture-Avoiding","lvl3":"Substitution Rule of Function Definition","lvl2":"Substitution in Functions"},"type":"lvl4","url":"/2020-04-14-the-substitution-model#implementation-of-capture-avoiding","position":20},{"hierarchy":{"lvl1":"The Substitution Model","lvl4":"Implementation of Capture-Avoiding","lvl3":"Substitution Rule of Function Definition","lvl2":"Substitution in Functions"},"content":"Augment the evaluation relation to maintain a stream (i.e., infinite list) of unused variable names. Each time you need a new one, take the head of the stream. But you have to be careful to use the tail of the stream anytime after that. To guarantee that they are unused, reserve some variable names for use by the interpreter alone, and make them illegal as variable names chosen by the programmer. For example, you might decide that programmer variable names may never start with the character $, then have a stream <$x1, $x2, $x3, ...> of fresh names.\n\nUse an imperative counter to simulate the stream from the previous strategy. For example, the following function is guaranteed to return a fresh variable name each time it is called:let gensym =\n  let counter = ref 0 in\n  fun () -> incr counter; \"$x\" ^ string_of_int !counter","type":"content","url":"/2020-04-14-the-substitution-model#implementation-of-capture-avoiding","position":21},{"hierarchy":{"lvl1":"The Environment Model"},"type":"lvl1","url":"/2020-04-16-the-environment-model","position":0},{"hierarchy":{"lvl1":"The Environment Model"},"content":"","type":"content","url":"/2020-04-16-the-environment-model","position":1},{"hierarchy":{"lvl1":"The Environment Model","lvl2":"Step Relation"},"type":"lvl2","url":"/2020-04-16-the-environment-model#step-relation","position":2},{"hierarchy":{"lvl1":"The Environment Model","lvl2":"Step Relation"},"content":"General Rules: don’t step on values; don’t step on variables\n\nSmall/Single Step Relation: e --> e', where e’ can be a value or another expression yet to be evaluated to a value\n\nMulti-Step Relation: e -->* e' is the reflexive closure of single step. It allows single step to be performed 0 or multiple times\n\nBig-Step Relation: e ==> v step expressions to values through multi-step and stop when it becomes values.","type":"content","url":"/2020-04-16-the-environment-model#step-relation","position":3},{"hierarchy":{"lvl1":"The Environment Model","lvl2":"Introduction"},"type":"lvl2","url":"/2020-04-16-the-environment-model#introduction","position":4},{"hierarchy":{"lvl1":"The Environment Model","lvl2":"Introduction"},"content":"For sake of efficiency, it would be better to substitute lazily: only when the value of a variable is needed should the interpreter have to do the substitution. That’s the key idea behind the environment model. In this model, there is a data structure called the dynamic environment, which is a dictionary mapping variable names to values. Whenever the value of a variable is needed, it’s looked up in that dictionary. （空间换时间）\n\nInstead of e ==> v, we now have <env, e> ==> v, where env denotes the environment, and <env, e> is called a machine configuration. That configuration represents the state of the computer as it evaluates a program: env represents a part of the computer’s memory (the binding of variables to values), and e represents the program.\n\n{} represent the empty environment,\n\n{x1:v1, x2:v2, ...} represent the environment that binds x1 to v1, etc.\n\nenv[x -> v] represent the environment env with the variable x additionally bound to the value v\n\nenv(x) represent the binding of x in env.","type":"content","url":"/2020-04-16-the-environment-model#introduction","position":5},{"hierarchy":{"lvl1":"The Environment Model","lvl2":"Evaluation"},"type":"lvl2","url":"/2020-04-16-the-environment-model#evaluation","position":6},{"hierarchy":{"lvl1":"The Environment Model","lvl2":"Evaluation"},"content":"Trivially,<env, x> ==> env(x)\n<env, fun x -> e> ==> fun x -> e","type":"content","url":"/2020-04-16-the-environment-model#evaluation","position":7},{"hierarchy":{"lvl1":"The Environment Model","lvl3":"Problem","lvl2":"Evaluation"},"type":"lvl3","url":"/2020-04-16-the-environment-model#problem","position":8},{"hierarchy":{"lvl1":"The Environment Model","lvl3":"Problem","lvl2":"Evaluation"},"content":"However, when we try to define the rules of function application, we run into trouble. Intuitively, we would write something like this, where we sort of “substitute” all the occurrences of x in e with v2 and get the result v.<env, e1 e2> ==> v\n  if <env, e1> ==> fun x -> e\n  and <env, e2> ==> v2\n  and <env[x -> v2], e> ==> v\n\nlet x = 1 in\nlet f = fun y -> x in\nlet x = 2 in\nf 0\n\nAccording to our semantics thus far, it would evaluate as follows:\n\nlet x = 1 would produce the environment {x:1}.\n\nlet f = fun y -> x would produce the environment {x:1, f:(fun y -> x)}.\n\nlet x = 2 would produce the environment {x:2, f:(fun y -> x)}. Note how the binding of x to 1 is shadowed by the new binding.\n\nNow we would evaluate <{x:2, f:(fun y -> x)}, f 0><{x:2, f:(fun y -> x)}, f 0> ==> 2\n  because <{x:2, f:(fun y -> x)}, f> ==> fun y -> x\n  and <{x:2, f:(fun y -> x)}, 0> ==> 0\n  and <{x:2, f:(fun y -> x)}[y -> 0], x> ==> 2`\n    because <{x:2, f:(fun y -> x), y:0}, x> ==> 2`\n\nThe result is therefore 2.\n\nHowever, this should evaluate to 1. Notice that we evaluated e1 in the current environment, or more formally, we used a dynamic scope here.","type":"content","url":"/2020-04-16-the-environment-model#problem","position":9},{"hierarchy":{"lvl1":"The Environment Model","lvl3":"Dynamic vs. Static Scope","lvl2":"Evaluation"},"type":"lvl3","url":"/2020-04-16-the-environment-model#dynamic-vs-static-scope","position":10},{"hierarchy":{"lvl1":"The Environment Model","lvl3":"Dynamic vs. Static Scope","lvl2":"Evaluation"},"content":"Dynamic Scope: the body of a function is evaluated in the current dynamic environment at the time the function is called, i.e. it uses the latest binding\n\nLexical Scope: the body of a function is evaluated in the old dynamic environment that existed at the time the function was defined.\n\nMost modern languages use lexical scope.","type":"content","url":"/2020-04-16-the-environment-model#dynamic-vs-static-scope","position":11},{"hierarchy":{"lvl1":"The Environment Model","lvl3":"Implementing Time Travel","lvl2":"Evaluation"},"type":"lvl3","url":"/2020-04-16-the-environment-model#implementing-time-travel","position":12},{"hierarchy":{"lvl1":"The Environment Model","lvl3":"Implementing Time Travel","lvl2":"Evaluation"},"content":"We have to store the variable binding at the time the function was defined. We define a new data structure called “closure” with two parts: (|exp, env|), where exp is the expression we are concerned about, and env the environment when the expression was defined.\n\nSince we only have to consider scope in function application, the exp in our closure must always be a function, denoted by fun x -> e. Therefore, we can also write our closure as (|x, exp, env|), where x is the argument name, e is the function body, and env is the environment when this function was defined. In fact, that’s how we define it when implementing the Environment Model.\n\nWhen we use big-step relation in evaluation, we say the result must be a value. Here, we can consider closure to be the value evaluated from a function. <env, fun x -> e> ==> (| fun x -> e, env |)\n\nUsing closure, we can now correctly define evaluation of function applications. (defenv represents the environment when e1 was defined)<env, e1 e2> ==> v\n  if <env, e1> ==> (| fun x -> e, defenv |)\n  and <env, e2> ==> v2\n  and <defenv[x -> v2], e> ==> v","type":"content","url":"/2020-04-16-the-environment-model#implementing-time-travel","position":13},{"hierarchy":{"lvl1":"Machine Learning Basics"},"type":"lvl1","url":"/2021-08-31-machine-learning-basics","position":0},{"hierarchy":{"lvl1":"Machine Learning Basics"},"content":"From Lecture: \n\nML Setupmore","type":"content","url":"/2021-08-31-machine-learning-basics","position":1},{"hierarchy":{"lvl1":"Machine Learning Basics","lvl2":"Basic Definitions"},"type":"lvl2","url":"/2021-08-31-machine-learning-basics#basic-definitions","position":2},{"hierarchy":{"lvl1":"Machine Learning Basics","lvl2":"Basic Definitions"},"content":"Labeled Data D = \\{(\\vec{x_1}, y_1), ..., \\vec{x_n}, y_n)\\} \\sim P^n: 我们常说\"任取n个样本\"，但虽然是\"任\"取的，还是要从某一处取出来。所以 P 的意义是我们的样本分布在一个未知的 distribution P, 我们从这个 P 中抓取 n 个样本，每个样本有 features \\vec{x_i} (因为有多个features所以总体用向量表示) 以及 label \\vec{y}.","type":"content","url":"/2021-08-31-machine-learning-basics#basic-definitions","position":3},{"hierarchy":{"lvl1":"Machine Learning Basics","lvl2":"Machine Learning"},"type":"lvl2","url":"/2021-08-31-machine-learning-basics#machine-learning","position":4},{"hierarchy":{"lvl1":"Machine Learning Basics","lvl2":"Machine Learning"},"content":"","type":"content","url":"/2021-08-31-machine-learning-basics#machine-learning","position":5},{"hierarchy":{"lvl1":"Machine Learning Basics","lvl3":"Train/Validate/Test Split","lvl2":"Machine Learning"},"type":"lvl3","url":"/2021-08-31-machine-learning-basics#train-validate-test-split","position":6},{"hierarchy":{"lvl1":"Machine Learning Basics","lvl3":"Train/Validate/Test Split","lvl2":"Machine Learning"},"content":"Train your model on the Training data set.\n\nOnly run Test set for one time. Because you want to test your model’s generalizability on a dataset representative of the whole distribution. If you train your model to get high score on Test set, it loses its meaning.\n\nIf you want to get a sense of your model’s generalizability, you can have another set called Validate set. Train your model until you get expected score on Validate set and then move on to the Test set.","type":"content","url":"/2021-08-31-machine-learning-basics#train-validate-test-split","position":7},{"hierarchy":{"lvl1":"Machine Learning Basics","lvl3":"Splitting the Data","lvl2":"Machine Learning"},"type":"lvl3","url":"/2021-08-31-machine-learning-basics#splitting-the-data","position":8},{"hierarchy":{"lvl1":"Machine Learning Basics","lvl3":"Splitting the Data","lvl2":"Machine Learning"},"content":"i.i.d. data: uniformly at random\n\ntemporal data: by time (We want to train on the past and predict/test the future)\n\nspecific case: by patient / instance  如果每个病人有多个病历，则我们不能随机分配病历，而要随机分配病人及它们对应的病历，尽量避免同一病人的某些病历在 train 某些病历在 test。这是因为同一病人虽然病历不同，但是他的病症大概率是相同的，要是这个人既出现在 train 里，也出现在 test 里，不就是作弊吗。","type":"content","url":"/2021-08-31-machine-learning-basics#splitting-the-data","position":9},{"hierarchy":{"lvl1":"Machine Learning Basics","lvl3":"No Free Lunch and Making Assumptions","lvl2":"Machine Learning"},"type":"lvl3","url":"/2021-08-31-machine-learning-basics#no-free-lunch-and-making-assumptions","position":10},{"hierarchy":{"lvl1":"Machine Learning Basics","lvl3":"No Free Lunch and Making Assumptions","lvl2":"Machine Learning"},"content":"There is no simple ML algorithm that works for all settings. Each model only works in some specific settings/background. You must make assumptions in order to learn. Each model only works under specific assumptions. Assumptions and settings are actually the same thing here.\n\n“Assumptions” just mean our belief of how our data is distributed. Most of the time when we choose a model, we make assumptions about the distribution our data was drawn from. For example, when we try to determine whether an email is spam or not, we count number of appearance of each word in the email. So we are assuming our features x are drawn from a multimodal distribution. When we use logistic regression, we assume conditional over x has a form like P(y|\\mathbf{x}_i)=\\frac{1}{1+e^{-y(\\mathbf{w}^T \\mathbf{x}_i+b)}}. These are both very bold assumptions and are very likely to fail. When our assumptions fail, our model will do a terrible job in prediction. Therefore, when our algorithm does not perform well, one big reason is that our assumption does not hold.","type":"content","url":"/2021-08-31-machine-learning-basics#no-free-lunch-and-making-assumptions","position":11},{"hierarchy":{"lvl1":"K-Nearest Neighbors"},"type":"lvl1","url":"/2021-09-02-k-nearest-neighbors","position":0},{"hierarchy":{"lvl1":"K-Nearest Neighbors"},"content":"","type":"content","url":"/2021-09-02-k-nearest-neighbors","position":1},{"hierarchy":{"lvl1":"K-Nearest Neighbors","lvl2":"The k-NN algorithm"},"type":"lvl2","url":"/2021-09-02-k-nearest-neighbors#the-k-nn-algorithm","position":2},{"hierarchy":{"lvl1":"K-Nearest Neighbors","lvl2":"The k-NN algorithm"},"content":"Assumption: Similar inputs have similar outputs / Similar points have similar labels.\n\nClassification rule: For a test input \\mathbf{x}, assign the most common label amongst its k most similar training inputs\n\nFormal Definition:\n\nFor a point \\mathbf{x}, denote the set of the k nearest neighbors of \\mathbf{x} as  S_\\mathbf{x}. Formally, S_\\mathbf{x}\\subseteq {D} s.t. |S_\\mathbf{x}|=k and  \\forall(\\mathbf{x}',y')\\in D\\backslash S_\\mathbf{x}, \\text{dist}(\\mathbf{x},\\mathbf{x}')\\ge\\max_{(\\mathbf{x}'',y'')\\in  S_\\mathbf{x}} \\text{dist}(\\mathbf{x},\\mathbf{x}'')\n\nClassifier h(\\mathbf{x}) takes in a test point and returns the most common label in S_\\mathbf{x}:[h(\\mathbf{x})=\\text{mode}(\\{y'':(\\mathbf{x}'',y'')\\in S_\\mathbf{x}\\})\n\nHow does k affect the classifier?\n\nAs k \\uparrow, decision boundary gets smoother.  When k=n, we always output mode\\{D\\}; if k =1, we always output test point’s nearest point.","type":"content","url":"/2021-09-02-k-nearest-neighbors#the-k-nn-algorithm","position":3},{"hierarchy":{"lvl1":"K-Nearest Neighbors","lvl2":"What distance function should we use?"},"type":"lvl2","url":"/2021-09-02-k-nearest-neighbors#what-distance-function-should-we-use","position":4},{"hierarchy":{"lvl1":"K-Nearest Neighbors","lvl2":"What distance function should we use?"},"content":"The k-nearest neighbor classifier fundamentally relies on a distance metric. The better that metric reflects label similarity, the better the classified will be. The most common choice is the Minkowski distance:\\text{dist}(\\mathbf{x},\\mathbf{z})=\\left(\\sum_{r=1}^d |x_r-z_r|^p\\right)^{1/p}\n\np = 1: Absolute Value / Manhattan Distance\n\np = 2: Euclidian Distance\n\np \\to \\infty: Max (其他项都会被最大项的 |x_r-z_r|^p 给 shadow 掉，再取 ...^{1/p} 回到 |x_r-z_r|)","type":"content","url":"/2021-09-02-k-nearest-neighbors#what-distance-function-should-we-use","position":5},{"hierarchy":{"lvl1":"K-Nearest Neighbors","lvl2":"Brief digression: Bayes optimal classifier"},"type":"lvl2","url":"/2021-09-02-k-nearest-neighbors#brief-digression-bayes-optimal-classifier","position":6},{"hierarchy":{"lvl1":"K-Nearest Neighbors","lvl2":"Brief digression: Bayes optimal classifier"},"content":"Assume (and this is almost never the case) we have all knowledge about the distribution. That is we know P(X,Y), the distribution from which our samples are drawn. we can then get every value we want. For example, if we want to have \\mathrm{P}(y|\\mathbf{x}) we just have to use the Bayes rule and calculate \\frac{P(\\mathbf{x},y)}{P(\\mathbf{x})}, where P(\\mathbf{x}) = \\int_{y'} P(\\mathbf{x},y').\n\nTherefore, if we know this ultimate distribution, which gives us \\mathrm{P}(y|\\mathbf{x}), then you would simply predict the most likely label:\\text{The Bayes optimal classifier predicts:}\\ y^* =  h_\\mathrm{opt}(\\mathbf{x}) = \\operatorname*{argmax}_y P(y|\\mathbf{x})\n\n即我们已知对于每个输入 x，输出各个 y 的可能性，那么给一个 x 只需输出最可能的 y 即可\n\nAlthough the Bayes optimal classifier is as good as it gets, it still can make mistakes. It is always wrong if a sample does not have the most likely label. We can compute the probability of that happening  precisely (which is exactly the error rate):\\epsilon_{BayesOpt}=1-\\mathrm{P}(h_\\mathrm{opt}(\\mathbf{x})|\\mathbf{x}) = 1- \\mathrm{P}(y^*|\\mathbf{x})\n\nAssume for example an email \\mathbf{x} can either be classified as  spam (+1) or ham (-1). For the same email \\mathbf{x} the conditional class probabilities are: \\mathrm{P}(+1| \\mathbf{x})=0.8\\\\        \\mathrm{P}(-1| \\mathbf{x})=0.2\\\\\n\n In this case the Bayes optimal classifier would predict the label  y^*=+1 as it is most likely, and its error rate would be  \\epsilon_{BayesOpt}=0.2. 也就是说，有0.2的邮件即使看起来很像 spam 但它们并不是 spam\n\nBayes optimal classifier is an lower bound on error: no model can do better than the Bayes optimal classifier.\n\nWe also have constant classifier as an upper bound on error: constant classifier always predicts the same constant - most common label in the training set (equivalent to a KNN with k = n). Your model should always do better than the constant classifier.","type":"content","url":"/2021-09-02-k-nearest-neighbors#brief-digression-bayes-optimal-classifier","position":7},{"hierarchy":{"lvl1":"K-Nearest Neighbors","lvl2":"1-NN Convergence Proof"},"type":"lvl2","url":"/2021-09-02-k-nearest-neighbors#id-1-nn-convergence-proof","position":8},{"hierarchy":{"lvl1":"K-Nearest Neighbors","lvl2":"1-NN Convergence Proof"},"content":"Cover and Hart 1967[\n\n1]: As n \\to \\infty, the 1-NN error is no more than twice the error of the Bayes Optimal classifier. (Similar guarantees hold for k>1.)\n\nn small\n\nn large\n\nn\\to\\infty\n\n\n\n\n\n\n\nLet \\mathbf{x}_\\mathrm{NN} be the nearest neighbor of our test point \\mathbf{x}_\\mathrm{t}. As n \\to \\infty, \\text{dist}(\\mathbf{x}_\\mathrm{NN},\\mathbf{x}_\\mathrm{t}) \\to 0, i.e. \\mathbf{x}_\\mathrm{NN} \\to \\mathbf{x}_{t}. (Since there are just too many points, the nearest neighbor will eventually become identical to our test point)\n\nSince we are doing 1NN, we return the label of \\mathbf{x}_\\mathrm{NN}. Consider the error rate: the probability that label of \\mathbf{x}_\\mathrm{NN} is not the label of \\mathbf{x}_\\mathrm{t}. This happens when test point has the most common label it should have, but NN has something else. Or when NN has the most common label it should have, but test point has something else.\n$$\\begin{align} \\epsilon_{NN} &= \\mathrm{P}(y^* | \\mathbf{x}_\\mathrm{t})(1-\\mathrm{P}(y^* | \\mathbf{x}_\\mathrm{NN})) + \\mathrm{P}(y^* | \\mathbf{x}_\\mathrm{NN})(1-\\mathrm{P}(y^* | \\mathbf{x}_\\mathrm{t})) \\\\ \n&\\le (1-\\mathrm{P}(y^* | \\mathbf{x}_\\mathrm{NN}))+(1-\\mathrm{P}(y^* | \\mathbf{x}_\\mathrm{t})) \\\\\n&= 2(1-\\mathrm{P}(y^* | \\mathbf{x}_\\mathrm{t}) \\\\\n&= 2\\epsilon_\\mathrm{BayesOpt}\n\\end{align}\n\n$\nwhere we used that \\mathrm{P}(y^* | \\mathbf{x}\\mathrm{t})=\\mathrm{P}(y^* | \\mathbf{x}\\mathrm{NN}) because we have assumption: \\mathbf{x}\\mathrm{NN} \\to \\mathbf{x}\\mathrm{t}$.\n\nGood news:  As n \\to\\infty, the 1-NN classifier is only a factor 2 worse than the best possible classifier.\n\nBad news: We are cursed!!","type":"content","url":"/2021-09-02-k-nearest-neighbors#id-1-nn-convergence-proof","position":9},{"hierarchy":{"lvl1":"K-Nearest Neighbors","lvl2":"Curse of Dimensionality"},"type":"lvl2","url":"/2021-09-02-k-nearest-neighbors#curse-of-dimensionality","position":10},{"hierarchy":{"lvl1":"K-Nearest Neighbors","lvl2":"Curse of Dimensionality"},"content":"","type":"content","url":"/2021-09-02-k-nearest-neighbors#curse-of-dimensionality","position":11},{"hierarchy":{"lvl1":"K-Nearest Neighbors","lvl3":"Distances between points","lvl2":"Curse of Dimensionality"},"type":"lvl3","url":"/2021-09-02-k-nearest-neighbors#distances-between-points","position":12},{"hierarchy":{"lvl1":"K-Nearest Neighbors","lvl3":"Distances between points","lvl2":"Curse of Dimensionality"},"content":"\n\nFigure demonstrating  \"the curse of  dimensionality’'. The histogram plots show the distributions of all pairwise distances between randomly distributed points within d-dimensional unit  squares.  As the number of dimensions d grows, all distances  concentrate within a very small range.\n\nThe kNN classifier makes the assumption that similar points share similar labels. Unfortunately, in high dimensional spaces, points that are drawn from a probability distribution, tend to never be close together. Intuition: 每增加一维度，点之间的距离就再变大一点。\n\n我们假设所有 training data 都在高维的某个 unit hypercube 中。假设 d=10, 对于某个 test point, 我们需要考虑离它最近的10个邻居。有了这10个邻居后，找到一个最小的能够包裹这10个邻居 hypercube，设它的边长为 \\ell, 我们来看看这个 hypercube 的体积。\n\nFormally, imagine the unit cube [0,1]^d. All training data is sampled uniformly within this cube, i.e. \\forall i, x_i\\in[0,1]^d, and we are considering the k=10 nearest neighbors of such a test point. Let \\ell be the edge length of the smallest hyper-cube that contains all k-nearest neighbor of a test point. Then \\ell^d\\approx\\frac{k}{n} and \\ell\\approx\\left(\\frac{k}{n}\\right)^{1/d}. If n= 1000, how big is \\ell?\n\nd\n\n\\ell\n\n2\n\n0.1\n\n10\n\n0.63\n\n100\n\n0.955\n\n1000\n\n0.9954\n\nSo as d\\gg 0 almost the entire space is needed to find the 10-NN.  This breaks down the k-NN assumptions, because the k-NN are not  particularly closer (and therefore more similar) than any other data  points in the training set.\n\nOne might think that one rescue could be to increase the number of  training samples, n, until the nearest neighbors are truly close to  the test point. How many data points would we need such that \\ell  becomes truly small? Fix \\ell=\\frac{1}{10}=0.1 \\Rightarrow n=\\frac{k}{\\ell^d}=k\\cdot  10^d, which grows exponentially! For d>100 we would need far more data  points than there are electrons in the universe...","type":"content","url":"/2021-09-02-k-nearest-neighbors#distances-between-points","position":13},{"hierarchy":{"lvl1":"K-Nearest Neighbors","lvl3":"Distances to hyperplanes","lvl2":"Curse of Dimensionality"},"type":"lvl3","url":"/2021-09-02-k-nearest-neighbors#distances-to-hyperplanes","position":14},{"hierarchy":{"lvl1":"K-Nearest Neighbors","lvl3":"Distances to hyperplanes","lvl2":"Curse of Dimensionality"},"content":"How about the distance to a hyperplane?  Consider the following figure. There are two blue points and a red  hyperplane. The following plot shows the scenario in 2d and the right plot in 3d. The distance to the red hyperplane remains unchanged as the third dimension is added. The reason is that the normal of the hyper-plane is orthogonal to the new dimension. This is a crucial observation.\n\n**In d dimensions, d-1 dimensions will be orthogonal to the normal of any given hyper-plane. 这d-1个维度对离hyperplane的距离毫无影响 **Movement in those dimensions cannot increase or decrease the distance to the hyperplane --- the points just shift around and remain at the same distance. As distances between pairwise points become very large in high dimensional spaces, distances to hyperplanes become comparatively tiny.\n\n​    \n\nThe curse of dimensionality has different effects on distances between two points and distances between points and hyperplanes.\n\n​    \n\nAn animation illustrating the effect  on randomly sampled data points in 2D, as a 3rd dimension is added (with random coordinates). As the points expand along the 3rd dimension they  spread out and their pairwise distances increase. However, their  distance to the hyper-plane (z=0.5) remains unchanged --- so in relative terms the distance from the data points to the hyper-plane shrinks  compared to their respective nearest neighbors.","type":"content","url":"/2021-09-02-k-nearest-neighbors#distances-to-hyperplanes","position":15},{"hierarchy":{"lvl1":"K-Nearest Neighbors","lvl3":"Data with low dimensional structure","lvl2":"Curse of Dimensionality"},"type":"lvl3","url":"/2021-09-02-k-nearest-neighbors#data-with-low-dimensional-structure","position":16},{"hierarchy":{"lvl1":"K-Nearest Neighbors","lvl3":"Data with low dimensional structure","lvl2":"Curse of Dimensionality"},"content":"However, not all is lost. Data may lie in low dimensional subspace or on sub-manifolds. The true dimensionality of the  data can be much lower than its ambient space. For example, an image of a face may require 18M pixels, a person may be able to describe this person with less than 50 attributes (e.g. male/female, blond/dark hair, …) along which faces vary.\n\n​    \n\nAn example of a data set in 3d that is drawn from an underlying 2-dimensional manifold. The blue points are  confined to the pink surface area, which is embedded in a 3-dimensional  ambient space.\n\n也就是说，前文提到的 curse of dimensionality 并不对实际模型应用起到太大印象，这是因为前面所讲的都是在随机的情况下，然而实际应用的可被预测的数据一般都具有一些隐含的结构（不然它怎么被预测呢）","type":"content","url":"/2021-09-02-k-nearest-neighbors#data-with-low-dimensional-structure","position":17},{"hierarchy":{"lvl1":"K-Nearest Neighbors","lvl2":"k-NN summary"},"type":"lvl2","url":"/2021-09-02-k-nearest-neighbors#k-nn-summary","position":18},{"hierarchy":{"lvl1":"K-Nearest Neighbors","lvl2":"k-NN summary"},"content":"k-NN is a simple and effective classifier if distances reliably  reflect a semantically meaningful notion of the dissimilarity. (It  becomes truly competitive through metric learning)\n\nAs n \\to \\infty, k-NN becomes provably very accurate, but also very slow.\n\nAs d \\gg 0, points drawn from a probability distribution stop  being similar to each other, and the kNN assumption breaks down.","type":"content","url":"/2021-09-02-k-nearest-neighbors#k-nn-summary","position":19},{"hierarchy":{"lvl1":"K-means clustering"},"type":"lvl1","url":"/2021-09-07-k-means-clustering","position":0},{"hierarchy":{"lvl1":"K-means clustering"},"content":"In unsupervised learning, we now only have features (x_{1},\\ldots,x_{n}, where x_{i} \\in \\mathbb{R}^{d}) and no corresponding labels y_{i}. Therefore, we are not looking to make predictions. Instead, we are  interested in uncovering structure in the feature vectors themselves.\n\nIn this lecture, we introduce K-means clustering. As we saw with KNN, sometimes we may suffer from the [curse of dimensionality](./2021-09-02-K-Nearest-Neighbors.md/#Curse of Dimenisionality), but any dataset that we can make predictions on must have “interesting” structures.  K-Means will discover the clustered structure of the high dimensional data. (This is a bit different from \n\nPCA, which discovers low dimensional structure. PCA transforms high dimensional data points to be desribed by some fewer lower dimensional vectors. K-Means, on the other hand, describes a data point using its relation with each clusters: x_i \\to [c_1, c_2, \\dots, c_k] where c_i describes x_i’s relation with cluster c_i)","type":"content","url":"/2021-09-07-k-means-clustering","position":1},{"hierarchy":{"lvl1":"K-means clustering","lvl2":"Objective"},"type":"lvl2","url":"/2021-09-07-k-means-clustering#objective","position":2},{"hierarchy":{"lvl1":"K-means clustering","lvl2":"Objective"},"content":"Given a set of n data points x_{1},\\ldots,x_{n} with  x_{i} \\in \\mathbb{R}^{d} ,  the goal of k-means clustering is to partition the data into  k  groups where each group contains\nsimilar objects as defined by their Euclidean distances. Formally, we want to partition the n feature vectors into k clusters C_j.\n\nWe now define what it means for a clustering to be “good.” We want to find clusters where the data points within each cluster are tightly grouped (i.e., exhibit low variability). Recall  \\parallel x \\parallel_{2}^{2} = x^{T}x, so  \\parallel x_{i} - x_{j} \\parallel_{2}  is the euclidean distance between the vectors  x_{i}  and  x_{j}.  Mathematically, for any cluster assignment we defineZ(\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{k}) = \\sum\\limits_{\\ell = 1}^{k}\\frac{1}{2|\\mathcal{C}_{\\ell}|}\\sum\\limits_{i,j \\in \\mathcal{C}_{\\ell}} \\parallel x_{i} - x_{j} \\parallel_{2}^{2}\n\nThe smaller  Z  is, the better we consider the clustering. Roughly speaking, we are trying to pick a clustering that minimizes the pairwise squared distances within each cluster (normalized by the cluster sizes).\n\nThe definition of  Z  can be rewritten in terms of the centroid of each cluster:\n$$\\begin{align}\nZ(\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{k}) \n&= \\sum\\limits_{\\ell = 1}^{k} Z_k \\\\\n&= \\sum\\limits_{\\ell = 1}^{k}\\sum\\limits_{i \\in \\mathcal{C}_{\\ell}} \\parallel x_{i} - \\mu_{\\ell} \\parallel_{2}^{2}\n\\end{align}where\n\n\\mu_{\\ell} = \\frac{1}{|\\mathcal{C}{\\ell}|}\\sum\\limits{i \\in \\mathcal{C}{\\ell}}x{i},\nZ_\\ell = \\sum\\limits_{i \\in \\mathcal{C}{\\ell}} \\parallel x{i} - \\mu_{\\ell} \\parallel_{2}^{2}\n$ \\mu_{\\ell} is the mean/centroid of the points in cluster  \\ell and Z_\\ell$ 为各 cluster 中所有点与其对应 centroid 的距离和\n\nNow that we have a formal way to define a good clustering of points, we would like to solve\\min\\limits_{\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{k}}Z(\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{k})\n\nUnfortunately, solving this equation is not computationally feasible. We cannot do much better then simply trying all possible  k  way partitions of the data and seeing which one yields the smallest objective value, so we have to relax our goal of finding the “best” clustering.","type":"content","url":"/2021-09-07-k-means-clustering#objective","position":3},{"hierarchy":{"lvl1":"K-means clustering","lvl2":"Lloyd’s algorithm for k-means"},"type":"lvl2","url":"/2021-09-07-k-means-clustering#lloyds-algorithm-for-k-means","position":4},{"hierarchy":{"lvl1":"K-means clustering","lvl2":"Lloyd’s algorithm for k-means"},"content":"The practical approach we will take finding a good clustering is to use a local search algorithm.\n\nLocal Search: 给一个现有的解决方案, interactively update it to a better and better clustering until it converges\n\nConverge: Do another local search, if our update result yields the same clusters we already have, we have a converge\n\nThis does not ensure convergence to the global solution.\n\nInput: data  \\{ x_{i}\\}_{i = 1}^{n}  and  k \n\nInitialize  C_{1},\\ldots,C_{k} \n\nWhile not converged:\n\nCompute  \\mu_{\\ell} = \\frac{1}{|\\mathcal{C}_{\\ell}|}\\sum\\limits_{i \\in \\mathcal{C}_{\\ell}}x_{i}  for  \\ell = 1,\\ldots,k.  [O(dn) - 每个 feature vector 都对应一个 centroid]\n\nUpdate  \\mathcal{C}_{1},\\ldots,\\mathcal{C}_{k}  by assigning each data point  x_{i}  to the cluster whose centroid  \\mu_{\\ell}  it is closest to. [O(dnk) - 每个 feature vector 都要与 k 个 centroid 算距离]\n\nIf the cluster assignments didn’t change, we have converged.\n\nReturn: cluster assignments  \\mathcal{C}_{1},\\ldots,\\mathcal{C}_{k} \n\nThis process actually has a nice feature that the objective function Z is non-increasing (and only fails to decrease if the cluster assignments have converged). This is like loop invariant - termination.\n\nThe cost of Lloyd’s algorithm is  \\mathcal{O}(ndk)  per iteration. 注意每次计算（加法或乘法）花费的时间都与 dimension d = |x_i| 有关。In practice its efficiency is strongly dependent on how many iterations it takes to converge.","type":"content","url":"/2021-09-07-k-means-clustering#lloyds-algorithm-for-k-means","position":5},{"hierarchy":{"lvl1":"K-means clustering","lvl2":"How many clusters and how they are initialized"},"type":"lvl2","url":"/2021-09-07-k-means-clustering#how-many-clusters-and-how-they-are-initialized","position":6},{"hierarchy":{"lvl1":"K-means clustering","lvl2":"How many clusters and how they are initialized"},"content":"These 2 are the implicit input / super parameters we have to decide for a KNN algorithm.","type":"content","url":"/2021-09-07-k-means-clustering#how-many-clusters-and-how-they-are-initialized","position":7},{"hierarchy":{"lvl1":"K-means clustering","lvl3":"Cluster Number k","lvl2":"How many clusters and how they are initialized"},"type":"lvl3","url":"/2021-09-07-k-means-clustering#cluster-number-k","position":8},{"hierarchy":{"lvl1":"K-means clustering","lvl3":"Cluster Number k","lvl2":"How many clusters and how they are initialized"},"content":"We want to pick a k that minimizes Z_k = Z(\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{k})\n\nHowever, this is problematic because we expect the in-cluster variation to decrease naturally with  k.  In fact, if we have exactly n clusters,  Z_{n} = 0  since every point is its own cluster. So, instead, we often look for the point at which  Z_{k}  stops decreasing quickly.\n\nThe intuition is that as  k\\rightarrow k + 1  if a single big cluster naturally subdivides into two clear clusters we will see that  Z_{k + 1} \\ll Z_{k}. In contrast, if we see  Z_{k + 1} \\approx Z_{k}, this might mean we have spitted up a clear cluster that does not naturally subdivide.","type":"content","url":"/2021-09-07-k-means-clustering#cluster-number-k","position":9},{"hierarchy":{"lvl1":"K-means clustering","lvl3":"Initial Cluster Assignment","lvl2":"How many clusters and how they are initialized"},"type":"lvl3","url":"/2021-09-07-k-means-clustering#initial-cluster-assignment","position":10},{"hierarchy":{"lvl1":"K-means clustering","lvl3":"Initial Cluster Assignment","lvl2":"How many clusters and how they are initialized"},"content":"Initial cluster assignment This is particularly important because Lloyd’s algorithm only finds a local optima. So, very different initialization could\nresult in convergence to very different solutions.\n\nWe can\n\nChoose several random clusterings and choose the one that gives the best result.\n\nChoose some random centroids, assign the rest points according to a weighted probability so that the farther they are from a centroid, the more likely they will be assigned to it. This is based on the idea that the cluster centers should be somewhat spread out.","type":"content","url":"/2021-09-07-k-means-clustering#initial-cluster-assignment","position":11},{"hierarchy":{"lvl1":"K-means clustering","lvl2":"Decision boundaries"},"type":"lvl2","url":"/2021-09-07-k-means-clustering#decision-boundaries","position":12},{"hierarchy":{"lvl1":"K-means clustering","lvl2":"Decision boundaries"},"content":"As said at the very beginning, unsupervised learning algorithm finds underlying structure of data points.\n\nObserve that given two cluster centroids  \\mu_{\\ell} and \\mu_{\\ell^{\\prime}} , the set of points equidistant from the two centroids defined as  \\{ x \\in \\mathbb{R}^{d}\\;|\\; \\parallel x - \\mu_{\\ell} \\parallel_{2} = \\parallel x - \\mu_{\\ell^{\\prime}} \\parallel_{2}\\} is a line (就是  \\mu_{\\ell} 和 \\mu_{\\ell^{\\prime}}  的中位线). This means that given a set of cluster centroids  \\mu_{i},\\ldots,\\mu_{k}  they implicitly partition space up into  k  domains whose boundaries are linear.","type":"content","url":"/2021-09-07-k-means-clustering#decision-boundaries","position":13},{"hierarchy":{"lvl1":"Principal Component Analysis"},"type":"lvl1","url":"/2021-09-09-principal-component-analysis","position":0},{"hierarchy":{"lvl1":"Principal Component Analysis"},"content":"PCA finds the low dimensional structure of high dimensional data.","type":"content","url":"/2021-09-09-principal-component-analysis","position":1},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl2":"Basic Ideas"},"type":"lvl2","url":"/2021-09-09-principal-component-analysis#basic-ideas","position":2},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl2":"Basic Ideas"},"content":"Whereas k-means clustering sought to partition the data into homogeneous subgroups, principal component analysis (PCA) will seek to find, if it exists, low-dimensional structure in the data set  \\{ x\\}_{i = 1}^{n}  (x_{i} \\in \\mathbb{R}^{d} ). The features in which the data vary the most describes this lower-dimensional subspace.\n\nPCA finds a low-dimensional representation of the data that captures most of the interesting behavior. Here, “interesting” will be defined as variability. This is analogous to computing “composite” features (i.e. linear combinations of entries in each  x_{i}  that explain most of the variability in the data.)\n\nTake a simple example: if there exists one unit vector  u \\in \\mathbb{R}^{d}  such that  x_{i} \\approx \\alpha_{i}u  for some scalar  \\alpha_{i}, then we can roughly represent all x_i by a much smaller number of “features.” (in this case only one u) If we accept that u is a valid “feature” for all of our data, we could approximate our data using only the scalar  \\alpha_{i} to describe how x_i behaves on that feature u. More concisely, we say that the  x_{i}  approximately lie in span\\{u\\}, a subspace of dimension 1.\n\nThis is illustrated in the next figure, where we see two dimensional data that approximately lies in a one dimensional subspace span\\{v\\}. If we know what v is, we can approximate all data using only \\alpha.","type":"content","url":"/2021-09-09-principal-component-analysis#basic-ideas","position":3},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl2":"Centering the data"},"type":"lvl2","url":"/2021-09-09-principal-component-analysis#centering-the-data","position":4},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl2":"Centering the data"},"content":"Typically in unsupervised learning, we are interested in understanding relationships between data points and not necessarily bulk properties of the data. If the data has a sufficiently large mean, i.e.  \\mu = \\frac{1}{n}\\sum\\limits_{i}x_{i}  is sufficiently far from zero, the best approximation of each data point is roughly \\mu.\n\nTherefore, to actually understand the relationship between features we would like to center our data before applying PCA by deleting the mean from data. Specifically, we let{\\hat{x}}_{i} = x_{i} - \\mu\n\nwhere  \\mu = \\frac{1}{n}\\sum\\limits_{i}x_{i}.  We now simply work with the centered feature vectors  \\{{\\hat{x}}_{i}\\}_{i = 1}^{n}  and will do so throughout the remainder of these notes.","type":"content","url":"/2021-09-09-principal-component-analysis#centering-the-data","position":5},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl2":"Maximizing the variance"},"type":"lvl2","url":"/2021-09-09-principal-component-analysis#maximizing-the-variance","position":6},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl2":"Maximizing the variance"},"content":"","type":"content","url":"/2021-09-09-principal-component-analysis#maximizing-the-variance","position":7},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl3":"Describing Problem","lvl2":"Maximizing the variance"},"type":"lvl3","url":"/2021-09-09-principal-component-analysis#describing-problem","position":8},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl3":"Describing Problem","lvl2":"Maximizing the variance"},"content":"To do PCA, we want to find a small set of composite features that capture most of the variability in the data. To illustrate this point, we will first consider finding a single composite feature that captures the most variability in the data set, then proceed to find the 2nd one, 3rd one...\n\nMathematically, we want to find a vector \\phi = [\\alpha_1, \\dots, \\alpha_d]\\in \\mathbb{R}^{d}  such that the sample variance of the scalarsz_{i} = \\phi^{T}{\\hat{x}}_{i}\n\nis as large as possible. Note \\phi here is the weight: given a data point x_i, for each dimension [x_i]_j we assign it a weight \\alpha_j, so we have a weighted sum across dimension of the original data point x_i. We try to come up with a composite feature that varies a lot, so we want to maximize the variance of z_i. Note the mean \\hat\\mu_z of these z is 0 because we did the normalization previous step.Var(z_i) = (z_i - \\hat\\mu_z)^2 = (z_i - 0)^2 = (\\phi^{T}{\\hat{x}}_{i})^2\n\nWe also have to constrain  \\parallel \\phi \\parallel_{2} = 1, or we could artificially inflate the variance by simply increasing the magnitude of the entries in  \\phi.\n\nWe can now formally define the first principal component of a data set:\n\nFirst principal component: The first principal component of a data set  \\{ x_{i}\\}_{i = 1}^{n}  is the vector  \\phi \\in \\mathbb{R}^{d}  that solves\\max\\limits_{\\parallel \\phi \\parallel_{2} = 1}\\frac{1}{n}\\sum\\limits_{i = 1}^{n}(\\phi^{T}{\\hat{x}}_{i})^{2}\n\nWe will consider the data matrix \\hat{X} = \\begin{bmatrix}\n| & & | \\\\\n{\\hat{x}}_{1} & \\cdots & {\\hat{x}}_{n} \\\\\n| & & | \\\\\n\\end{bmatrix} \\in \\mathbb{R}^{d \\times n}\n\n. This allows us to rephrase the problem as\\max\\limits_{\\parallel \\phi \\parallel_{2} = 1} \\parallel {\\hat{X}}^{T}\\phi \\parallel_{2}\n\nIn other words,  \\phi  is the unit vector that makes the matrix  {\\hat{X}}^{T}  as large as possible.","type":"content","url":"/2021-09-09-principal-component-analysis#describing-problem","position":9},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl3":"Solving Problem - 1st PC","lvl2":"Maximizing the variance"},"type":"lvl3","url":"/2021-09-09-principal-component-analysis#solving-problem-1st-pc","position":10},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl3":"Solving Problem - 1st PC","lvl2":"Maximizing the variance"},"content":"We will show how to find the first PC (Principal Components) in this section. We can use singular value decomposition (SVD). To simplify this presentation we make the reasonable assumption that  n \\geq d. \nWe can always decompose the matrix  \\hat{X}  as\\hat{X} = U\\Sigma V^{T}\n\nwhere U  is a  d \\times d  orthonormal matrix,  V is an n \\times d  orthonormal matrix,  \\Sigma is a d \\times d  diagonal matrix with  \\Sigma_{ii} = \\sigma_{i} \\geq 0,   and  \\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{d}. Remind again we have n data vectors. Each vector has d dimension.U = \\begin{bmatrix}\n| & & | \\\\\nu_{1} & \\cdots & u_{d} \\\\\n| & & | \\\\\n\\end{bmatrix},\\quad \\Sigma = \\begin{bmatrix}\n\\sigma_{1} & & \\\\\n & \\ddots & \\\\\n & & \\sigma_{d} \\\\\n\\end{bmatrix},\\quad\\text{and } V = \\begin{bmatrix}\n| & & | \\\\\nv_{1} & \\cdots & v_{d} \\\\\n| & & | \\\\\n\\end{bmatrix}\\quad\n\nwe call  u_{i}  the left singular vectors,  v_{i}  the right singular vectors, and  \\sigma_{i}  the singular values. This SVD can be done in  \\mathcal{O}(nd^{2}). 这些都是 SVD 的定义而已\n\nClaim: we achieve max \\parallel {\\hat{X}}^{T}\\phi \\parallel_{2} when we have  \\phi = u_{1}.\n\nProof: Note U is a  d \\times d  orthonormal matrix, so its columns constitute a base of \\mathbb{R}^d. Since we defined \\phi \\in \\mathbb{R}^d, we can write \\phi as a combination of these bases:\\begin{align}\n\\phi &= \\sum\\limits_{i = 1}^{d}a_{i}u_{i}  \\\\\n&= U\\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\dots \\\\ a_d \\end{bmatrix}\n\\end{align}\n\nwhere  \\sum\\limits_{i}a_{i}^{2} = 1  (because we want  \\parallel \\phi \\parallel_{2} = 1 ). We now observe\\begin{align}\n\\Sigma U^{T} \\phi &= \\Sigma U^T U \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\dots \\\\ a_d \\end{bmatrix} \n= \\Sigma (U^T U) \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\dots \\\\ a_d \\end{bmatrix} \n= \\Sigma I \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\dots \\\\ a_d \\end{bmatrix} \n= \\begin{bmatrix} \\sigma_1 a_1 \\\\ \\sigma_2 a_2 \\\\ \\dots \\\\ \\sigma_d a_d \\\\ \\end{bmatrix} \\\\\n\\end{align}\n\nTherefore,\n\n$$\\begin{align}\n{\\parallel {\\hat{X}}^{T}\\phi \\parallel_{2}^{2}} \n &= \\parallel V \\Sigma U^{T}\\phi \\parallel_{2}^{2} \\\\\n &= \\left\\| \\begin{bmatrix}\n| & & | \\\\\nv_{1} & \\cdots & v_{d} \\\\\n| & & | \\\\\n\\end{bmatrix} \\begin{bmatrix} \\sigma_1 a_1 \\\\ \\sigma_2 a_2 \\\\ \\dots \\\\ \\sigma_d a_d \\\\ \\end{bmatrix} \\right\\|_{2}^{2} \\\\\n\n&= \\left\\|\n\\sigma_1 a_1 \\begin{bmatrix} | \\\\ v_{1}\\\\ | \\\\ \\end{bmatrix} \n  + \\sigma_2 a_2 \\begin{bmatrix} | \\\\ v_{2}\\\\ | \\\\ \\end{bmatrix} \n  + \\dots\n  + \\sigma_d a_d \\begin{bmatrix} | \\\\ v_{d}\\\\ | \\\\ \\end{bmatrix} \n\\right\\|_{2}^{2} \\\\\n\n &= \\sum\\limits_{i = 1}^{d}(\\sigma_{i}a_{i})^{2} \\\\\n\\end{align}\n\n$$\n\nAccording to assumption  \\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{d}, we achieve max when  a_{1} = 1 and a_{i} = 0  for  i \\neq 1.  证毕","type":"content","url":"/2021-09-09-principal-component-analysis#solving-problem-1st-pc","position":11},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl3":"Solving Problem - Other PCs","lvl2":"Maximizing the variance"},"type":"lvl3","url":"/2021-09-09-principal-component-analysis#solving-problem-other-pcs","position":12},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl3":"Solving Problem - Other PCs","lvl2":"Maximizing the variance"},"content":"So, the first left singular value of  \\hat{X}  gives us the first principal component of the data. What about finding additional directions? Formally, we want to find  \\psi  such that  y_{i} = \\psi^{T}{\\hat{x}}_{i} has the next most variability. 但我们又不能简单地说 \\psi \\not= \\phi, 因为我们可以随便找一个间隔 \\phi 非常近的 \\phi 即可。 Therefore, we need to force the second PC to be orthogonal to the first, i.e.  \\psi^{T}\\phi = 0 . We can actually find the second principal component is  \\psi = u_{2} in our SVD matrix U.  Fig.\n\n11 illustrates how the first two principal components look\nfor a stylized data set. We see that they reveal directions in which the\ndata varies significantly.\n\nMore generally, the SVD actually gives us all the principal components of the data set  \\{ x_{i}\\}_{i = 1}^{n}.\n\nPrincipal components: Principal component  \\ell  of data set  \\{ x_{i}\\}_{i = 1}^{n}  is denoted  \\phi_{\\ell}  and satisfies\\phi_{\\ell} = u_{\\ell}\n\nwhere  \\hat{X} = U\\Sigma V^{T}  is the SVD of  \\hat{X}. ","type":"content","url":"/2021-09-09-principal-component-analysis#solving-problem-other-pcs","position":13},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl2":"Explaining variability in the data"},"type":"lvl2","url":"/2021-09-09-principal-component-analysis#explaining-variability-in-the-data","position":14},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl2":"Explaining variability in the data"},"content":"Having obtained the answer, we can look back into our problem \\max\\limits_{\\parallel \\phi \\parallel_{2} = 1} \\parallel {\\hat{X}}^{T}\\phi \\parallel_{2} in a cleaner way.\\begin{align}\n({\\hat{X}}^{T}\\phi)^2 \n &= ({\\hat{X}}^{T}\\phi)^T ({\\hat{X}}^{T}\\phi) \\\\\n &= \\phi^T{\\hat{X}}{\\hat{X}}^{T}\\phi \\\\\n &= \\phi^T \\sigma \\phi &&\\text{we set $\\phi$ as a eigenvector of ${\\hat{X}}{\\hat{X}}^{T}$} \\\\\n &= \\sigma \\phi^T \\phi \\\\\n  &= \\sigma &&\\parallel \\phi \\parallel_{2} = 1\n\\end{align}\n\nThe equation above helps us recall that we were maximizing the variance of this vector {\\hat{X}}^{T}\\phi and the maximized variance is the eigenvalue of matrix {\\hat{X}}{\\hat{X}}^{T}. In other words, the singular values reveal the sample variance of \\phi_\\ell^Tx_i. This result generalizes to PCs other than the first one.\n\nIf we project our data into the span of the first k eigenvectors, the variance we can achieve is the first k eigenvalues summed up Formally, the total variability of the data captured by the first k PC is  \\sum\\limits_{i = 1}^{k}\\sigma_{i}^{2}. This is the value we kept with k interesting composite dimension.\n\nOne way we determine k - how many vectors we want for PCA is to look at how much of the variance at all dimensions are captured by our k composite interesting dimensions:\\frac{\\sum\\limits_{i = 1}^{k}\\sigma_{i}^{2}}{\\sum\\limits_{i = 1}^{d}\\sigma_{i}^{2}}\n= \\frac{\\text{variance at $k$ composite interesting dimensions}}{\\text{variance at all dimensions}}\n= \\frac{\\text{variance captured by $k$ PC}}{\\text{total variance in data}}\n\nWe want to pick a relatively small k such that a relatively big ratio can be achieved. In practice, we can do a similar thing as we did with K-Means: we can pick  k  by identifying when we have diminishing returns in explaining variance by adding more principal components, i.e. looking for a knee in the plot of singular values.","type":"content","url":"/2021-09-09-principal-component-analysis#explaining-variability-in-the-data","position":15},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl2":"PCA Application"},"type":"lvl2","url":"/2021-09-09-principal-component-analysis#pca-application","position":16},{"hierarchy":{"lvl1":"Principal Component Analysis","lvl2":"PCA Application"},"content":"We use PCA to reduce dimensions.\n\nA common use of PCA is for data visualization. In particular, if we have high dimensional data that is hard to visualize we can sometimes see key features of the data by plotting its projection onto a few (1, 2, or 3) principal components. For example, if  d = 2  this corresponds to forming a scatter plot of  (\\phi_{1}^{T}{\\hat{x}}_{i},\\phi_{2}^{T}{\\hat{x}}_{i}).","type":"content","url":"/2021-09-09-principal-component-analysis#pca-application","position":17},{"hierarchy":{"lvl1":"The Perceptron"},"type":"lvl1","url":"/2021-09-14-the-perceptron","position":0},{"hierarchy":{"lvl1":"The Perceptron"},"content":"","type":"content","url":"/2021-09-14-the-perceptron","position":1},{"hierarchy":{"lvl1":"The Perceptron","lvl2":"Assumptions"},"type":"lvl2","url":"/2021-09-14-the-perceptron#assumptions","position":2},{"hierarchy":{"lvl1":"The Perceptron","lvl2":"Assumptions"},"content":"Binary classification (i.e. y_i \\in \\{-1, +1\\})\n\nData is linearly separable","type":"content","url":"/2021-09-14-the-perceptron#assumptions","position":3},{"hierarchy":{"lvl1":"The Perceptron","lvl2":"Classifier"},"type":"lvl2","url":"/2021-09-14-the-perceptron#classifier","position":4},{"hierarchy":{"lvl1":"The Perceptron","lvl2":"Classifier"},"content":"h(x_i) = \\textrm{sign}(\\mathbf{w}^\\top \\mathbf{x}_i + b)\n\nw is the normal vector of the hyperplane. It defines this hyperplane. b is the bias term (without the bias term, the hyperplane that  \\mathbf{w} defines would always have to go through the origin). Dealing with b can be a pain, so we ‘absorb’ it into the feature  vector \\mathbf{w} by adding one additional constant dimension. Under this convention,\\mathbf{x}_i \\hspace{0.1in} \\text{becomes} \\hspace{0.1in}  \\begin{bmatrix} \\mathbf{x}_i \\\\ 1  \\end{bmatrix} \\\\ \\mathbf{w} \\hspace{0.1in} \\text{becomes} \\hspace{0.1in} \\begin{bmatrix}  \\mathbf{w} \\\\ b  \\end{bmatrix}\n\nWe can verify that\\begin{bmatrix} \\mathbf{x}_i \\\\ 1  \\end{bmatrix}^\\top \\begin{bmatrix}  \\mathbf{w} \\\\ b  \\end{bmatrix} = \\mathbf{w}^\\top \\mathbf{x}_i + b\n\n于是，我们通过把 b 藏在一个新的维度中“消除”了 b. Using this, we can simplify the above formulation of h(\\mathbf{x}_i)  toh(\\mathbf{x}_i) = \\textrm{sign}(\\mathbf{w}^\\top \\mathbf{x})\n\n\n\n在这里我们看到两个例子，一个原数据在一维，一个原数据在二维。(Left:) 我们无法找到一个过原点的平面将两个数据分开 (Right:) 但在我们再加入一维度时 (x_i \\to \\begin{bmatrix} x_i \\\\1 \\end{bmatrix})，就可以找到过原点的分隔平面了\n\nObservation: Note thaty_i(\\mathbf{w}^\\top \\mathbf{x}_i) > 0 \\iff \\text{classification }(\\mathbf{w}^\\top \\mathbf{x}_i) \\text{ and result } y_i \\text{ has the same sign } \\iff \\mathbf{x}_i \\hspace{0.1in} \\text{is classified correctly}\n\nwhere “classified correctly” means that x_i is on the correct side of the hyperplane defined by \\mathbf{w}. Also, note that the left side depends on y_i \\in {-1, +1} (it wouldn’t work if, for example y_i \\in {0, +1}).","type":"content","url":"/2021-09-14-the-perceptron#classifier","position":5},{"hierarchy":{"lvl1":"The Perceptron","lvl2":"Perceptron Algorithm"},"type":"lvl2","url":"/2021-09-14-the-perceptron#perceptron-algorithm","position":6},{"hierarchy":{"lvl1":"The Perceptron","lvl2":"Perceptron Algorithm"},"content":"","type":"content","url":"/2021-09-14-the-perceptron#perceptron-algorithm","position":7},{"hierarchy":{"lvl1":"The Perceptron","lvl3":"An Intuitive Example","lvl2":"Perceptron Algorithm"},"type":"lvl3","url":"/2021-09-14-the-perceptron#an-intuitive-example","position":8},{"hierarchy":{"lvl1":"The Perceptron","lvl3":"An Intuitive Example","lvl2":"Perceptron Algorithm"},"content":"\n\n(Left:) The  hyperplane defined by \\mathbf{w}_t misclassifies one red (-1) and one  blue (+1) point. (Middle:) The red point \\mathbf{x} is chosen and used for an update. Because its label is -1 we need to subtract  \\mathbf{x} from \\mathbf{w}_t. (Right:) The udpated hyperplane  \\mathbf{w}_{t+1}=\\mathbf{w}_t-\\mathbf{x} separates the two classes and the Perceptron algorithm has converged.","type":"content","url":"/2021-09-14-the-perceptron#an-intuitive-example","position":9},{"hierarchy":{"lvl1":"The Perceptron","lvl3":"How did we update w?","lvl2":"Perceptron Algorithm"},"type":"lvl3","url":"/2021-09-14-the-perceptron#how-did-we-update-w","position":10},{"hierarchy":{"lvl1":"The Perceptron","lvl3":"How did we update w?","lvl2":"Perceptron Algorithm"},"content":"Why do we want to update w by setting \\vec{w} \\prime = \\vec{w} + y\\vec{x}?\n\nNote we updated w based on point x because we classified x incorrectly, so we want to move to a more correct direction. Let’s look at the classification result of x after such an update: \\vec{w}\\prime \\cdot \\vec{x}=  (\\vec{w} + y\\vec{x}) \\cdot \\vec{x} = \\vec{w} \\cdot \\vec{x} + y\\vec{x}^2 \\gt  \\vec{w} \\cdot \\vec{x}. Though we still do not know whether x is now labelled correctly (\\vec{w}\\prime \\cdot \\vec{x}), but at least we know the classification result is somewhat better because it increased a bit.","type":"content","url":"/2021-09-14-the-perceptron#how-did-we-update-w","position":11},{"hierarchy":{"lvl1":"The Perceptron","lvl2":"Proving Perceptron Converges"},"type":"lvl2","url":"/2021-09-14-the-perceptron#proving-perceptron-converges","position":12},{"hierarchy":{"lvl1":"The Perceptron","lvl2":"Proving Perceptron Converges"},"content":"The Perceptron  was arguably the first algorithm with a strong formal guarantee.","type":"content","url":"/2021-09-14-the-perceptron#proving-perceptron-converges","position":13},{"hierarchy":{"lvl1":"The Perceptron","lvl3":"Claim","lvl2":"Proving Perceptron Converges"},"type":"lvl3","url":"/2021-09-14-the-perceptron#claim","position":14},{"hierarchy":{"lvl1":"The Perceptron","lvl3":"Claim","lvl2":"Proving Perceptron Converges"},"content":"If a data set is linearly separable, the Perceptron will find a separating hyperplane in a finite number of updates. (If the data is not linearly separable, it will loop forever.)","type":"content","url":"/2021-09-14-the-perceptron#claim","position":15},{"hierarchy":{"lvl1":"The Perceptron","lvl3":"Setup","lvl2":"Proving Perceptron Converges"},"type":"lvl3","url":"/2021-09-14-the-perceptron#setup","position":16},{"hierarchy":{"lvl1":"The Perceptron","lvl3":"Setup","lvl2":"Proving Perceptron Converges"},"content":"The argument goes as follows: Suppose the answer classification hyperplane exists, so \\exists \\mathbf{w}^* such that \\forall (\\mathbf{x}_i, y_i) \\in D, y_i(\\mathbf{x}^\\top \\mathbf{w}^*  ) > 0.\n\nNow, suppose that we rescale each data point and the \\mathbf{w}^* such that all data points are within a unit hypersphere and w^* normal vector of hyperplane is exactly on the unit sphere.||\\mathbf{w}^*|| = 1 \\hspace{0.3in} \\text{and} \\hspace{0.3in} \\forall \\mathbf{x}_i \\in D \\hspace{0.1in}||\\mathbf{x}_i|| \\le 1\n\nLet us define the Margin \\gamma of the hyperplane \\mathbf{w}^*  as  \\gamma = \\min_{(\\mathbf{x}_i, y_i) \\in D}|\\mathbf{x}_i^\\top \\mathbf{w}^*|: 即所有数据点中离 hyperplane 最近的距离\n\nObserve: \\forall\\mathbf{x}, \\text{we have } y(\\mathbf{x}^\\top  \\mathbf{w}^*)=|\\mathbf{x}^\\top \\mathbf{w}^*|\\geq \\gamma. Because  \\mathbf{w}^* is a perfect classifier, so all training data points  (\\mathbf{x},y) lie on the “correct” side of the hyper-plane and  therefore y=sign(\\mathbf{x}^\\top \\mathbf{w}^*). The second inequality  follows directly from the definition of the margin \\gamma.\n\nTo summarize our setup:\n\nAll inputs \\mathbf{x}_i live within the unit sphere\n\nThere exists a separating hyperplane defined by \\mathbf{w}^*,  with \\|\\mathbf{w}^*\\|=1 (i.e. \\mathbf{w}^*  lies exactly on the unit sphere).\n\n\\gamma is the distance from this hyperplane (blue) to the closest data point.","type":"content","url":"/2021-09-14-the-perceptron#setup","position":17},{"hierarchy":{"lvl1":"The Perceptron","lvl3":"WTS","lvl2":"Proving Perceptron Converges"},"type":"lvl3","url":"/2021-09-14-the-perceptron#wts","position":18},{"hierarchy":{"lvl1":"The Perceptron","lvl3":"WTS","lvl2":"Proving Perceptron Converges"},"content":"If all of the above holds, then the Perceptron algorithm makes at most 1 / \\gamma^2 mistakes.","type":"content","url":"/2021-09-14-the-perceptron#wts","position":19},{"hierarchy":{"lvl1":"The Perceptron","lvl3":"Strategy","lvl2":"Proving Perceptron Converges"},"type":"lvl3","url":"/2021-09-14-the-perceptron#strategy","position":20},{"hierarchy":{"lvl1":"The Perceptron","lvl3":"Strategy","lvl2":"Proving Perceptron Converges"},"content":"Keeping what we defined above, consider the effect of an update  (\\mathbf{w} becomes \\mathbf{w}+y\\mathbf{x})  on the two terms  \\mathbf{w}^\\top \\mathbf{w}^* and \\mathbf{w}^\\top \\mathbf{w}.\n\n\\mathbf{w} \\cdot \\mathbf{w}^*= \\mathbf{w}^\\top \\mathbf{w}^*: 我们希望它大，即我们的平面 \\mathbf{w}^\\top  和正确答案平面 \\mathbf{w}^* 的方向越接近越好\n\n\\mathbf{w}\\cdot\\mathbf{w} = \\mathbf{w}^\\top \\mathbf{w}: 但同时，我们希望这个值小，因为我们不希望 \\mathbf{w}\\cdot\\mathbf{w}^* 大了是单纯因为 \\mathbf{w} 的大小变大了 (\\|\\mathbf{w}^*\\|=1 但谁都没规定 \\mathbf{w} 的大小)","type":"content","url":"/2021-09-14-the-perceptron#strategy","position":21},{"hierarchy":{"lvl1":"The Perceptron","lvl3":"Proof","lvl2":"Proving Perceptron Converges"},"type":"lvl3","url":"/2021-09-14-the-perceptron#proof","position":22},{"hierarchy":{"lvl1":"The Perceptron","lvl3":"Proof","lvl2":"Proving Perceptron Converges"},"content":"We will use two facts:\n\ny( \\mathbf{x}^\\top  \\mathbf{w})\\leq 0: This holds because  \\mathbf x is misclassified by \\mathbf{w} - otherwise we wouldn’t make the update.\n\ny( \\mathbf{x}^\\top  \\mathbf{w}^*) \\ge \\gamma \\gt0: First holds directly from definition of margin \\gamma; second holds because \\mathbf{w}^* is a separating hyper-plane and classifies all points correctly.\n\nCall the updated plane \\mathbf{w}\\prime = \\mathbf{w} + y\\mathbf{x}\n\nConsider the effect of an update on \\mathbf{w}^\\top \\mathbf{w}^*:\\mathbf{w}\\prime^\\top  \\mathbf{w}^* = (\\mathbf{w} + y\\mathbf{x})^\\top \\mathbf{w}^*= \\mathbf{w}^\\top  \\mathbf{w}^* + y(\\mathbf{x}^\\top  \\mathbf{w}^*) \\ge \\mathbf{w}^\\top  \\mathbf{w}^* + \\gamma\n\nThe inequality follows from the fact that, for \\mathbf{w}^*, the  distance from the hyperplane defined by \\mathbf{w}^* to \\mathbf{x}  must be at least \\gamma (i.e. y (\\mathbf{x}^\\top   \\mathbf{w}^*)=|\\mathbf{x}^\\top \\mathbf{w}^*|\\geq \\gamma).\n\nhis means that for each update, \\mathbf{w}^\\top \\mathbf{w}^* grows by at least \\gamma.\n\nConsider the effect of an update on \\mathbf{w}^\\top \\mathbf{w}:\\mathbf{w}\\prime^\\top \\mathbf{w}\\prime\n  = (\\mathbf{w} + y\\mathbf{x})^\\top   (\\mathbf{w} + y\\mathbf{x})\n  =  \\mathbf{w}^\\top \\mathbf{w} +  \\underbrace{2y(\\mathbf{w}^\\top\\mathbf{x})}_{<0} +  \\underbrace{y^2(\\mathbf{x}^\\top  \\mathbf{x})}_{0\\leq \\ \\text{this value} \\ \\leq 1}\n  \\le  \\mathbf{w}^\\top \\mathbf{w} + 1\n\nThe inequality follows from the fact that\n\n2y(\\mathbf{w}^\\top  \\mathbf{x}) < 0 as we had to make an update, meaning \\mathbf{x} was misclassified\n\n0\\leq y^2(\\mathbf{x}^\\top  \\mathbf{x}) \\le 1 as y^2 = 1 and all \\mathbf{x}^\\top  \\mathbf{x}\\leq 1 (because \\|\\mathbf  x\\|\\leq 1).\n\nThis means that for each update, \\mathbf{w}^\\top \\mathbf{w} grows by at most 1.\n\nNow remember from the Perceptron algorithm that we initialize  \\mathbf{w}=\\mathbf{0}. Hence, initially \\mathbf{w}^\\top\\mathbf{w}=0  and \\mathbf{w}^\\top\\mathbf{w}^*=0 and after M updates the following  two inequalities must hold:\n\n\\mathbf{w}^\\top\\mathbf{w}^*\\geq M\\gamma\n\n\\mathbf{w}^\\top \\mathbf{w}\\leq M.\n\nWe can then complete the proof:\\begin{align}\n     M\\gamma\n     &\\le \\mathbf{w}^\\top \\mathbf{w}^* &&\\text{By (1)} \\\\\n     &=\\|\\mathbf{w}\\|\\cos(\\theta) && \\text{by definition of  inner-product, where $\\theta$ is the angle between $\\mathbf{w}$ and  $\\mathbf{w}^*$.}\\\\\n     &\\leq ||\\mathbf{w}|| &&\\text{by definition of $\\cos$, we  must have $\\cos(\\theta)\\leq 1$.} \\\\\n     &= \\sqrt{\\mathbf{w}^\\top \\mathbf{w}} && \\text{by definition  of $\\|\\mathbf{w}\\|$} \\\\\n     &\\le \\sqrt{M} &&\\text{By (2)} \\\\  \n     & \\textrm{ }\\\\\n     &\\Rightarrow M\\gamma \\le \\sqrt{M} \\\\\n     &\\Rightarrow M^2\\gamma^2 \\le M \\\\\n     &\\Rightarrow M \\le \\frac{1}{\\gamma^2} && \\text{And hence,  the number of updates $M$ is bounded from above by a constant.}\n     \\end{align}","type":"content","url":"/2021-09-14-the-perceptron#proof","position":23},{"hierarchy":{"lvl1":"The Perceptron","lvl2":"Problems and History"},"type":"lvl2","url":"/2021-09-14-the-perceptron#problems-and-history","position":24},{"hierarchy":{"lvl1":"The Perceptron","lvl2":"Problems and History"},"content":"Perceptron suffers from the limitation of a linear model: If no separating hyperplane exists, perceptron will NEVER converge. (There will always be some points on the wrong side and it will iterate forever)\n\nIt does not generalize well either. Though it corrects all data points correctly, the decision boundary is almost arbitrary. And if a test point is given, Perceptron will very likely to classify it to the wrong side.\n\nEspecially suffered from the 1st problem, AI winter came as a result.","type":"content","url":"/2021-09-14-the-perceptron#problems-and-history","position":25},{"hierarchy":{"lvl1":"MLE and MAP"},"type":"lvl1","url":"/2021-09-16-mle-and-map","position":0},{"hierarchy":{"lvl1":"MLE and MAP"},"content":"We will talk about two ways of estimating a parameter: MLE and MAP. Their formulas look really similar to generative learning and discriminative as if one corresponded to generative and the other corresponded to discriminative. However, MLE/MAP has nothing to do with generative/discriminative learning. They just look similar. That’s all.","type":"content","url":"/2021-09-16-mle-and-map","position":1},{"hierarchy":{"lvl1":"MLE and MAP","lvl2":"MLE and Coin Toss"},"type":"lvl2","url":"/2021-09-16-mle-and-map#mle-and-coin-toss","position":2},{"hierarchy":{"lvl1":"MLE and MAP","lvl2":"MLE and Coin Toss"},"content":"Say we have a coin, and we want to find \\theta = P(H): the probability that this coin comes up heads when we toss it.\n\nWe toss it n = 10 times and obtain the following sequence of  outcomes: D=\\{H, T, T, H, H, H, T, T, T, T\\}. Therefore, we observed n_H=4 heads and n_T=6 tails. So, intuitively,  P(H) \\approx \\frac{n_H}{n_H + n_T} = \\frac{4}{10}= 0.4\n\n. We will derive this more formally with Maximum Likelihood Estimation (MLE)","type":"content","url":"/2021-09-16-mle-and-map#mle-and-coin-toss","position":3},{"hierarchy":{"lvl1":"MLE and MAP","lvl3":"Maximum Likelihood Estimation (MLE)","lvl2":"MLE and Coin Toss"},"type":"lvl3","url":"/2021-09-16-mle-and-map#maximum-likelihood-estimation-mle","position":4},{"hierarchy":{"lvl1":"MLE and MAP","lvl3":"Maximum Likelihood Estimation (MLE)","lvl2":"MLE and Coin Toss"},"content":"The estimator we just mentioned is the Maximum Likelihood Estimate. In particular, we want to find a distribution that makes the data we observed as likely as possible. MLE is done in two steps:\n\nMake an explicit modeling assumption about what type of distribution your data was sampled from.\n\nSet the parameters of this distribution so that the data you observed is as likely as possible.\n\nBefore proceeding to MLE, we already observed our data. Namely, in our coin example, D, n, n_H, n_T are are set values.\n\nWe will assume that coin toss is of binomial distribution Bin(n,\\theta), where n is number of tosses we made and \\theta is the probability coming up head we are trying to estimate. Formally, given this is a binomial distribution with probability \\theta, we have\\begin{align} P(D\\mid \\theta) &= \\begin{pmatrix} n_H + n_T \\\\  n_H  \\end{pmatrix}  \\theta^{n_H} (1 - \\theta)^{n_T},  \\end{align}\n\nMLE Principle: Find \\hat{\\theta} to maximize P(D; \\theta), the likelihood of the data, :\\begin{align} \\hat{\\theta}_{MLE} &= \\operatorname*{argmax}_{\\theta} \\,P(D ; \\theta)  \\end{align}\n\nNote we have P(D;\\theta) here. ; means \\theta is a parameter of this distribution, just like \\mu, \\gamma are parameters in \\mathcal{N}(\\mu, \\gamma). This will be different from how we view \\theta in MAP we will talk later. You can still write this as  P(D\\mid \\theta), but just remember when we say “given \\theta” in a MLE context, we don’t mean \\theta is a Random Variable. It should be treated as a parameter.\n\nA common procedure we take to solve a maximum problem:\n\nWe don’t want to see all these products, so take the \\log of the function to convert them into sums.\n\nCompute its derivative, and equate it with zero to find the extreme point.\n\nApplying this procedure:\\begin{align} \\hat{\\theta}_{MLE} \n&= \\operatorname*{argmax}_{\\theta} \\,P(D;  \\theta) \\\\  \n&= \\operatorname*{argmax}_{\\theta} \\begin{pmatrix} n_H + n_T \\\\  n_H \\end{pmatrix} \\theta^{n_H} (1 - \\theta)^{n_T} \\\\\n&= \\operatorname*{argmax}_{\\theta} \\,\\log\\begin{pmatrix} n_H + n_T  \\\\ n_H \\end{pmatrix} + n_H \\cdot \\log(\\theta) + n_T \\cdot \\log(1 -  \\theta) && \\text{combinatorial is just constant}\\\\ \n&= \\operatorname*{argmax}_{\\theta} \\, n_H \\cdot \\log(\\theta) + n_T  \\cdot \\log(1 - \\theta) \\end{align}\n\nWe can then solve for \\theta by taking the derivative and equating it with zero. This results in\\begin{align} \\frac{n_H}{\\theta} = \\frac{n_T}{1 - \\theta} \\Longrightarrow n_H -  n_H\\theta = n_T\\theta \\Longrightarrow \\theta = \\frac{n_H}{n_H + n_T} \\end{align}","type":"content","url":"/2021-09-16-mle-and-map#maximum-likelihood-estimation-mle","position":5},{"hierarchy":{"lvl1":"MLE and MAP","lvl3":"Smoothing","lvl2":"MLE and Coin Toss"},"type":"lvl3","url":"/2021-09-16-mle-and-map#smoothing","position":6},{"hierarchy":{"lvl1":"MLE and MAP","lvl3":"Smoothing","lvl2":"MLE and Coin Toss"},"content":"If n is large and your model/distribution is correct, then MLE finds the true parameters, but the MLE can overfit the data if n is small. It works well when n is large. For example, suppose you observe H,H,H,H,H.  \\hat{\\theta}_{MLE} = \\frac{n_H}{n_H + n_T}= \\frac{5}{5} = 1.\n\nSimple fix: We can add m imaginary throws and get some imaginary results based on our intuition about the distribution. For example, if we have a hunch that that \\theta is close to 0.5. Call our intuition probability \\theta'. The m imaginary throws will result in m\\theta' heads and m(1-\\theta') tails. Add these imaginary results to our data, so\\hat{\\theta} =  \\frac{n_H + m\\theta'}{n_H + n_T + m}\n\nFor large n, this is an insignificant change. For small n, it incorporates your “prior belief” about what \\theta should be.\n\nThis idea of “prior belief” actually gives rise to another class of thinking about distribution.","type":"content","url":"/2021-09-16-mle-and-map#smoothing","position":7},{"hierarchy":{"lvl1":"MLE and MAP","lvl2":"MAP and Coin Toss with Prior Knowledge"},"type":"lvl2","url":"/2021-09-16-mle-and-map#map-and-coin-toss-with-prior-knowledge","position":8},{"hierarchy":{"lvl1":"MLE and MAP","lvl2":"MAP and Coin Toss with Prior Knowledge"},"content":"","type":"content","url":"/2021-09-16-mle-and-map#map-and-coin-toss-with-prior-knowledge","position":9},{"hierarchy":{"lvl1":"MLE and MAP","lvl3":"The Bayesian Way","lvl2":"MAP and Coin Toss with Prior Knowledge"},"type":"lvl3","url":"/2021-09-16-mle-and-map#the-bayesian-way","position":10},{"hierarchy":{"lvl1":"MLE and MAP","lvl3":"The Bayesian Way","lvl2":"MAP and Coin Toss with Prior Knowledge"},"content":"Frequentists think \\theta is the probability distribution from where our data is drawn. Therefore, it should be treated as a constant, something we cannot control.\n\nBayesians consider it just another random variable, whose distribution can reflect our prior knowledge / assumption about the data distribution.\n\nFormally, they model \\theta as a random variable, drawn from a distribution P(\\theta). Note that \\theta is not a random variable associated  with an event in a sample space. You can specify a prior belief P(\\theta) defining what values you believe \\theta is likely to take on.\n\nNow, we can look at P(\\theta \\mid D) = \\frac{P(D\\mid \\theta) P(\\theta)}{P(D)} , where\n\nP(\\theta) is the prior distribution over the parameter(s) \\theta - what I believe before seeing any data.\n\nP(D \\mid \\theta) is the likelihood of the data given the parameter(s) \\theta - same as in MLE\n\nP(\\theta \\mid D) is the posterior distribution over the parameter(s) \\theta -  what I believe after observing the data.\n\nP(D) is the total probability of drawing such a data considering all possible distributions. P(D) = \\int_{\\theta'}P(D|\\theta')P(\\theta'), but this value is just a constant in our maximization problem and we can ignore it.\n\nA prior distribution reflects our uncertainty about the true value of p before observing the coin tosses. After the experiment is performed and the data are gathered, the prior distribution is updated using Bayes’ rule; this yields the posterior distribution, which reflects our new beliefs about p.\n\nStory 8.3.3 (Beta-Binomial conjugacy) from Introduction to Probability - Joseph K. Blitzstein, Jessica Hwang\n\nA natural choice for the prior P(\\theta) is the Beta distribution\\begin{align} P(\\theta) = \\frac{\\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1}}{B(\\alpha, \\beta)} \\end{align}\n\nwhere B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)  \\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} is the normalization constant. Why do we choose this complicated thing as our prior belief?\n\nIf we choose Beta Distribution as our prior distribution, we will see later that after performing an experiment of binomial distribution and when we want to update to the posterior distribution, the posterior distribution of \\theta after observing n_H is still a Beta distribution! This is a special relationship between the Beta and Binomial distributions called conjugacy: if we have a Beta prior distribution on p and data that are conditionally Binomial given p, then when going from prior to posterior, we don’t leave the family of Beta distributions. We say that the Beta is the conjugate prior of the Binomial.\n\n8.3.3 (Beta-Binomial conjugacy) from Introduction to Probability - Joseph K. Blitzstein, Jessica Hwang\n\nFor a Beta Distribution P(\\theta) \\propto \\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1}. \\alpha 和 \\beta 就是 \\theta 和 1-\\theta 两个方向的权重, relatively\n\n\\alpha \\gt \\beta: \\theta \\to 1\n\n\\beta \\gt \\alpha : \\theta \\to 0\n\n\\beta = \\alpha : \\theta \\to 0.5\n\n假设 \\alpha =3, \\beta=1, P(\\theta) = \\propto \\theta^{2}(1 - \\theta)^{0} = \\theta^{2}. 观察到 \\theta 越大 P(\\theta) 越大，即越大的  \\theta 越容易被取到","type":"content","url":"/2021-09-16-mle-and-map#the-bayesian-way","position":11},{"hierarchy":{"lvl1":"MLE and MAP","lvl3":"Maximum a Posteriori Probability Estimation (MAP)","lvl2":"MAP and Coin Toss with Prior Knowledge"},"type":"lvl3","url":"/2021-09-16-mle-and-map#maximum-a-posteriori-probability-estimation-map","position":12},{"hierarchy":{"lvl1":"MLE and MAP","lvl3":"Maximum a Posteriori Probability Estimation (MAP)","lvl2":"MAP and Coin Toss with Prior Knowledge"},"content":"MAP Principle: Find \\hat{\\theta} that maximizes the posterior distribution P(\\theta \\mid D):\\begin{align}\n \\hat{\\theta}_{MAP} &= \\operatorname*{argmax}_{\\theta} \\,P(\\theta \\mid D) \\\\\n\t\t\t\t\t&= \\operatorname*{argmax}_{\\theta} \\frac{P(D\\mid \\theta) P(\\theta)}{P(D)} && \\text{By Bayes rule} \\\\\n\t\t\t\t\t&= \\operatorname*{argmax}_{\\theta} \\, \\log P(D \\mid \\theta) + \\log P(\\theta) && \\text{Ignore Constant $D$, Take log}\n\\end{align}\n\nFor out coin flipping scenario, we get:\\begin{align}\n \\hat{\\theta}_{MAP} &= \\operatorname*{argmax}_{\\theta} \\;P(\\theta | Data) \\\\\n&= \\operatorname*{argmax}_{\\theta} \\;\\log(P(Data | \\theta)) + \\log(P(\\theta)) \\\\\n&= \\operatorname*{argmax}_{\\theta} \\;n_H \\cdot \\log(\\theta) + n_T \\cdot \\log(1 - \\theta) + (\\alpha - 1)\\cdot \\log(\\theta) + (\\beta - 1) \\cdot \\log(1 - \\theta) \\\\\n&= \\operatorname*{argmax}_{\\theta} \\;(n_H + \\alpha - 1) \\cdot \\log(\\theta) + (n_T + \\beta - 1) \\cdot \\log(1 - \\theta) \\\\\n&\\Longrightarrow  \\hat{\\theta}_{MAP} = \\frac{n_H + \\alpha - 1}{n_H + n_T + \\beta + \\alpha - 2}\n\\end{align}\n\nA similar calculation will give us\\begin{align} P(\\theta \\mid D) \\propto P(D \\mid \\theta) P(\\theta) \\propto \\theta^{n_H + \\alpha -1} (1 - \\theta)^{n_T + \\beta -1} \\end{align}\n\nWe notice:\n\nThe MAP estimate is identical to MLE with \\alpha-1 hallucinated heads and \\beta-1 hallucinated tails\n\nAs n \\rightarrow \\infty, \\hat\\theta_{MAP} \\rightarrow  \\hat\\theta_{MLE} as our prior knowledge \\alpha-1 and \\beta-1 become irrelevant compared to very large n_H,n_T.\n\nMAP is a great estimator if an accurate prior belief is available (and mathematically tractable).\n\nThe posterior distribution can act as the prior if we subsequently observe additional data. To see this, we can imagine taking observations one at a time and after each observation updating the current posterior distribution by multiplying by the likelihood function for the new observation and then normalizing to obtain the new, revised posterior distribution. This sequential approach to learning arises naturally when we adopt a Bayesian viewpoint.\n\n2.1.1 (The beta distribution) from Pattern Recognition and Machine Learning - Christopher Bishop","type":"content","url":"/2021-09-16-mle-and-map#maximum-a-posteriori-probability-estimation-map","position":13},{"hierarchy":{"lvl1":"MLE and MAP","lvl2":"“True” Bayesian approach"},"type":"lvl2","url":"/2021-09-16-mle-and-map#true-bayesian-approach","position":14},{"hierarchy":{"lvl1":"MLE and MAP","lvl2":"“True” Bayesian approach"},"content":"Note that MAP is only one way to get an estimator in Bayesian way. There is much more information in P(\\theta \\mid D) and we threw away most of them by taking only the max. A true Bayesian approach is to use the posterior predictive distribution P(\\theta\\mid D) directly to make prediction about the label Y of a test sample X based on dataset D. Simply put, it integrates over all possible models \\theta to make a prediction. Mathematically, we can represent it as:\\begin{align}\nP[Y\\mid (D,X)]\n= &\\int_{\\theta}P[(Y,\\theta) \\mid (D,X)] \\hspace{0.05in} d\\theta && (P(A) = \\int_{B}P(A,b) db) \\\\\n= &\\int_{\\theta} P[(Y \\mid \\theta), (D,X)] \\hspace{0.05in} P(\\theta | D) \\hspace{0.05in} d\\theta && \\textrm{(Chain rule: $P(A,B|C)=P(A|B,C)P(B|C)$)}\n\\end{align}\n\nIntuition behind each step is:\\begin{align}\n&P[Y\\mid (D,X)] && \\text{given our data $D$ and a test point $X$, estimate x's label $Y$} \\\\\n= &\\int_{\\theta}P[(Y,\\theta) \\mid (D,X)] \\hspace{0.05in} d\\theta  && \\text{how do we estimate? we use some model $\\theta$} \\\\\n= &\\int_{\\theta} P[Y \\mid (\\theta,D,X)] \\hspace{0.05in} P(\\theta | D) \\hspace{0.05in} d\\theta && \\text{how do we get $\\theta$? based on training data $D$}\n\\end{align}\n\nUnfortunately, the above is generally intractable.","type":"content","url":"/2021-09-16-mle-and-map#true-bayesian-approach","position":15},{"hierarchy":{"lvl1":"MLE and MAP","lvl2":"Summary"},"type":"lvl2","url":"/2021-09-16-mle-and-map#summary","position":16},{"hierarchy":{"lvl1":"MLE and MAP","lvl2":"Summary"},"content":"MLE Prediction: P[(y|x_t);\\theta] Learning:  \\theta=\\operatorname*{argmax}_\\theta P(D;\\theta). Here \\theta is purely a model parameter.\n\nMAP Prediction: P[y|(x_t,\\theta)] Learning:  \\theta=\\operatorname*{argmax}_\\theta P(\\theta|D)\\propto P(D \\mid  \\theta) P(\\theta). Here \\theta is a random variable.\n\n“True Bayesian” Prediction:  P(y|x_t,D)=\\int_{\\theta}P(y|\\theta)P(\\theta|D)d\\theta. Here \\theta  is integrated out - our prediction takes all possible models into  account.\n\nAs always the differences are subtle. In MLE we maximize \\log\\left[P(D;\\theta)\\right] in MAP we maximize  \\log\\left[P(D|\\theta)\\right]+\\log\\left[P(\\theta)\\right]. So  essentially in MAP we only add the term \\log\\left[P(\\theta)\\right] to  our optimization. This term is independent of the data and penalizes if the parameters, \\theta deviate too much from what we believe is reasonable.","type":"content","url":"/2021-09-16-mle-and-map#summary","position":17},{"hierarchy":{"lvl1":"Naive Bayes"},"type":"lvl1","url":"/2021-09-21-naive-bayes","position":0},{"hierarchy":{"lvl1":"Naive Bayes"},"content":"From Lecture: \n\nBayes Classifier and Naive Bayesmore","type":"content","url":"/2021-09-21-naive-bayes","position":1},{"hierarchy":{"lvl1":"Naive Bayes","lvl2":"Generative vs. Discriminative Algorithm"},"type":"lvl2","url":"/2021-09-21-naive-bayes#generative-vs-discriminative-algorithm","position":2},{"hierarchy":{"lvl1":"Naive Bayes","lvl2":"Generative vs. Discriminative Algorithm"},"content":"When we estimate P(X,Y), we call it generative learning. We try to model the distribution behind the scene - the distribution we draw all our samples from. (Remember how we described this omnipotent distribution when talking about the \n\nBayes Optimal Classifier) We achieve this often by modelling P(X,Y)=P(X|Y)P(Y).\n\nWhen we only estimate P(Y|X) directly, then we call it discriminative learning. We try to model the probability of a label given features. This is more aligned with our common definition of “predicting” things.","type":"content","url":"/2021-09-21-naive-bayes#generative-vs-discriminative-algorithm","position":3},{"hierarchy":{"lvl1":"Naive Bayes","lvl2":"Introduction - Estimating Probability Directly From Data"},"type":"lvl2","url":"/2021-09-21-naive-bayes#introduction-estimating-probability-directly-from-data","position":4},{"hierarchy":{"lvl1":"Naive Bayes","lvl2":"Introduction - Estimating Probability Directly From Data"},"content":"","type":"content","url":"/2021-09-21-naive-bayes#introduction-estimating-probability-directly-from-data","position":5},{"hierarchy":{"lvl1":"Naive Bayes","lvl3":"Something Generative","lvl2":"Introduction - Estimating Probability Directly From Data"},"type":"lvl3","url":"/2021-09-21-naive-bayes#something-generative","position":6},{"hierarchy":{"lvl1":"Naive Bayes","lvl3":"Something Generative","lvl2":"Introduction - Estimating Probability Directly From Data"},"content":"Our training consists of the set  D=\\{(\\mathbf{x}_1,y_1),\\dots,(\\mathbf{x}_n,y_n)\\} drawn from some  unknown distribution P(X,Y). Because all pairs are sampled i.i.d., we obtainP(D)=P((\\mathbf{x}_1,y_1),\\dots,(\\mathbf{x}_n,y_n))=\\Pi_{\\alpha=1}^n    P(\\mathbf{x}_\\alpha,y_\\alpha).\n\nIf we do have enough data, we could estimate P(X,Y) simply through counting:\\hat P(\\mathbf{x},y)\n=\\frac{\\text{\\# (x,y) appeared in our data}}{\\text{\\# total data}}\n= \\frac{\\sum_{i=1}^{n} I(\\mathbf{x}_i =  \\mathbf{x} \\wedge {y}_i = y)}{n}\n\nwhere I(\\mathbf{x}_i=\\mathbf{x} \\wedge {y}_i=y)=1 if  \\mathbf{x}_i=\\mathbf{x} and {y}_i=y and 0 otherwise.","type":"content","url":"/2021-09-21-naive-bayes#something-generative","position":7},{"hierarchy":{"lvl1":"Naive Bayes","lvl3":"Something Discriminative","lvl2":"Introduction - Estimating Probability Directly From Data"},"type":"lvl3","url":"/2021-09-21-naive-bayes#something-discriminative","position":8},{"hierarchy":{"lvl1":"Naive Bayes","lvl3":"Something Discriminative","lvl2":"Introduction - Estimating Probability Directly From Data"},"content":"We are also interested in using similar technique to estimate P(Y|X). Because we know if we have P(Y|X), we can then use the Bayes Optimal Classifier to make predictions. We now want to make some prediction \\hat{P}(y|\\mathbf{x}) based on dataset, and pretend our estimate \\hat{P}(y|\\mathbf{x}) to well describe our real distribution P(Y|X), so we use \\hat{P}(y|\\mathbf{x}) in place of {P}(y|\\mathbf{x}) in BOC.\n\nSo how can we estimate \\hat{P}(y | \\mathbf{x})? Previously we have derived that \\hat P(y)=\\frac{\\sum_{i=1}^n  I(y_i=y)}{n}. Similarly, \\hat P(\\mathbf{x})=\\frac{\\sum_{i=1}^n  I(\\mathbf{x}_i=\\mathbf{x})}{n} and \\hat  P(y,\\mathbf{x})=\\frac{\\sum_{i=1}^{n} I(\\mathbf{x}_i = \\mathbf{x} \\wedge  {y}_i = y)}{n}. We can put these two together\\hat{P}(y|\\mathbf{x}) =  \\frac{\\hat{P}(y,\\mathbf{x})}{P(\\mathbf{x})} =  \\frac{\\sum_{i=1}^{n} I(\\mathbf{x}_i = \\mathbf{x} \\wedge {y}_i = y)}{  \\sum_{i=1}^{n} I(\\mathbf{x}_i = \\mathbf{x})}\n\nThe Venn diagram illustrates that the MLE method estimates \\hat{P}(y|\\mathbf{x}) as  \\hat{P}(y|\\mathbf{x}) = \\frac{|C|}{|B|}\n\nProblem: The estimate is only good if there are many training data with the exact same features as \\mathbf{x}! In high dimensional spaces (or with continuous \\mathbf{x}), this never happens! So |B| \\rightarrow 0 and |C| \\rightarrow 0.","type":"content","url":"/2021-09-21-naive-bayes#something-discriminative","position":9},{"hierarchy":{"lvl1":"Naive Bayes","lvl2":"Naive Bayes"},"type":"lvl2","url":"/2021-09-21-naive-bayes#naive-bayes","position":10},{"hierarchy":{"lvl1":"Naive Bayes","lvl2":"Naive Bayes"},"content":"Naive Bayes will give a solution. It is a kind of generative learning.\n\nSo we change gears a bit and turn to estimate P(\\mathbf{x} | y) \\text{ and } P(y) in the Bayes formula:P(y | \\mathbf{x}) = \\frac{P(\\mathbf{x} | y)P(y)}{P(\\mathbf{x})}.\n\nEstimating P(y) is easy (assume there are not many classes in this classification problem). We simply need to count how many times we observe each class. Define the fraction of time we see label c as \\hat\\pi_c.P(y = c)  = \\frac{\\sum_{i=1}^{n} I(y_i = c)}{n} = \\hat\\pi_c\n\nEstimating P(\\mathbf{x}|y), however, is not easy! Here we have to make a very bold additional assumption: the Naive Bayes assumption\n\nNaive Bayes Assumption: Each feature values are mutually independent given the label.\\begin{align}\nP(\\mathbf{x} | y) &= P(\\begin{bmatrix} x_1, x_2, \\cdots, x_d \\end{bmatrix} \\mid y) \\\\ \n&= \\prod_{\\alpha = 1}^{d} P([\\mathbf{x}]_\\alpha | y) &&\\text{each entry is mutually independent}\n\\end{align}\n\nNaive Bayes Assumption helps us decompose a single d-dimension problem into d 1-dimension problems.\n\n这是大胆的是因为假设我们用各种词出现的次数来判断一封邮件是否是垃圾邮件，那么明显各个词出现的概率是相关的，他们不可能 independent。\n\n Illustration behind the Naive Bayes algorithm: 假设 dim(x)=2, 竖轴是 [x]_1, 横轴是 [x]_2. 我们实际上有 [x]_1, [x]_2, y, P 四个量需要表示。y 用颜色来表示，所以应该是个三维图来着，那么高度轴就应该是 probability P，但是这里就画了二维，碍于图画我们只能想象一下了画的这些曲线其实都在高度轴上\n\nfig1: P(x|y)\n\nfig2: P([x]_1|y)\n\nfig3: P([x]_2|y)\n\nfig4: \\prod_\\alpha P(x_\\alpha|y) 这幅图所表示的实际上是三维空间向二维空间的一个投影\n\nWe then define our Bayes Classifier as:\\begin{align} h(\\mathbf{x}) \n&= \\operatorname*{argmax}_y P(y | \\mathbf{x}) \\\\ \n&= \\operatorname*{argmax}_y \\; \\frac{P(\\mathbf{x} |  y)P(y)}{P(\\mathbf{x})} \\\\ \n&= \\operatorname*{argmax}_y \\; P(\\mathbf{x} | y) P(y) &&  \\text{($P(\\mathbf{x})$ does not depend on $y$)} \\\\ &= \\operatorname*{argmax}_y \\; P(y) \\prod_{\\alpha=1}^{d} P(x_\\alpha | y)   && \\text{(by the naive Bayes assumption)}\\\\ \n&= \\operatorname*{argmax}_y \\; \\hat\\pi_y \\prod_{\\alpha=1}^{d} P(x_\\alpha | y) && \\text{previous definition of $\\hat\\pi_y$} \\\\\n&= \\operatorname*{argmax}_y \\; \\log(\\hat\\pi_y) + \\sum_{\\alpha = 1}^{d} \\log(P(x_\\alpha | y))  && \\text{(as log is a monotonic function)} \\end{align}\n\nEstimating \\log(P(x_\\alpha | y)) is easy as we only need to consider one dimension each time.\n\nTake a log or not, this formula defines Naive Bayes.\\boxed{h(\\mathbf{x}) = \\operatorname*{argmax}_y \\; \\hat\\pi_y \\prod_{\\alpha=1}^{d} P(x_\\alpha | y)}","type":"content","url":"/2021-09-21-naive-bayes#naive-bayes","position":11},{"hierarchy":{"lvl1":"Naive Bayes","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"type":"lvl2","url":"/2021-09-21-naive-bayes#estimating-p-mathbf-x-alpha-y-3-notable-cases","position":12},{"hierarchy":{"lvl1":"Naive Bayes","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"content":"We will talk about 3 cases where we use Naive Bayes to make predictions. In each case, we make explicit assumptions (refer back to \n\nour first chapter to learn about what “assumption” means) about distribution of our samples.","type":"content","url":"/2021-09-21-naive-bayes#estimating-p-mathbf-x-alpha-y-3-notable-cases","position":13},{"hierarchy":{"lvl1":"Naive Bayes","lvl3":"Case #1: Categorical features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"type":"lvl3","url":"/2021-09-21-naive-bayes#case-1-categorical-features","position":14},{"hierarchy":{"lvl1":"Naive Bayes","lvl3":"Case #1: Categorical features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"content":"For d dimensional data, think of it as having d independent dices. Each dice corresponds to a feature in our data point. Each dice has some different values. We roll each dice exactly once, record the result in a corresponding entry in our feature vector.","type":"content","url":"/2021-09-21-naive-bayes#case-1-categorical-features","position":15},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Features","lvl3":"Case #1: Categorical features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"type":"lvl4","url":"/2021-09-21-naive-bayes#features","position":16},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Features","lvl3":"Case #1: Categorical features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"content":"[\\mathbf{x}]_\\alpha \\in \\{f_1, f_2, \\cdots, f_{K_\\alpha}\\}\n\nEach feature \\alpha falls into one of K_\\alpha categories. (So if we have binary features at entry \\alpha, we would have K_\\alpha = 2.)","type":"content","url":"/2021-09-21-naive-bayes#features","position":17},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Model P(x_\\alpha \\mid y):","lvl3":"Case #1: Categorical features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"type":"lvl4","url":"/2021-09-21-naive-bayes#model-p-x-alpha-mid-y","position":18},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Model P(x_\\alpha \\mid y):","lvl3":"Case #1: Categorical features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"content":"对于 x 的每一维度 \\alpha, 我们都有一个 K_\\alpha \\times |C| 的矩阵来表示给定 label y=c, x_{\\alpha} 取值 j  的概率。P(x_{\\alpha} = j | y=c) = [\\theta_{jc}]_{\\alpha} \\\\\n\\text{ and } \\sum_{j=1}^{K_\\alpha} [\\theta_{jc}]_{\\alpha} = 1\n\nwhere [\\theta_{jc}]_{\\alpha}  is the probability of feature \\alpha having the value j, given that the label is c. And the constraint indicates that x_{\\alpha} must have one of the categories {1, \\dots, K_\\alpha}.","type":"content","url":"/2021-09-21-naive-bayes#model-p-x-alpha-mid-y","position":19},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Parameter estimation:","lvl3":"Case #1: Categorical features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"type":"lvl4","url":"/2021-09-21-naive-bayes#parameter-estimation","position":20},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Parameter estimation:","lvl3":"Case #1: Categorical features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"content":"\\begin{align} [\\hat\\theta_{jc}]_{\\alpha} \n&= \\frac{\\sum_{i=1}^{n} I(y_i = c) I([\\mathbf{x}_i]_\\alpha = j) + l}\n\t\t{\\sum_{i=1}^{n} I(y_i = c) + lK_\\alpha} \\end{align}\n\nwhere l is a smoothing parameter. By setting l=0 we get an MLE estimator, l>0  leads to MAP. If we set l= +1 we get Laplace smoothing. (Here we made an implicit assumption that each category has the same possibility of happening, so we will have lK_\\alpha imaginary draws and we get [\\mathbf{x}_i]_\\alpha = j happens l times)[\\hat\\theta_{jc}]_{\\alpha} =\n\\frac{\\text{\\# of samples with label c that have feature } \\alpha \\text{ with value $j$ }}\n\t {\\text{\\# of samples with label $c$}}","type":"content","url":"/2021-09-21-naive-bayes#parameter-estimation","position":21},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Prediction:","lvl3":"Case #1: Categorical features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"type":"lvl4","url":"/2021-09-21-naive-bayes#prediction","position":22},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Prediction:","lvl3":"Case #1: Categorical features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"content":"\\begin{align}\n& \\operatorname*{argmax}_y \\; P(y\\mid \\mathbf{x}) \\\\\n= \\; &\\operatorname*{argmax}_y \\; \\hat\\pi_y \\prod_{\\alpha=1}^{d} P(x_\\alpha | y) &&\\text{previous definition} \\\\\n\\propto \\; &\\operatorname*{argmax}_y \\; \\hat\\pi_c \\prod_{\\alpha = 1}^{d} [\\hat\\theta_{jc}]_\\alpha\n\\end{align}\n\nP(y) = \\hat\\pi_c was defined previously\n\n P(\\mathbf{x}) = \\prod_{\\alpha = 1}^{d} [\\hat\\theta_{jc}]_\\alpha is derived from the Naive Bayes Assumption\n\nP(\\mathbf{x}) is a constant\n\n这个预测共需要 dim \\times K_\\alpha \\times |C| 个值，对比如果我们直接采用 P(x,y) 进行预测，我们需要 K_\\alpha^{dim} \\times |C| 个值是一个巨大的改进。","type":"content","url":"/2021-09-21-naive-bayes#prediction","position":23},{"hierarchy":{"lvl1":"Naive Bayes","lvl3":"Case #2: Multinomial features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"type":"lvl3","url":"/2021-09-21-naive-bayes#case-2-multinomial-features","position":24},{"hierarchy":{"lvl1":"Naive Bayes","lvl3":"Case #2: Multinomial features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"content":"We have one die of d values. Roll it multiple times and jot down the number of times each value appeared. Record that number in the entry corresponding to that value. Therefore, the value of the i^{th}  feature shows how many times a particular value appeared.\n\n接着以根据词频判别垃圾邮件作为例子，在 multinomial features 中，[x]_\\alpha=j 就是某个词 \\alpha 出现了 j 次。","type":"content","url":"/2021-09-21-naive-bayes#case-2-multinomial-features","position":25},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Features:","lvl3":"Case #2: Multinomial features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"type":"lvl4","url":"/2021-09-21-naive-bayes#features-1","position":26},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Features:","lvl3":"Case #2: Multinomial features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"content":"\\begin{align} x_\\alpha \\in {0, 1, 2, \\dots, m} \\text{ and } m = \\sum_{\\alpha = 1}^d  x_\\alpha  \\end{align}\n\nm is the length of this letter and d is the size of the vocabulary","type":"content","url":"/2021-09-21-naive-bayes#features-1","position":27},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Model P(\\mathbf{x} \\mid y):","lvl3":"Case #2: Multinomial features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"type":"lvl4","url":"/2021-09-21-naive-bayes#model-p-mathbf-x-mid-y","position":28},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Model P(\\mathbf{x} \\mid y):","lvl3":"Case #2: Multinomial features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"content":"虽然本情境中每个词的出现是互相独立的，但是由于信件的长度已经确定，我们只能按照总体来估计，不能像原来一样估计一个P(x_\\alpha \\mid y). This situation can be modeled by this multinomial distributionP(\\mathbf{x} \\mid m, y=c) = \\frac{m!}{x_1! \\cdot x_2! \\cdot \\dots \\cdot  x_d!} \\prod_{\\alpha = 1}^d \\left(\\theta_{\\alpha c}\\right)^{x_\\alpha}\n\nwhere \\theta_{\\alpha c} is the probability of selecting word x_\\alpha and \\sum_{\\alpha = 1}^d \\theta_{\\alpha c} =1 (you have to select one of the d words). This equation describes the probability of having such an email (of some number of appearance times of each vocabulary) given it is a spam (or not).\n\nSo, we can use this to generate a spam email, i.e., a document \\mathbf{x} of class y = \\text{spam} by picking m words independently at random from the vocabulary of d words using  P(\\mathbf{x} \\mid y = \\text{spam}).","type":"content","url":"/2021-09-21-naive-bayes#model-p-mathbf-x-mid-y","position":29},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Parameter estimation:","lvl3":"Case #2: Multinomial features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"type":"lvl4","url":"/2021-09-21-naive-bayes#parameter-estimation-1","position":30},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Parameter estimation:","lvl3":"Case #2: Multinomial features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"content":"\\begin{align} \\hat\\theta_{\\alpha c} = \\frac{\\sum_{i = 1}^{n} I(y_i = c) x_{i\\alpha} +  l}{\\sum_{i=1}^{n} I(y_i = c) m_i + l \\cdot d } \\end{align}\n\nwhere m_i=\\sum_{\\beta = 1}^{d} x_{i\\beta} denotes the number of words  in document i and l is the smoothing parameter.\\hat\\theta_{\\alpha c} = \n\\frac{\\text{\\# of times word } \\alpha \\text{ appears in all spam emails}}\n{\\text{\\# of words in all spam emails combined}}.","type":"content","url":"/2021-09-21-naive-bayes#parameter-estimation-1","position":31},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Prediction:","lvl3":"Case #2: Multinomial features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"type":"lvl4","url":"/2021-09-21-naive-bayes#prediction-1","position":32},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Prediction:","lvl3":"Case #2: Multinomial features","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"content":"If you look back at our Bayesian Classifier definition: \\operatorname*{argmax}_y \\; \\frac{P(\\mathbf{x} |  y)P(y)}{P(\\mathbf{x})}\n\nThe factorial terms are present both at the factor P(\\mathbf{x} |  y) and the denominator P(\\mathbf{x}) so they cancel out. And all we need to care about is just the \\theta and \\pi\\begin{align}\n&\\operatorname*{argmax}_c \\; P(y = c \\mid \\mathbf{x}) \\\\\n= \\; &\\operatorname*{argmax}_y \\; \\hat\\pi_y \\prod_{\\alpha=1}^{d} P(x_\\alpha | y) \\\\\n\\propto \\;  &\\operatorname*{argmax}_c \\; \\hat\\pi_c \\prod_{\\alpha = 1}^d \\hat\\theta_{\\alpha c}^{x_\\alpha}\n\\end{align}","type":"content","url":"/2021-09-21-naive-bayes#prediction-1","position":33},{"hierarchy":{"lvl1":"Naive Bayes","lvl3":"Case #3: Continuous Normal Features (Gaussian Naive Bayes)","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"type":"lvl3","url":"/2021-09-21-naive-bayes#case-3-continuous-normal-features-gaussian-naive-bayes","position":34},{"hierarchy":{"lvl1":"Naive Bayes","lvl3":"Case #3: Continuous Normal Features (Gaussian Naive Bayes)","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"content":"In this situation, we assume each class conditional feature distribution P(x_\\alpha|y) originates from a Gaussian distribution with its own mean \\mu_{\\alpha,y} and variance \\sigma_{\\alpha,y}^2. So P(x_\\alpha \\mid y=c) = \\mathcal{N}\\left(\\mu_{\\alpha c},  \\sigma^{2}_{\\alpha c}\\right). Since we have the Naive Bayes Assumption, each class conditional feature distribution is actually independent from each other.","type":"content","url":"/2021-09-21-naive-bayes#case-3-continuous-normal-features-gaussian-naive-bayes","position":35},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Features:","lvl3":"Case #3: Continuous Normal Features (Gaussian Naive Bayes)","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"type":"lvl4","url":"/2021-09-21-naive-bayes#features-2","position":36},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Features:","lvl3":"Case #3: Continuous Normal Features (Gaussian Naive Bayes)","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"content":"\\begin{align} x_\\alpha \\in \\mathbb{R} && \\text{(each feature takes on a real value)} \\end{align}","type":"content","url":"/2021-09-21-naive-bayes#features-2","position":37},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Model P(x_\\alpha \\mid y):","lvl3":"Case #3: Continuous Normal Features (Gaussian Naive Bayes)","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"type":"lvl4","url":"/2021-09-21-naive-bayes#model-p-x-alpha-mid-y-1","position":38},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Model P(x_\\alpha \\mid y):","lvl3":"Case #3: Continuous Normal Features (Gaussian Naive Bayes)","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"content":"Since it is a Gaussian distribution,\\begin{align} P(x_\\alpha \\mid y=c) = \\mathcal{N}\\left(\\mu_{\\alpha c},  \\sigma^{2}_{\\alpha c}\\right) = \\frac{1}{\\sqrt{2 \\pi} \\sigma_{\\alpha c}}  e^{-\\frac{1}{2} \\left(\\frac{x_\\alpha - \\mu_{\\alpha c}}{\\sigma_{\\alpha  c}}\\right)^2}  \\end{align}\n\nTo computer \\mu_{\\alpha c},  \\sigma^{2}_{\\alpha c}: The mean \\mu_{\\alpha,y} of dim \\; \\alpha given label c  is just the average feature value of dimension \\alpha from all samples with label y. The (squared) standard deviation is simply the variance of this estimate.\\begin{align} \\mu_{\\alpha c} &\\leftarrow \\frac{1}{n_c} \\sum_{i = 1}^{n} I(y_i = c) x_{i\\alpha} && \\text{where $n_c = \\sum_{i=1}^{n} I(y_i = c)$}  \\\\ \\sigma_{\\alpha c}^2 &\\leftarrow \\frac{1}{n_c} \\sum_{i=1}^{n} I(y_i = c)(x_{i\\alpha} - \\mu_{\\alpha c})^2 \\end{align}\n\nThe full distribution is P(\\mathbf{x}|y)\\sim  \\mathcal{N}(\\mathbf{\\mu}_y,\\Sigma_y), where \\Sigma_y is a diagonal covariance matrix with [\\Sigma_y]_{\\alpha,\\alpha}=\\sigma^2_{\\alpha,y}. Diagonal because we have our Naive Bayes Assumption of each dimension being independent from each other.","type":"content","url":"/2021-09-21-naive-bayes#model-p-x-alpha-mid-y-1","position":39},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Parameter estimation:","lvl3":"Case #3: Continuous Normal Features (Gaussian Naive Bayes)","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"type":"lvl4","url":"/2021-09-21-naive-bayes#parameter-estimation-2","position":40},{"hierarchy":{"lvl1":"Naive Bayes","lvl4":"Parameter estimation:","lvl3":"Case #3: Continuous Normal Features (Gaussian Naive Bayes)","lvl2":"Estimating P([\\mathbf{x}]_\\alpha | y) - 3 Notable Cases"},"content":"According to\\operatorname*{argmax}_c \\; P(y = c \\mid \\mathbf{x}) \n= \\; \\operatorname*{argmax}_y \\; \\hat\\pi_y \\prod_{\\alpha=1}^{d} P(x_\\alpha | y)\n\nWe only have to multiply each P(x_\\alpha | y) together.","type":"content","url":"/2021-09-21-naive-bayes#parameter-estimation-2","position":41},{"hierarchy":{"lvl1":"Naive Bayes","lvl2":"Naive Bayes is a linear classifier"},"type":"lvl2","url":"/2021-09-21-naive-bayes#naive-bayes-is-a-linear-classifier","position":42},{"hierarchy":{"lvl1":"Naive Bayes","lvl2":"Naive Bayes is a linear classifier"},"content":"A linear classifier has the form \\hat y = \\mathbf{w}^\\top \\mathbf{x} + b\n\nNaive Bayes leads to a linear decision boundary in many common cases (including multinomial and continuous normal).","type":"content","url":"/2021-09-21-naive-bayes#naive-bayes-is-a-linear-classifier","position":43},{"hierarchy":{"lvl1":"Naive Bayes","lvl3":"Multinomial Case","lvl2":"Naive Bayes is a linear classifier"},"type":"lvl3","url":"/2021-09-21-naive-bayes#multinomial-case","position":44},{"hierarchy":{"lvl1":"Naive Bayes","lvl3":"Multinomial Case","lvl2":"Naive Bayes is a linear classifier"},"content":"Suppose that y_i \\in \\{-1, +1\\} and features are multinomial\n\nWe can show that \\exists \\mathbf{w},  b such thath(\\mathbf{x}) = \\operatorname*{argmax}_y \\; P(y) \\prod_{\\alpha - 1}^d  P(x_\\alpha \\mid y) = \\textrm{sign}(\\mathbf{w}^\\top \\mathbf{x} + b)\n\nThat is, our naive bayes classifier always gives the same classification as our linear classifier.h(\\mathbf{x}) = +1  \\Longleftrightarrow \\mathbf{w}^\\top \\mathbf{x} + b > 0\n\nAs we showed before, P(x_\\alpha|y=+1)\\propto\\theta_{\\alpha+}^{x_\\alpha} and P(Y=+1)=\\pi_+. We continue to define\\begin{align} [\\mathbf{w}]_\\alpha &= \\log(\\theta_{\\alpha +}) - \\log(\\theta_{\\alpha -}) \\\\ b &= \\log(\\pi_+) - \\log(\\pi_-) \\end{align}\n\nLet’s start a linear classification with this \\textbf{w} and b, we will arrive at our Naive Bayes classifier h(x) as go through the steps below.\n\n$$\\begin{align}\n\\mathbf{w}^\\top \\mathbf{x} + b > 0 &\\Longleftrightarrow \\sum_{\\alpha = 1}^{d} [\\mathbf{x}]_\\alpha \n\\overbrace{(\\log(\\theta_{\\alpha +}) - \\log(\\theta_{\\alpha -}))}^{[\\mathbf{w}]_\\alpha} + \\overbrace{\\log(\\pi_+) - \\log(\\pi_-)}^b > 0  && \\text{(Plugging in definition of $\\mathbf{w},b$.)}\\\\\n\n&\\Longleftrightarrow \\exp\\left(\\sum_{\\alpha = 1}^{d} [\\mathbf{x}]_\\alpha \n{(\\log(\\theta_{\\alpha +}) - \\log(\\theta_{\\alpha -}))} + {\\log(\\pi_+) - \\log(\\pi_-)} \\right)> 1  && \\text{(exponentiating both sides)}\\\\\n\n&\\Longleftrightarrow \\prod_{\\alpha = 1}^{d} \n\\frac{\\exp\\left( \\log\\theta_{\\alpha +}^{[\\mathbf{x}]_\\alpha} + \\log(\\pi_+)\\right)} \n{\\exp\\left(\\log\\theta_{\\alpha -}^{[\\mathbf{x}]_\\alpha} + \\log(\\pi_-)\\right)} \n> 1   && \\text{Because $a\\log(b)=\\log(b^a)$ and $\\exp{(a-b)}=\\frac{e^a}{e^b}$ operations}\\\\\n\n&\\Longleftrightarrow \\prod_{\\alpha = 1}^{d} \n\\frac{\\theta_{\\alpha +}^{[\\mathbf{x}]_\\alpha} \\pi_+}\n{\\theta_{\\alpha -}^{[\\mathbf{x}]_\\alpha} \\pi_-} > 1   && \\text{Because $\\exp(\\log(a))=a$ and $e^{a+b}=e^ae^b$}\\\\\n\n\n&\\Longleftrightarrow \\frac{\\prod_{\\alpha = 1}^{d} P([\\mathbf{x}]_\\alpha | Y = +1)\\pi_+}{\\prod_{\\alpha =1}^{d}P([\\mathbf{x}]_\\alpha | Y = -1)\\pi_-} > 1 && \\text{Because $P([\\mathbf{x}]_\\alpha | Y = -1)=\\theta^{\\mathbf{x}]_\\alpha}_{\\alpha-}$}\\\\\n\n&\\Longleftrightarrow \\frac{P(\\mathbf{x} | Y = +1)\\pi_+}{P(\\mathbf{x} | Y = -1)\\pi_-} > 1 && \\text{By the naive Bayes assumption. }\\\\\n\n&\\Longleftrightarrow \\frac{P(Y = +1 |\\mathbf{x})}{P( Y = -1|\\mathbf{x})}>1 && \\text{By Bayes rule (the denominator $P(\\mathbf{x})$ cancels out, and $\\pi_+=P(Y=+1)$.)} \\\\\n\n\n&\\Longleftrightarrow P(Y = +1 | \\mathbf{x}) > P(Y = -1 | \\mathbf{x})  \\\\\n&\\Longleftrightarrow \\operatorname*{argmax}_y  P(Y=y|\\mathbf{x})=+1 && \\text{i.e. the point $\\mathbf{x}$ lies on the positive side of the hyperplane iff Naive Bayes predicts +1}\n\\end{align}\n\n$$","type":"content","url":"/2021-09-21-naive-bayes#multinomial-case","position":45},{"hierarchy":{"lvl1":"Naive Bayes","lvl3":"Continuous Features Case","lvl2":"Naive Bayes is a linear classifier"},"type":"lvl3","url":"/2021-09-21-naive-bayes#continuous-features-case","position":46},{"hierarchy":{"lvl1":"Naive Bayes","lvl3":"Continuous Features Case","lvl2":"Naive Bayes is a linear classifier"},"content":"we can show that P(y \\mid \\mathbf{x}) = \\frac{1}{1 + e^{-y (\\mathbf{w}^\\top \\mathbf{x}  +b) }}\n\n. This model is also known as logistic regression. Naive Bayes and Logistic Regression produce asymptotically the same model if the Naive Bayes assumption holds.","type":"content","url":"/2021-09-21-naive-bayes#continuous-features-case","position":47},{"hierarchy":{"lvl1":"Logistic Regression"},"type":"lvl1","url":"/2021-09-28-logistic-regression","position":0},{"hierarchy":{"lvl1":"Logistic Regression"},"content":"","type":"content","url":"/2021-09-28-logistic-regression","position":1},{"hierarchy":{"lvl1":"Logistic Regression","lvl2":"Background Setup"},"type":"lvl2","url":"/2021-09-28-logistic-regression#background-setup","position":2},{"hierarchy":{"lvl1":"Logistic Regression","lvl2":"Background Setup"},"content":"When we do Logistic Regression, y takes two values -1, +1, so y\\in\\{+1,-1\\}. We also make the assumption that P(y|\\mathbf{x}_i) takes on exactly this form: (if you such an assumption is just ridiculous, refer back to the \n\nfirst lecture where we talked about assumptions. It is indeed bold an possibly ridculous, but the model works perfectly if the distribution looks something likes this)P(y|\\mathbf{x}_i)=\\frac{1}{1+e^{-y(\\mathbf{w}^T \\mathbf{x}_i+b)}}\n\nwhere \\mathbf{w} and b are parameters of this classifier. We can perform a little sanity check and find that P(+1|\\mathbf{x}_i) + P(-1|\\mathbf{x}_i) = 1.\n\nThe prediction result of Logistic Regression will beh(\\mathbf{x}) = P(+1 \\mid \\mathbf{x}) = \\frac{1}{1+e^{-(\\mathbf{w}^T \\mathbf{x}_i+b)}}","type":"content","url":"/2021-09-28-logistic-regression#background-setup","position":3},{"hierarchy":{"lvl1":"Logistic Regression","lvl2":"Logistic Regression and Perceptron"},"type":"lvl2","url":"/2021-09-28-logistic-regression#logistic-regression-and-perceptron","position":4},{"hierarchy":{"lvl1":"Logistic Regression","lvl2":"Logistic Regression and Perceptron"},"content":"","type":"content","url":"/2021-09-28-logistic-regression#logistic-regression-and-perceptron","position":5},{"hierarchy":{"lvl1":"Logistic Regression","lvl3":"LR’s Decision Boundary is Linear","lvl2":"Logistic Regression and Perceptron"},"type":"lvl3","url":"/2021-09-28-logistic-regression#lrs-decision-boundary-is-linear","position":6},{"hierarchy":{"lvl1":"Logistic Regression","lvl3":"LR’s Decision Boundary is Linear","lvl2":"Logistic Regression and Perceptron"},"content":"For a test point \\mathbf{x}, we assign label y=1 to it if\\begin{align}\n  & P(y=+1\\mid \\mathbf{x}) \\gt P(y=-1\\mid \\mathbf{x}) \\\\\n\\iff & 1+e^{(\\mathbf{w}^T \\mathbf{x}_i+b)}\\gt  {1+e^{-(\\mathbf{w}^T \\mathbf{x}_i+b)}} \\\\\n\\iff & e^{(\\mathbf{w}^T \\mathbf{x}_i+b)} \\gt e^{-(\\mathbf{w}^T \\mathbf{x}_i+b)}\\\\\n\\iff & \\mathbf{w}^T \\mathbf{x}_i+b \\gt 0\n\\end{align}\n\nAs we see from above, it is at last a linear equation that determines our label assignment.","type":"content","url":"/2021-09-28-logistic-regression#lrs-decision-boundary-is-linear","position":7},{"hierarchy":{"lvl1":"Logistic Regression","lvl3":"Difference from Perceptron","lvl2":"Logistic Regression and Perceptron"},"type":"lvl3","url":"/2021-09-28-logistic-regression#difference-from-perceptron","position":8},{"hierarchy":{"lvl1":"Logistic Regression","lvl3":"Difference from Perceptron","lvl2":"Logistic Regression and Perceptron"},"content":"Even though we have a linear decision boundary, this doesn’t mean we will make the same decision boundary as another model (such as Perceptron) would give. In fact, LR is in many ways much better than the Perceptron even though both of them draw a hyperplane to classify data points into two classes:\n\nPerceptron\n\nLinear Regression\n\nonly care about the point is on which side of the hyperplane, but do not care about its distance from the plane. A point only has two possibility: being classified on the head side, or on the tail side. It is too binary and does not take into account variability of distance to hyperplane.\n\ncare about point’s distance to the hyperplane. The predicted value actually represents “how confident we are with our estimation”. Therefore, when w^Tx+b=0, the point x is on the plane, which is equivalent to having 0.5 probability on head side and 0.5 probability on the tail side.","type":"content","url":"/2021-09-28-logistic-regression#difference-from-perceptron","position":9},{"hierarchy":{"lvl1":"Logistic Regression","lvl2":"LR and Gaussian Naive Bayes"},"type":"lvl2","url":"/2021-09-28-logistic-regression#lr-and-gaussian-naive-bayes","position":10},{"hierarchy":{"lvl1":"Logistic Regression","lvl2":"LR and Gaussian Naive Bayes"},"content":"Logistic Regression is implied by a special form of Gaussian Naive Bayes. Recall for a Gaussian Naive Bayes, we have the distribution of each feature given a label c as P(x_\\alpha \\mid y=c) = \\mathcal{N}\\left(\\mu_{\\alpha c},  \\sigma^{2}_{\\alpha c}\\right). Here, let’s assume that all features have the same deviation from the mean regardless of the label given, so we actually haveP(x_\\alpha \\mid y=c) = \\mathcal{N}\\left(\\mu_{\\alpha c},  \\sigma^{2}_{\\alpha}\\right)\n\nIn this case, we can actually prove that our Gaussian Naive Bayes gives the same result as a Logistic Regression with\\mathbf{w}_i = \\frac{\\mu_{i,-1}-\\mu_{i,+1}}{\\sigma^2_i}, b = \\ln \\frac{1-\\pi}{\\pi}+\\sum_i \\frac{\\mu^2_{i,+1}-\\mu^2_{i,-1}}{2\\sigma^2_i}\n\nA detailed proof can be found \n\nhere in section 3.1.\n\nTherefore, we have shown that Logistic Regression, a discriminative model, can actually be inferred by a special case of Gaussian Naive Bayes, a generative model. We usually call Logistic Regression the discriminative counterpart of Naive Bayes.","type":"content","url":"/2021-09-28-logistic-regression#lr-and-gaussian-naive-bayes","position":11},{"hierarchy":{"lvl1":"Logistic Regression","lvl2":"Estimating \\mathbf{w} in LR"},"type":"lvl2","url":"/2021-09-28-logistic-regression#estimating-mathbf-w-in-lr","position":12},{"hierarchy":{"lvl1":"Logistic Regression","lvl2":"Estimating \\mathbf{w} in LR"},"content":"Throughout this section we absorbed the parameter b into \\mathbf{w} through an additional constant dimension (similar to the \n\nPerceptron).\n\nIn addition, we only give out the formula for finding \\hat{\\mathbf{w}}. We will talk about methods of actually calculating the value (minimum) in the next lecture.","type":"content","url":"/2021-09-28-logistic-regression#estimating-mathbf-w-in-lr","position":13},{"hierarchy":{"lvl1":"Logistic Regression","lvl3":"Maximum Likelihood Estimate (MLE)","lvl2":"Estimating \\mathbf{w} in LR"},"type":"lvl3","url":"/2021-09-28-logistic-regression#maximum-likelihood-estimate-mle","position":14},{"hierarchy":{"lvl1":"Logistic Regression","lvl3":"Maximum Likelihood Estimate (MLE)","lvl2":"Estimating \\mathbf{w} in LR"},"content":"We want to find the parameter \\mathbf w that maximizes\\begin{aligned} P(\\mathbf y \\mid X; \\mathbf{w}) = \\prod_{i=1}^n P(y_i \\mid \\mathbf{x}_i; \\mathbf{w}) \\end{aligned}\n\nwhere X is all the training feature vectors X=\\left[\\mathbf{x}_1, \\dots,\\mathbf{x}_i, \\dots,  \\mathbf{x}_n\\right] \\in \\mathbb R^{d \\times n} and \\mathbf y is the labels of all data. We can turn it into a big product because of course all samples are drawn i.i.d. From the previous equation,\\begin{aligned}\n\\hat{\\mathbf{w}}_{MLE} \n&= \\operatorname*{argmax}_{\\mathbf{w}} \\; \\log \\bigg(\\prod_{i=1}^n P(y_i|\\mathbf{x}_i,\\mathbf{w})\\bigg)\\\\\n&= \\operatorname*{argmax}_{\\mathbf{w}} -\\sum_{i=1}^n \\log(1+e^{-y_i \\mathbf{w}^T \\mathbf{x}_i})\\\\\n&=\\operatorname*{argmin}_{\\mathbf{w}}\\sum_{i=1}^n \\log(1+e^{-y_i \\mathbf{w}^T \\mathbf{x}_i})\n\\end{aligned}\n\nNote that unlike in other algorithms, we do not set a constraint on \\mathbf w here. That is because the || \\mathbf w ||, size of \\mathbf w mattters here. It isn’t something simply defines a hyperplane, but this value effect how quickly/steep Logistic Regression changes from 0 to 1. When || \\mathbf w || is big, LR changes faster and the graph appears more steep.","type":"content","url":"/2021-09-28-logistic-regression#maximum-likelihood-estimate-mle","position":15},{"hierarchy":{"lvl1":"Logistic Regression","lvl3":"Maximum a Posteriori Estimate (MAP)","lvl2":"Estimating \\mathbf{w} in LR"},"type":"lvl3","url":"/2021-09-28-logistic-regression#maximum-a-posteriori-estimate-map","position":16},{"hierarchy":{"lvl1":"Logistic Regression","lvl3":"Maximum a Posteriori Estimate (MAP)","lvl2":"Estimating \\mathbf{w} in LR"},"content":"In the MAP estimate we treat \\mathbf{w} as a random variable and can specify a prior belief distribution over it. We may use the Gaussian approximation \\mathbf{w} \\sim \\mathbf{\\mathcal{N}}(\\mathbf 0,\\sigma^2 I), which says we do not have preferential direction of the plane \\mathbf w describes, but we do make an assumption about the scale of \\mathbf w.\n\nOur goal in MAP is to find the most likely model parameters given the data, i.e., the parameters that maximaize the posterior.\\begin{aligned}\n\\hat{\\mathbf{w}}_{MAP} \n= & \\; \\operatorname*{argmax}_{\\mathbf{w}} P(\\mathbf{w} \\mid X, \\mathbf y)\\\\\n= & \\; \\operatorname*{argmax}_{\\mathbf{w}} \\propto P(\\mathbf y \\mid X, \\mathbf{w}) \\; P(\\mathbf{w}) \\\\\n= & \\; \\operatorname*{argmax}_{\\mathbf{w}}\\log \\, \\left(P(\\mathbf y \\mid X, \\mathbf{w}) P(\\mathbf{w})\\right) \\\\ \n= & \\; \\operatorname*{argmin}_{\\mathbf{w}} \\sum_{i=1}^n \\log(1+e^{-y_i\\mathbf{w}^T \\mathbf{x}_i})+\\lambda\\mathbf{w}^\\top\\mathbf{w},\n\\end{aligned},\n\nwhere \\lambda = \\frac{1}{2\\sigma^2}. Note we are implicitly making \\mathbf w  small here by also trying to minimize \\mathbf{w}^\\top\\mathbf{w}.","type":"content","url":"/2021-09-28-logistic-regression#maximum-a-posteriori-estimate-map","position":17},{"hierarchy":{"lvl1":"Gradient Descent"},"type":"lvl1","url":"/2021-09-30-gradient-descent","position":0},{"hierarchy":{"lvl1":"Gradient Descent"},"content":"","type":"content","url":"/2021-09-30-gradient-descent","position":1},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"argmin from Last Time"},"type":"lvl2","url":"/2021-09-30-gradient-descent#argmin-from-last-time","position":2},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"argmin from Last Time"},"content":"In the previous lecture on \n\nLogistic Regression we wrote down expressions for the parameters in our model as solutions to optimization problems that do not have closed form solutions. Specifically, given data \\{(x_{i},y_{i})\\}_{i = 1}^{n} with x_{i} \\in \\mathbb{R}^{d} and y_{i} \\in \\{ + 1, - 1\\} we saw that{\\hat{w}}_{\\text{MLE}} = \\operatorname*{argmin}_{w \\in \\mathbb{R}^{d},b \\in \\mathbb{R}}\\;\\sum\\limits_{i = 1}^{n}\\log(1 + e^{- y_{i}(w^{T}x_{i} + b)})\n\nand{\\hat{w}}_{\\text{MAP}} = \\operatorname*{argmin}_{w \\in \\mathbb{R}^{d},b \\in \\mathbb{R}}\\;\\sum\\limits_{i = 1}^{n}\\log(1 + e^{- y_{i}(w^{T}x_{i} + b)}) + \\lambda w^{T}w\n\nThese notes will discuss general strategies to solve these problems and, therefore, we abstract our problem to \\min\\limits_{w}\\ell(w),\n\n where \\ell:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}. In other words, we would like to find the vector w that makes \\ell(w) as small as possible.","type":"content","url":"/2021-09-30-gradient-descent#argmin-from-last-time","position":3},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"Assumptions"},"type":"lvl2","url":"/2021-09-30-gradient-descent#assumptions","position":4},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"Assumptions"},"content":"Before diving into the mathematical and algorithmic details we will make some assumptions about our problem to simplify the discussion. Specifically, we will assume that:\n\n\\ell is convex, so local minimum found is a global minimum.\n\n\\ell is at least thrice continuously differentiable, so our Taylor approximations will work later.\n\nThere are no constraints placed on w. It is common to consider the problem \\min\\limits_{w \\in \\mathcal{C}}\\ell(w) where \\mathcal{C} represents some constraints on w (e.g., w is entrywise non-negative). Adding constraints is a level of complexity we will not address here.","type":"content","url":"/2021-09-30-gradient-descent#assumptions","position":5},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"Defining Minimizer"},"type":"lvl2","url":"/2021-09-30-gradient-descent#defining-minimizer","position":6},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"Defining Minimizer"},"content":"We call w^{\\ast} a local minimizer of \\ell if:\n\n\\exists \\epsilon, s.t. \\forall w \\text{ where } \\parallel w - w^{\\ast} \\parallel_{2} < \\epsilon we have \\ell(w^{\\ast}) \\leq \\ell(w).\n\nw^* is a global minimizer if\n\n\\forall w,  we have that \\ell(w^{\\ast}) \\leq \\ell(w).","type":"content","url":"/2021-09-30-gradient-descent#defining-minimizer","position":7},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"Taylor Expansion"},"type":"lvl2","url":"/2021-09-30-gradient-descent#taylor-expansion","position":8},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"Taylor Expansion"},"content":"Recall that the first order Taylor expansion of \\ell centered at w can be written as\\ell(w + p) \\approx \\ell(w) + g(w)^{T}p,\n\nwhere g(w) is the gradient of \\ell evaluated at w, so g(w) = \\nabla\\ell(w). This is the linear approximation of \\ell and has error \\mathcal{O}( \\parallel p \\parallel_{2}^{2}).\n\nSimilarly, the second order Taylor expansion of \\ell centered at w can be written as\\ell(w + p) \\approx \\ell(w) + g(w)^{T}p + \\frac{1}{2}p^{T}H(w)p\n\nwhere H(w) is the Hessian of \\ell evaluated at w. This is the quadratic approximation of \\ell and has error \\mathcal{O}( \\parallel p \\parallel_{2}^{3}).","type":"content","url":"/2021-09-30-gradient-descent#taylor-expansion","position":9},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"Search Direction Methods"},"type":"lvl2","url":"/2021-09-30-gradient-descent#search-direction-methods","position":10},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"Search Direction Methods"},"content":"We can’t really find the exact point w^* where a minimum is achieved. Therefore, the core idea to solve such problem is that given a starting point w^{0} we construct a sequence of iterates w^{1},w^{2},\\ldots with the goal that w^{k}\\rightarrow w^{\\ast} as k\\rightarrow\\infty. In a search direction method we will think of constructing w^{k + 1} from w^{k} by writing it as w^{k + 1} = w^{k} + s for some “step” s. Concretely, this means our methods will have the following generic format:\n\nInput: initial guess w^{0}\n\nk = 0;\n\nWhile not converged:\n\nPick a step s\n\nw^{k + 1} = w^{k} + s and k = k + 1\n\nCheck for convergence; if converged set \\hat{w} = w^{k}\n\nReturn: \\hat{w}\n\nThere are two clearly ambiguous steps in the above algorithm: how do we pick s and how do we determine when we have converged. We will talk about two main methods used in picking s and briefly touch on determining convergence.","type":"content","url":"/2021-09-30-gradient-descent#search-direction-methods","position":11},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"Gradient Descent"},"type":"lvl2","url":"/2021-09-30-gradient-descent#gradient-descent","position":12},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"Gradient Descent"},"content":"We use the first order derivative to approximate our function \\ell. At first attempt, we try to minimize this function. However, this is not attainable because linear function has no minimum - it just goes all the way to negative infinity. Therefore, we just take some step along this approximation to decrease the value by a bit.\n\nIf we directly look at the first order approximation, gradient descent can be also interpreted as follows: given we are currently at w^{k}, determine the direction in which the function decreases the fastest (along the gradient) at this point and update our w by taking a step in that direction. Consider the linear approximation to \\ell at w^{k} provided above by the Taylor series:\\ell(w^{k+1})  = \\ell(w^{k}+s) = \\ell(w^{k}) + (\\nabla\\ell(w^{k}))^{T}s\n\nthen the fastest direction to descend is simply s = - \\nabla\\ell(w^{k}), so we just go along the gradient. We want to get a smaller value so go along the negative direction. We go some \\alpha \\gt 0 distance, so set s ass = - \\alpha \\nabla\\ell(w^{k})","type":"content","url":"/2021-09-30-gradient-descent#gradient-descent","position":13},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"\\alpha","lvl2":"Gradient Descent"},"type":"lvl3","url":"/2021-09-30-gradient-descent#id-alpha","position":14},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"\\alpha","lvl2":"Gradient Descent"},"content":"\\alpha is often referred to as the step size in classical optimization, but learning rate in Machine Learning.\n\nline search - find the best \\alpha, but expensive so we don’t use it\n\nfix \\alpha - might not converge\n\ndecay \\alpha - set \\alpha = c/k at iteration k for any constant c > 0, so we take big step early on and take smaller steps as we think we are going to the minimizer. This guarantees convergence.","type":"content","url":"/2021-09-30-gradient-descent#id-alpha","position":15},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"Adagrad","lvl2":"Gradient Descent"},"type":"lvl3","url":"/2021-09-30-gradient-descent#adagrad","position":16},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"Adagrad","lvl2":"Gradient Descent"},"content":"Adagrad (technically, diagonal Adagrad) can be seen as an improvement of decay \\alpha, where we determine the step size on an entry basis  by the steepness of the function - big “decay” for steep direction, and small “decay” for flat direction.\n\nNote in gradient descent, we have the same learning rate across entries, which basically says “we learn the same amount for both rare and common features”. However, we want to have\n\nrare features - large learning rate. If set the learning rate too small, we won’t be able to change these rarely changed values at all; As we update the model, we always have a small gradient along the rare features, because these rarely occurred events usually have a subtle effect on the model (whether have a pet, whether they follow the traffic light, ...) If we draw a graph of our function, it is flat at this direction.\n\ncommon features - low learning rate. If set the learning rate too large, our model will simply jump back and forth because of these common features; As we update the model, we always have a large running gradient along the common features , because they usually have the greatest influence on model (Gender, Race, Age). If we draw a graph of our function, it is steep at this direction.\n\nTherefore, we decrease the step size by a lot along the direction where we already took a large step (that’s along the steep direction, common features) and decrease the step size only by a bit along the direction where we just took a small step (the flat direction, rare features.)\n\nAdagrad keeps track of history of steepness by keeping a running average of the squared gradient at each entry. It then sets a small learning rate for variables with large past gradients and a large learning rate for features with small past gradients.\n\nInput: \\ell, \\nabla\\ell, parameter \\epsilon > 0, and initial learning rate \\alpha.\n\nSet w_{j}^{0} = 0 and z_{j} = 0 for j = 1,\\ldots,d. k = 0;\n\nWhile not converged:\n\nCompute the gradient \\nabla w^k and record values for each entry of the gradient g_{j} = [\\nabla w^k]_j\n\nFor each dimension j, calculate the moving sum of squared gradient at dimension j: z_{j} = z_{j} + g_{j}^{2} for j = 1,\\ldots,d\n\nw_{j}^{k + 1} = w_{j}^{k} - \\alpha\\frac{g_{j}}{\\sqrt{z_{j} + \\epsilon}} for j = 1,\\ldots,d.\n\nk = k + 1\n\nCheck for convergence; if converged set \\hat{w} = w^{k}\n\nReturn: \\hat{w}","type":"content","url":"/2021-09-30-gradient-descent#adagrad","position":17},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"Convergence","lvl2":"Gradient Descent"},"type":"lvl3","url":"/2021-09-30-gradient-descent#convergence","position":18},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"Convergence","lvl2":"Gradient Descent"},"content":"Assuming the gradient is non-zero, we can show that there is always some small enough \\alpha such that \\ell(w^{k} - \\alpha \\nabla\\ell(w^{k})) < \\ell(w^{k}). In particular, we have\\ell(w^{k} - \\alpha \\nabla\\ell(w^{k})) = \\ell(w^{k}) - \\alpha (\\nabla\\ell(w^{k}))^{T}\\nabla\\ell(w^{k}) + \\mathcal{O}(\\alpha^{2})\n\nSince (\\nabla\\ell(w^{k}))^{T}\\nabla\\ell(w^{k}) > 0 and \\alpha^{2}\\rightarrow 0 faster than \\alpha as \\alpha\\rightarrow 0, we conclude that for some sufficiently small \\alpha > 0 we have that \\ell(w^{k} - \\alpha \\nabla\\ell(w^{k})) < \\ell(w^{k}).","type":"content","url":"/2021-09-30-gradient-descent#convergence","position":19},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"Newton’s Method"},"type":"lvl2","url":"/2021-09-30-gradient-descent#newtons-method","position":20},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"Newton’s Method"},"content":"Newton’s Method instead uses the second order derivative to approximate \\ell. We will pretend our approximation represents \\ell very well, finds the minimum point of our quadratic approximation, and claim it also to be the minimum of our original function \\ell. This works really well when our function indeed looks similar to a quadratic function locally, but when our approximation is off, the result will be way off and may never converge.\\ell(w^{k} + s) \\approx \\ell(w^{k}) + (\\nabla\\ell(w^{k}))^{T}s + \\frac{1}{2}s^{T}H(w^{k})s\n\nSpecifically, we now chose a step by explicitly minimizing the quadratic approximation to \\ell at w^{k}. We can find this minimum because \\ell is convex. To accomplish this, we solve\\min\\limits_{s}\\ell(w^{k}) + (\\nabla\\ell(w^{k}))^{T}s + \\frac{1}{2}s^{T}H(w^{k})s\n\nby differentiating and setting the gradient equal to zero. Since the gradient of our quadratic approximation is \\nabla\\ell(w^{k}) + H(w^{k})s this implies that s solves the linear systemH(w^{k})s = - \\nabla\\ell(w^{k})\n\nThis system is solvable because convex \\ell guarantees a positive semi-definite H(w) for all w. (In fact, we need to solve this function by having a positive definite H, which is guaranteed by a strictly convex \\ell, but we will introduce a workaround later)","type":"content","url":"/2021-09-30-gradient-descent#newtons-method","position":21},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"Potential Issues","lvl2":"Newton’s Method"},"type":"lvl3","url":"/2021-09-30-gradient-descent#potential-issues","position":22},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"Potential Issues","lvl2":"Newton’s Method"},"content":"","type":"content","url":"/2021-09-30-gradient-descent#potential-issues","position":23},{"hierarchy":{"lvl1":"Gradient Descent","lvl4":"Bad Approximation","lvl3":"Potential Issues","lvl2":"Newton’s Method"},"type":"lvl4","url":"/2021-09-30-gradient-descent#bad-approximation","position":24},{"hierarchy":{"lvl1":"Gradient Descent","lvl4":"Bad Approximation","lvl3":"Potential Issues","lvl2":"Newton’s Method"},"content":"Newton’s method has very good properties in the neighborhood of a strict local minimizer and once close enough to a solution it converges rapidly. However, if you are far from where the quadratic approximation is valid, Newton’s Method can diverge or enter a cycle. Practically, a fix for this is to introduce a step size \\alpha > 0 and formally sets = - \\alpha\\lbrack H(w^{k})\\rbrack^{- 1}\\nabla\\ell(w^{k})\n\nWe typically start with \\alpha = 1 since that is the proper step to take if the quadratic is a good approximation. However, if this step seems poor (e.g. \\ell(w^{k} + \\alpha s) > \\ell(w^{k})) then we can decrease \\alpha at that iteration.","type":"content","url":"/2021-09-30-gradient-descent#bad-approximation","position":25},{"hierarchy":{"lvl1":"Gradient Descent","lvl4":"Positive Semi-Definite H(w^k)","lvl3":"Potential Issues","lvl2":"Newton’s Method"},"type":"lvl4","url":"/2021-09-30-gradient-descent#positive-semi-definite-h-w-k","position":26},{"hierarchy":{"lvl1":"Gradient Descent","lvl4":"Positive Semi-Definite H(w^k)","lvl3":"Potential Issues","lvl2":"Newton’s Method"},"content":"When \\ell is convex but not strictly convex we can only assume that H(w^{k}) is positive semi-definite\n\nWhen the original function / our approximation is really flat (remember Hessian matrix represents curvature), so H is not invertible.\n\nIn principle this means that H(w^{k})s = - \\nabla\\ell(w^{k}) may not have a solution. Or, if H(w^{k}) has a zero eigenvalue then we always have infinite solutions. A “quick fix” is to simply solve(H(w^{k}) + \\epsilon I)s = - \\nabla\\ell(w^{k})\n\nfor some small parameter \\epsilon instead. This lightly regularizes the quadratic approximation to \\ell at w^{k} and ensures it has a strict global minimizer.","type":"content","url":"/2021-09-30-gradient-descent#positive-semi-definite-h-w-k","position":27},{"hierarchy":{"lvl1":"Gradient Descent","lvl4":"Slow Running Time","lvl3":"Potential Issues","lvl2":"Newton’s Method"},"type":"lvl4","url":"/2021-09-30-gradient-descent#slow-running-time","position":28},{"hierarchy":{"lvl1":"Gradient Descent","lvl4":"Slow Running Time","lvl3":"Potential Issues","lvl2":"Newton’s Method"},"content":"Newton’s method requires a second order approximation of the original function, which involved the Hessian matrix H. There are a lot of parameters to compute compared to a first order approximation.","type":"content","url":"/2021-09-30-gradient-descent#slow-running-time","position":29},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"Checking for convergence"},"type":"lvl2","url":"/2021-09-30-gradient-descent#checking-for-convergence","position":30},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"Checking for convergence"},"content":"Relative change in the iterates, i.e. \\frac{\\parallel w^{k + 1} - w^{k} \\parallel_{2}}{\\parallel w^{k} \\parallel_{2}} < \\delta_{1} for some small \\delta_{1} > 0.\n\nA reasonably small gradient, i.e., \\parallel \\nabla\\ell(w^{k}) \\parallel_{2} < \\delta_{2} for some small \\delta_{2} > 0.","type":"content","url":"/2021-09-30-gradient-descent#checking-for-convergence","position":31},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression"},"type":"lvl1","url":"/2021-10-05-important-distributions-and-linear-regr","position":0},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression"},"content":"","type":"content","url":"/2021-10-05-important-distributions-and-linear-regr","position":1},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl2":"Two Important Distributions in ML"},"type":"lvl2","url":"/2021-10-05-important-distributions-and-linear-regr#two-important-distributions-in-ml","position":2},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl2":"Two Important Distributions in ML"},"content":"","type":"content","url":"/2021-10-05-important-distributions-and-linear-regr#two-important-distributions-in-ml","position":3},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl3":"Gaussian Distribution","lvl2":"Two Important Distributions in ML"},"type":"lvl3","url":"/2021-10-05-important-distributions-and-linear-regr#gaussian-distribution","position":4},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl3":"Gaussian Distribution","lvl2":"Two Important Distributions in ML"},"content":"When you have a bunch of Random Variables adding together, it is likely to be Gaussian distributed, but we do want these Random Variables to have finite mean and finite variance. In this case, they will be Gaussian according to the Central Limit Theorem.","type":"content","url":"/2021-10-05-important-distributions-and-linear-regr#gaussian-distribution","position":5},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl3":"Power Law Distribution","lvl2":"Two Important Distributions in ML"},"type":"lvl3","url":"/2021-10-05-important-distributions-and-linear-regr#power-law-distribution","position":6},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl3":"Power Law Distribution","lvl2":"Two Important Distributions in ML"},"content":"P(X) = X^{-k}\n\nThe sum of a bunch finite mean but infinite variance Random Variables doesn’t usually converge, but when it does, it gives Power Law Distribution. Examples of power law: wealth per person, word frequency.\n\nPower Law is scale free: Select an arbitrary part of the graph and zoom into it, we will find it have the exact same shape as the original graph.","type":"content","url":"/2021-10-05-important-distributions-and-linear-regr#power-law-distribution","position":7},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl2":"Linear Regression"},"type":"lvl2","url":"/2021-10-05-important-distributions-and-linear-regr#linear-regression","position":8},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl2":"Linear Regression"},"content":"","type":"content","url":"/2021-10-05-important-distributions-and-linear-regr#linear-regression","position":9},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl3":"Assumptions","lvl2":"Linear Regression"},"type":"lvl3","url":"/2021-10-05-important-distributions-and-linear-regr#assumptions","position":10},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl3":"Assumptions","lvl2":"Linear Regression"},"content":"注意以下讨论中我们认为直线过原点，如果不过就将线加一维即可，详见前文 \n\nPerceptron\n\nWe assume that label is in the space of real numbers and is somewhat linearly distributed. Mathematically,y_{i} \\in \\mathbb{R}, \\; y_{i} = \\mathbf{w}^\\top\\mathbf{x}_i + \\epsilon_i\n\nwhere \\epsilon is a the noise: we said label is “somewhat” linear, so they can’t always be exactly on the line. The \\epsilon here is the offset between the label and the line.\n\nWe also assume that the noise is Gaussian distributed\\epsilon_i \\sim N(0, \\sigma^2)\n\nso for a fixed distance, each point has the same probability of being that distance away from the line. It is also reasonable to have mean as 0, because if it \\gt 0, it means majority points a more off to the above, so we can just move our line up a bit; similar for \\lt 0, we just move the line down a bit.\n根据我们这里的假设，可以通过将 \\epsilon_i 的 distribution 向我们的模型预测直线 \\mathbf{w}^\\top\\mathbf{x}_i 移动得到y_i|\\mathbf{x}_i \\sim N(\\mathbf{w}^\\top\\mathbf{x}_i,  \\sigma^2) \\Rightarrow  P(y_i|\\mathbf{x}_i,\\mathbf{w})=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(\\mathbf{x}_i^\\top\\mathbf{w}-y_i)^2}{2\\sigma^2}}\n\n有了 y 的分布，我们现在希望找出 \\mathbf w。","type":"content","url":"/2021-10-05-important-distributions-and-linear-regr#assumptions","position":11},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl3":"Estimating with MLE","lvl2":"Linear Regression"},"type":"lvl3","url":"/2021-10-05-important-distributions-and-linear-regr#estimating-with-mle","position":12},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl3":"Estimating with MLE","lvl2":"Linear Regression"},"content":"\\begin{aligned}\n\\mathbf{w} &= \\operatorname*{argmax}_{\\mathbf{\\mathbf{w}}} P(y_1,\\mathbf{x}_1,...,y_n,\\mathbf{x}_n|\\mathbf{w})\\\\\n&= \\operatorname*{argmax}_{\\mathbf{\\mathbf{w}}} \\prod_{i=1}^n P(y_i,\\mathbf{x}_i|\\mathbf{w}) & \\textrm{Because data points are independently sampled.}\\\\\n&= \\operatorname*{argmax}_{\\mathbf{\\mathbf{w}}} \\prod_{i=1}^n P(y_i|\\mathbf{x}_i,\\mathbf{w})P(\\mathbf{x}_i|\\mathbf{w}) & \\textrm{Chain rule of probability.}\\\\\n&= \\operatorname*{argmax}_{\\mathbf{\\mathbf{w}}} \\prod_{i=1}^n P(y_i|\\mathbf{x}_i,\\mathbf{w})P(\\mathbf{x}_i) & \\textrm{$\\mathbf{x}_i$ is independent of $\\mathbf{w}$, we only model $P(y_i|\\mathbf{x})$}\\\\\n&= \\operatorname*{argmax}_{\\mathbf{\\mathbf{w}}} \\prod_{i=1}^n P(y_i|\\mathbf{x}_i,\\mathbf{w}) & \\textrm{$P(\\mathbf{x}_i)$ is a constant - can be dropped}\\\\\n&= \\operatorname*{argmax}_{\\mathbf{\\mathbf{w}}} \\sum_{i=1}^n \\log\\left[P(y_i|\\mathbf{x}_i,\\mathbf{w})\\right] & \\textrm{log is a monotonic function}\\\\\n&= \\operatorname*{argmax}_{\\mathbf{\\mathbf{w}}} \\sum_{i=1}^n \\left[ \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\log\\left(e^{-\\frac{(\\mathbf{x}_i^\\top\\mathbf{w}-y_i)^2}{2\\sigma^2}}\\right)\\right] & \\textrm{Plugging in probability distribution}\\\\\n&= \\operatorname*{argmax}_{\\mathbf{\\mathbf{w}}} -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (\\mathbf{x}_i^\\top\\mathbf{w}-y_i)^2 & \\textrm{First term is a constant, and $\\log(e^z)=z$}\\\\\n&= \\operatorname*{argmin}_{\\mathbf{\\mathbf{w}}} \\frac{1}{n}\\sum_{i=1}^n (\\mathbf{x}_i^\\top\\mathbf{w}-y_i)^2 & \\textrm{$\\frac{1}{n}$ makes the loss interpretable (average squared error).}\\\\\n\\end{aligned}\n\nTherefore, maximizing parameter \\textbf{w} is equivalent to minimizing a loss function, l(\\mathbf{w}) =  \\frac{1}{n}\\sum_{i=1}^n (\\mathbf{x}_i^\\top\\mathbf{w}-y_i)^2. This  particular loss function is also known as the squared loss or Ordinary Least Squares (OLS). OLS has a closed form:\n\n\\mathbf{w} = (\\mathbf{X  X^\\top})^{-1}\\mathbf{X}\\mathbf{y}^\\top where  \\mathbf{X}=\\left[\\mathbf{x}_1,\\dots,\\mathbf{x}_n\\right] and  \\mathbf{y}=\\left[y_1,\\dots,y_n\\right].","type":"content","url":"/2021-10-05-important-distributions-and-linear-regr#estimating-with-mle","position":13},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl3":"Estimating with MAP","lvl2":"Linear Regression"},"type":"lvl3","url":"/2021-10-05-important-distributions-and-linear-regr#estimating-with-map","position":14},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl3":"Estimating with MAP","lvl2":"Linear Regression"},"content":"With MAP, we can have a prior belief on our \\textbf w, so we make the additional assumption:\n\nP(\\mathbf{w}) \\sim \\mathcal N(0, \\tau^2I) = \\frac{1}{\\sqrt{2\\pi\\tau^2}}e^{-\\frac{\\mathbf{w}^\\top\\mathbf{w}}{2\\tau^2}} This is to say: all features have a same deviation \\tau^2 and are drawn independently from each other, so each feature has the same probability of being big or small.\\begin{align}\n\\mathbf{w} &= \\operatorname*{argmax}_{\\mathbf{\\mathbf{w}}} P(\\mathbf{w}|y_1,\\mathbf{x}_1,...,y_n,\\mathbf{x}_n)\\\\\n&= \\operatorname*{argmax}_{\\mathbf{\\mathbf{w}}} \\frac{P(y_1,\\mathbf{x}_1,...,y_n,\\mathbf{x}_n|\\mathbf{w})P(\\mathbf{w})}{P(y_1,\\mathbf{x}_1,...,y_n,\\mathbf{x}_n)}\\\\\n&= \\operatorname*{argmax}_{\\mathbf{\\mathbf{w}}} \\left[\\prod_{i=1}^n P(y_i|\\mathbf{x}_i,\\mathbf{w})\\right]P(\\mathbf{w})\\\\\n&= \\operatorname*{argmax}_{\\mathbf{\\mathbf{w}}} \\sum_{i=1}^n  \\log P(y_i|\\mathbf{x}_i,\\mathbf{w})+ \\log P(\\mathbf{w})\\\\\n&= \\operatorname*{argmin}_{\\mathbf{\\mathbf{w}}} \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (\\mathbf{x}_i^\\top\\mathbf{w}-y_i)^2 + \\frac{1}{2\\tau^2}\\mathbf{w}^\\top\\mathbf{w} \n\t&&\\text{具体步骤与上文一样} \\\\\n&= \\operatorname*{argmin}_{\\mathbf{\\mathbf{w}}} \\frac{1}{n} \\sum_{i=1}^n (\\mathbf{x}_i^\\top\\mathbf{w}-y_i)^2 + \\lambda|| \\mathbf{w}||_2^2 \n\t&&\\text{给整个式子$\\times \\frac{2\\sigma^2}{n}$  并设 } \\lambda=\\frac{\\sigma^2}{n\\tau^2}\\\\\n\\end{align}\n\nThis objective is known as Ridge Regression. It has a closed form  solution of: \\mathbf{w} = (\\mathbf{X X^{\\top}}+\\lambda  \\mathbf{I})^{-1}\\mathbf{X}\\mathbf{y}^\\top, where  \\mathbf{X}=\\left[\\mathbf{x}_1,\\dots,\\mathbf{x}_n\\right] and  \\mathbf{y}=\\left[y_1,\\dots,y_n\\right].","type":"content","url":"/2021-10-05-important-distributions-and-linear-regr#estimating-with-map","position":15},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl3":"Summary","lvl2":"Linear Regression"},"type":"lvl3","url":"/2021-10-05-important-distributions-and-linear-regr#summary","position":16},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl3":"Summary","lvl2":"Linear Regression"},"content":"","type":"content","url":"/2021-10-05-important-distributions-and-linear-regr#summary","position":17},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl4":"Matrix Form Linear Regression","lvl3":"Summary","lvl2":"Linear Regression"},"type":"lvl4","url":"/2021-10-05-important-distributions-and-linear-regr#matrix-form-linear-regression","position":18},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl4":"Matrix Form Linear Regression","lvl3":"Summary","lvl2":"Linear Regression"},"content":"\\begin{align}\n\\ell(w) &= \\frac{1}{n} \\sum_{i=1}^n (x_i^T w - y_i)^2 = \\frac 1 n \\|Xw - Y\\|^2\\\\\n\\nabla \\ell(w) &= \\frac{2}{n} \\sum_{i=1}^n x_i (x_i^T w - y_i) = \\frac 2 n X^T (Xw - Y)\\\\\n\\nabla^2 \\ell(w) &= \\frac{2 }{n} \\sum_{i=1}^n x_i x_i^T = \\frac 2 n X^T X\n\\end{align}","type":"content","url":"/2021-10-05-important-distributions-and-linear-regr#matrix-form-linear-regression","position":19},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl4":"Ordinary Least Squares:","lvl3":"Summary","lvl2":"Linear Regression"},"type":"lvl4","url":"/2021-10-05-important-distributions-and-linear-regr#ordinary-least-squares","position":20},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl4":"Ordinary Least Squares:","lvl3":"Summary","lvl2":"Linear Regression"},"content":"\\operatorname*{min}_{\\mathbf{\\mathbf{w}}} \\frac{1}{n}\\sum_{i=1}^n (\\mathbf{x}_i^\\top\\mathbf{w}-y_i)^2.\n\nSquared loss.\n\nNo regularization.\n\nClosed form: \\mathbf{w} = (\\mathbf{X X^\\top})^{-1}\\mathbf{X} \\mathbf{y}^\\top.","type":"content","url":"/2021-10-05-important-distributions-and-linear-regr#ordinary-least-squares","position":21},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl4":"Ridge Regression:","lvl3":"Summary","lvl2":"Linear Regression"},"type":"lvl4","url":"/2021-10-05-important-distributions-and-linear-regr#ridge-regression","position":22},{"hierarchy":{"lvl1":"Important Distributions and Linear Regression","lvl4":"Ridge Regression:","lvl3":"Summary","lvl2":"Linear Regression"},"content":"\\operatorname*{min}_{\\mathbf{\\mathbf{w}}} \\frac{1}{n}\\sum_{i=1}^n  (\\mathbf{x}_i^\\top\\mathbf{w}-y_i)^2 + \\lambda ||\\mathbf{w}||_2^2.\n\nSquared loss.\n\nl2\\text{-regularization}.\n\nClosed form: \\mathbf{w} = (\\mathbf{X X^{\\top}}+\\lambda \\mathbf{I})^{-1}\\mathbf{X} \\mathbf{y}^\\top.","type":"content","url":"/2021-10-05-important-distributions-and-linear-regr#ridge-regression","position":23},{"hierarchy":{"lvl1":"SVM - Support Vector Machine"},"type":"lvl1","url":"/2021-10-07-svm-support-vector-machine","position":0},{"hierarchy":{"lvl1":"SVM - Support Vector Machine"},"content":"\n\nThe left picture is what we have achieved so far with a perceptron. The perceptron can give us either the blue line or a red line, so the result does not generalize well. We can view SVM (on the right) as an improvement of the Perceptron - it finds the one that maximizes the distance to the closest data points from both classes. We say it is the maximum margin separating hyperplane.","type":"content","url":"/2021-10-07-svm-support-vector-machine","position":1},{"hierarchy":{"lvl1":"SVM - Support Vector Machine","lvl2":"Defining Margin"},"type":"lvl2","url":"/2021-10-07-svm-support-vector-machine#defining-margin","position":2},{"hierarchy":{"lvl1":"SVM - Support Vector Machine","lvl2":"Defining Margin"},"content":"We know the distance from a point \\textbf x to a plane \\mathcal{H} = \\left\\{\\mathbf{x}\\vert{}\\mathbf{w}^T\\mathbf{x}+b=0\\right\\} is \\mathbf{d(x_i, \\mathcal H)} = \\frac{\\left | \\mathbf{w}^T\\mathbf{x}+b \\right |}{\\left \\| \\mathbf{w} \\right \\|_{2}}.\n\nUsing the definition of margin from \n\nPerceptron, we have margin of \\mathcal{H} with respect to out dataset D is \\gamma(\\mathbf{w},b)=\\min_{\\mathbf{x}\\in D}\\frac{\\left | \\mathbf{w}^T\\mathbf{x}+b \\right |}{\\left \\| \\mathbf{w} \\right \\|_{2}}\n\nBy definition, the margin and hyperplane are scale invariant:  \\gamma(\\beta\\mathbf{w},\\beta b)=\\gamma(\\mathbf{w},b), \\forall \\beta  \\neq 0","type":"content","url":"/2021-10-07-svm-support-vector-machine#defining-margin","position":3},{"hierarchy":{"lvl1":"SVM - Support Vector Machine","lvl2":"Max Margin Classifier"},"type":"lvl2","url":"/2021-10-07-svm-support-vector-machine#max-margin-classifier","position":4},{"hierarchy":{"lvl1":"SVM - Support Vector Machine","lvl2":"Max Margin Classifier"},"content":"We can formulate our search for the maximum margin separating hyperplane as a constrained optimization problem. The objective is to maximize the margin under the constraints that all data points must lie on the correct side of the hyperplane:\\underbrace{\\max_{\\mathbf{w},b}\\gamma(\\mathbf{w},b)}_{maximize \\ margin}  \\textrm{such that} \\ \\  \\underbrace{\\forall i \\  y_{i}(\\mathbf{w}^Tx_{i}+b)\\geq 0}_{separating \\ hyperplane}\n\nIf we plug in the definition of \\gamma we obtain:\\underbrace{\\max_{\\mathbf{w},b}\\underbrace{\\frac{1}{\\left \\|  \\mathbf{w} \\right \\|}_{2}\\min_{\\mathbf{x}_{i}\\in D}\\left |  \\mathbf{w}^T\\mathbf{x}_{i}+b \\right |}_{\\gamma(\\mathbf{w},b)} \\  }_{maximize \\ margin} \\ \\  s.t. \\ \\  \\underbrace{\\forall i \\  y_{i}(\\mathbf{w}^Tx_{i}+b)\\geq 0}_{separating \\ hyperplane}\n\nMaximizing minimum seems almost impossible to do, so we want to remove the min in the value we are trying to maximize. We can move this min to be a new constraint: Because the hyperplane is scale invariant, we can fix the scale of  \\mathbf{w},b anyway we want. We choose them such that\\min_{\\mathbf{x}\\in D}\\left | \\mathbf{w}^T\\mathbf{x}+b  \\right |=1\n\nso our objective becomes:\\max_{\\mathbf{w},b}\\frac{1}{\\left \\| \\mathbf{w} \\right \\|_{2}}\\min_{\\mathbf{x}\\in D}\\left | \\mathbf{w}^T\\mathbf{x}+b  \\right | = \\max_{\\mathbf{w},b}\\frac{1}{\\left \\| \\mathbf{w} \\right \\|_{2}}\\cdot 1 = \\min_{\\mathbf{w},b}\\left \\| \\mathbf{w} \\right \\|_{2} =  \\min_{\\mathbf{w},b} \\mathbf{w}^\\top \\mathbf{w}\n\nAdd our re-scaling as an equality constraint to the overall problem, our new optimization problem to solve is:\\begin{align}\n&\\min_{\\mathbf{w},b} \\mathbf{w}^\\top\\mathbf{w} & \\\\\n&\\textrm{s.t. } \n\\begin{matrix}\n\\forall i, \\ y_{i}(\\mathbf{w}^T \\mathbf{x}_{i}+b)&\\geq 0\\\\ \n\\min_{i}\\left | \\mathbf{w}^T \\mathbf{x}_{i}+b \\right | &= 1\n\\end{matrix}\n\\end{align}\n\nThese two new constraints are equivalent to \\forall i \\ y_{i}(\\mathbf{w}^T  \\mathbf{x}_{i}+b) \\geq 1 (Don’t ask me how to do a right to left proof, I do not know), so we can rewrite our optimization problem as:\\begin{align}\n&\\min_{\\mathbf{w},b}\\mathbf{w}^T\\mathbf{w}&\\\\ \n&\\textrm{s.t.} \\ \\ \\ \\forall i \\ y_{i}(\\mathbf{w}^T \\mathbf{x}_{i}+b) \\geq 1 &\n\\end{align}\n\nThis new formulation is simply a quadratic optimization problem. The objective is quadratic and the constraints are all linear.  We can be solve it efficiently with any QCQP (Quadratically Constrained Quadratic Program) solver. It has a unique solution  whenever a separating hyper plane exists. It also has a nice interpretation: Find the simplest hyperplane (where simpler means  smaller \\mathbf{w}^\\top\\mathbf{w}) such that all inputs lie at least 1 unit away from the hyperplane on the correct side.\n\nAn intuition behind this formula is: the new constraint can be written as |\\mathbf{w}^T \\mathbf{x}_{i}+b| \\geq 1 . Note this is | \\mathbf{w}|_{2} \\; \\mathbf{d(x_i, \\mathcal H)} \\geq 1, so \\sqrt{\\mathbf{w}^T\\mathbf{w}} \\; \\mathbf{d(x_i, \\mathcal H)} \\geq 1. Since we are minimizing \\sqrt{\\mathbf{w}^T\\mathbf{w}}, the only way we can achieve the greater than 1 constraint is to make \\mathbf d big. This is exactly what we want.\n\nWe can write the prediction result as h({\\mathbf{x}_i}) = w^{\\top}{\\mathbf{x}_i}+b, so h(\\mathbf{x}_i)y_{i} represents classification result correctness. If it is positive, the prediction is correct. And the bigger this value, the more correct we get. If it is negative, the prediction is wrong.","type":"content","url":"/2021-10-07-svm-support-vector-machine#max-margin-classifier","position":5},{"hierarchy":{"lvl1":"SVM - Support Vector Machine","lvl2":"Support Vectors"},"type":"lvl2","url":"/2021-10-07-svm-support-vector-machine#support-vectors","position":6},{"hierarchy":{"lvl1":"SVM - Support Vector Machine","lvl2":"Support Vectors"},"content":"For the optimal \\mathbf{w},b pair, some training points will have  tight constraints, i.e.y_{i}(\\mathbf{w}^T \\mathbf{x}_{i}+b) = 1\n\nWe refer to these training points as support vectors. These vectors define the maximum margin of the hyperplane to the data set and they therefore determine the shape of the hyperplane. If you were to move one of them and retrain the SVM, the resulting hyperplane would change. The opposite is the case for non-support vectors.","type":"content","url":"/2021-10-07-svm-support-vector-machine#support-vectors","position":7},{"hierarchy":{"lvl1":"SVM - Support Vector Machine","lvl2":"SVM with soft constraints"},"type":"lvl2","url":"/2021-10-07-svm-support-vector-machine#svm-with-soft-constraints","position":8},{"hierarchy":{"lvl1":"SVM - Support Vector Machine","lvl2":"SVM with soft constraints"},"content":"If the data is low dimensional it is often the case that there is no separating hyperplane between the two classes, so there is no solution to the optimization problems above. We can fix this by allowing the constraints to be violated (distance of x_i to the hyperplane to be greater than the margin) ever so slight with the introduction of slack variables \\xi. At the same time, we don’t want to loose the constraints too much, so we also minimize these slack variables along the way.\\begin{matrix}\n        \\min_{\\mathbf{w},b}\\mathbf{w}^T\\mathbf{w}+C\\sum_{i=1}^{n}\\xi _{i}         \\\\ \n        s.t. \\ \\forall i \\ y_{i}(\\mathbf{w}^T\\mathbf{x}_{i}+b)\\geq 1-\\xi_i         \\\\ \n         \\forall i \\ \\xi_i \\geq 0        \n\\end{matrix}\n\nAs the slack \\xi_i gets larger, we allow point x_i to be closer to the hyperplane \\ y_{i}(\\mathbf{w}^T\\mathbf{x}_{i}+b) \\lt 1, or even on the other side \\ y_{i}(\\mathbf{w}^T\\mathbf{x}_{i}+b) \\lt 0\n\nIf C is very large, the SVM becomes very strict and tries to get all points to be on the right side of the hyperplane. If C is very small,  the SVM becomes very loose and may “sacrifice” some points to obtain a  simpler (i.e. lower \\|\\mathbf{w}\\|_2^2) solution.","type":"content","url":"/2021-10-07-svm-support-vector-machine#svm-with-soft-constraints","position":9},{"hierarchy":{"lvl1":"SVM - Support Vector Machine","lvl3":"Unconstrained Formulation","lvl2":"SVM with soft constraints"},"type":"lvl3","url":"/2021-10-07-svm-support-vector-machine#unconstrained-formulation","position":10},{"hierarchy":{"lvl1":"SVM - Support Vector Machine","lvl3":"Unconstrained Formulation","lvl2":"SVM with soft constraints"},"content":"We will express our “SVM with soft constraints” in an unconstrained form.\n\nNote if y_{i}(\\mathbf{w}^T \\mathbf{x}_{i}+b)<1, so current point is too close (or on the other side) to the plane, \\xi_i is the distance away from the margin point. If point x_i is just as far as other good points, \\xi_i is simply 0. Formulate this in the equation below:\\xi_i=\\left\\{\n\\begin{array}{cc}\n\\ 1-y_{i}(\\mathbf{w}^T \\mathbf{x}_{i}+b) & \\textrm{ if $y_{i}(\\mathbf{w}^T \\mathbf{x}_{i}+b)<1$}\\\\\n0 & \\textrm{ if $y_{i}(\\mathbf{w}^T \\mathbf{x}_{i}+b)\\geq 1$}\n\\end{array}\n\\right.\n\nwhich is equivalent to the following closed form:\\xi_i=\\max(1-y_{i}(\\mathbf{w}^T \\mathbf{x}_{i}+b) ,0)\n\nThis closed form is called the hinge loss. If we plug this closed form into the objective of our SVM optimization problem, we obtain the following unconstrained version as loss function and regularizer:\\min_{\\mathbf{w},b}\\underbrace{\\mathbf{w}^T\\mathbf{w}}_{l_{2}-regularizer}+        C\\  \\sum_{i=1}^{n}\\underbrace{\\max\\left [ 1-y_{i}(\\mathbf{w}^T \\mathbf{x}+b),0 \\right ]}_{hinge-loss}\n\nWe optimize SVM paramters \\mathbf{w},b by minimizing this loss function just like we did in logistic regression (e.g. through gradient descent). The only difference is that we have the hinge loss here instead of the logistic loss.","type":"content","url":"/2021-10-07-svm-support-vector-machine#unconstrained-formulation","position":11},{"hierarchy":{"lvl1":"Empirical Risk Minimization"},"type":"lvl1","url":"/2021-10-14-empirical-risk-minimization","position":0},{"hierarchy":{"lvl1":"Empirical Risk Minimization"},"content":"","type":"content","url":"/2021-10-14-empirical-risk-minimization","position":1},{"hierarchy":{"lvl1":"Empirical Risk Minimization","lvl2":"Recap"},"type":"lvl2","url":"/2021-10-14-empirical-risk-minimization#recap","position":2},{"hierarchy":{"lvl1":"Empirical Risk Minimization","lvl2":"Recap"},"content":"Remember the unconstrained SVM Formulation\\min_{\\mathbf{w}}\\ C\\underset{Hinge-Loss}{\\underbrace{\\sum_{i=1}^{n}\\max[1-y_{i}\\underset{h({\\mathbf{x}_i})}{\\underbrace{(w^{\\top}{\\mathbf{x}_i}+b)}},0]}}+\\underset{l_{2}-Regularizer}{\\underbrace{\\left\\Vert w\\right\\Vert _{z}^{2}}}\n\nwhere h({\\mathbf{x}_i}) = w^{\\top}{\\mathbf{x}_i}+b is our prediction, the hinge loss is the SVM’s error function, and the \\left.l_{2}\\right.-regularizer reflects the complexity of the solution, and penalizes complex solutions (those with big w).\n\nWe can generalize problem of this form as empirical risk minimization with\n\nloss function  \\ell: continuous function which penalizes training error\n\nregularizer r: shape your model to the shape your prefer (usually a continuous function which penalizes classifier complexity)\\min_{\\mathbf{w}}\\frac{1}{n}\\sum_{i=1}^{n}\\underset{Loss}{\\underbrace{l(h_{\\mathbf{w}}({\\mathbf{x}_i}),y_{i})}}+\\underset{Regularizer}{\\underbrace{\\lambda r(w)}},\n\nIn SVM, \\ell = \\max[1-y_{i}h({\\mathbf{x}_i}), 0], r(w) = \\left\\Vert w\\right\\Vert _{z}^{2}, \\lambda = \\frac{1}{C}.","type":"content","url":"/2021-10-14-empirical-risk-minimization#recap","position":3},{"hierarchy":{"lvl1":"Empirical Risk Minimization","lvl2":"Binary Classification Loss Functions"},"type":"lvl2","url":"/2021-10-14-empirical-risk-minimization#binary-classification-loss-functions","position":4},{"hierarchy":{"lvl1":"Empirical Risk Minimization","lvl2":"Binary Classification Loss Functions"},"content":"Loss function in binary classification problem is always about h(\\mathbf{x}_i)y_{i} - classification result correctness.\n\nLoss \\ell\n\nUsage\n\nComments\n\nHinge-Loss \\max\\left[1-h_{\\mathbf{w}}(\\mathbf{x}_{i})y_{i},0\\right]^{p}\n\nStandard SVM(\\left.p=1\\right.)Differentiable/Squared Hinge Loss SVM (\\left.p=2\\right.)\n\nWhen used for Standard SVM, the loss function denotes the size of the margin between linear separator and its closest points in either class. Only differentiable everywhere with \\left.p=2\\right., but then it penalizes mistake much more aggressively.\n\nLog-Loss \\left.\\log(1+e^{-h_{\\mathbf{w}}(\\mathbf{x}_{i})y_{i}})\\right.\n\nLogistic Regression\n\nVery popular loss functions in ML, since its outputs are well-calibrated probabilities.\n\nExponential Loss \\left. e^{-h_{\\mathbf{w}}(\\mathbf{x}_{i})y_{i}}\\right.\n\nAdaBoost\n\nThis function is very aggressive. The loss of a mis-prediction increases exponentially with the value of -h_{\\mathbf{w}}(\\mathbf{x}_i)y_i. This can lead to nice convergence results,  for example in the case of Adaboost,  but it can also cause problems with noisy data or when you simply mistakenly mislabeled data.\n\nZero-One Loss\\left.\\delta(\\textrm{sign}(h_{\\mathbf{w}}(\\mathbf{x}_{i}))\\neq y_{i})\\right.\n\nActual Classification Loss\n\nNon-continuous and thus impractical to optimize.\n\n\n\nFigure 4.1: Plots of Common Classification Loss Functions:\n\nx-axis: \\left.h(\\mathbf{x}_{i})y_{i}\\right., or “correctness” of prediction\n\ny-axis: loss value\n\nA minor point: from the graph we know that Exponential Loss is a strict upperbound of 0/1 Loss. This will be useful later for proving its convergence.","type":"content","url":"/2021-10-14-empirical-risk-minimization#binary-classification-loss-functions","position":5},{"hierarchy":{"lvl1":"Empirical Risk Minimization","lvl2":"Regression Loss Functions"},"type":"lvl2","url":"/2021-10-14-empirical-risk-minimization#regression-loss-functions","position":6},{"hierarchy":{"lvl1":"Empirical Risk Minimization","lvl2":"Regression Loss Functions"},"content":"Loss function in regression is always about the offset between prediction and original value z = y - h(\\mathbf x).\n\nLoss \\ell\n\nComments\n\nSquared Loss \\left(h(\\mathbf{x}_{i})-y_{i}\\right)^{2}\n\nMost popular regression loss functionAlso known as Ordinary Least Squares (OLS)ADVANTAGE: Differentiable everywhereDISADVANTAGE: Somewhat sensitive to outliers/noiseEstimates Mean Label\n\nAbsolute Loss \\left|h(\\mathbf{x}_{i})-y_{i}\\right|\n\nAlso a very popular loss functionEstimates Median LabelADVANTAGE: Less sensitive to noiseDISADVANTAGE: Not differentiable at 0 (the point which minimization is intended to bring us to)\n\nHuber Loss\\begin{cases} \\frac{1}{2}\\left(h(\\mathbf{x}_{i})-y_{i}\\right)^{2} & \\text{if} \\left|h(\\mathbf{x}_{i})-y_{i}\\right|<\\delta\\\\ \\delta(\\left|h(\\mathbf{x}_{i})-y_{i}\\right|-\\frac{\\delta}{2})& \\text{otherwise} \\end{cases}\n\nAlso known as Smooth Absolute LossADVANTAGE: “Best of Both Worlds” of Squared and Absolute LossOnce-differentiableTakes on behavior of Squared-Loss when loss is small, and Absolute Loss when loss is large.\n\nLog-Cosh Loss \\left.log(cosh(h(\\mathbf{x}_{i})-y_{i}))\\right. ,\\left.cosh(x)=\\frac{e^{x}+e^{-x}}{2}\\right.\n\nADVANTAGE: Similar to Huber Loss, but twice differentiable everywhere\n\nIn the squared loss, the biggest loss shadows all the other losses - it wants to use whatever’s possible to decrease the biggest loss. We say the absolute loss is somewhat an “improvement” of the squared loss because it treats all losses more fairly. For example, in squared loss, 10 samples each diff by 1 is 10 loss, but 1 sample differs by 10 is 100 loss. On the other hand, 10 diff by 1 and 1 diff by 10 are both 10 loss in absolute value loss.\n\n\n\nFigure 4.2: Plots of Common Regression Loss Functions:\n\nx-axis: h(\\mathbf{x}_{i})-y_{i}, or “error” of prediction\n\ny-axis: loss value","type":"content","url":"/2021-10-14-empirical-risk-minimization#regression-loss-functions","position":7},{"hierarchy":{"lvl1":"Empirical Risk Minimization","lvl2":"Regularizers"},"type":"lvl2","url":"/2021-10-14-empirical-risk-minimization#regularizers","position":8},{"hierarchy":{"lvl1":"Empirical Risk Minimization","lvl2":"Regularizers"},"content":"Remember with Lagrange multipliers, which says for all B\\geq0, there exists \\lambda\\geq0 such that the two problems below are equivalent, and vice versa.\\min_{\\mathbf{w}} f(\\mathbf w) \\textrm { s.t. } g(\\mathbf{w})\\leq B  \\Leftrightarrow  \n\\min_{\\mathbf{w}} f(\\mathbf w) +\\lambda g(\\mathbf{w})\n\nWe can therefore change the formulation of the optimization problem with regularizers to obtain a better geometric intuition:\\min_{\\mathbf{w},b} \\sum_{i=1}^n\\ell(h_\\mathbf{w}(\\mathbf{x}),y_i)+\\lambda r(\\mathbf{w})  \n\\Leftrightarrow  \\min_{\\mathbf{w},b} \\sum_{i=1}^n\\ell(h_\\mathbf{w}(\\mathbf{x}),y_i) \\textrm { subject to: } r(\\mathbf{w})\\leq B\n\nRegularizer r(\\mathbf{w})\n\nProperties\n\nl_{2}-Regularization\\left.r(\\mathbf{w}) = \\mathbf{w}^{\\top}\\mathbf{w} = |{\\mathbf{w}}|_{2}^{2}\\right.\n\nADVANTAGE: Strictly Convex, DifferentiableDISADVANTAGE: Dense Solutions (it uses weights on all features, i.e. relies on all features to some degree. Ideally we would like to avoid this)\n\nl_{1}-Regularization \\left.r(\\mathbf{w}) = |\\mathbf{w}|_{1}\\right.\n\nConvex (but not strictly)DISADVANTAGE: Not differentiable at 0 (the point which minimization is intended to bring us to)Effect: Sparse (i.e. not Dense) Solutions\n\nl_p-Norm \\left.|{\\mathbf{w}}|_{p} = (\\sum\\limits_{i=1}^d v_{i}^{p})^{1/p}\\right.\n\n(often \\left.0<p\\leq1\\right.) DISADVANTAGE: Non-convex, Not differentiable, Initialization dependentADVANTAGE: Very sparse solutions\n\nFigure 4.3: Plots of Common Regularizers","type":"content","url":"/2021-10-14-empirical-risk-minimization#regularizers","position":9},{"hierarchy":{"lvl1":"Empirical Risk Minimization","lvl2":"Famous Special Cases"},"type":"lvl2","url":"/2021-10-14-empirical-risk-minimization#famous-special-cases","position":10},{"hierarchy":{"lvl1":"Empirical Risk Minimization","lvl2":"Famous Special Cases"},"content":"This section includes several special cases that deal with risk  minimization, such as Ordinary Least Squares, Ridge Regression, Lasso, and Logistic Regression. Table 4.4 provides information on their loss  functions, regularizers, as well as solutions.\n\nLoss and Regularizer\n\nComments\n\nOrdinary Least Squares             \\min_{\\mathbf{w}} \\frac{1}{n}\\sum\\limits_{i=1}^n (\\mathbf{w}^{\\top}x_{i}-y_{i})^{2}\n\nSquared Loss             No Regularization             Closed form solution:             \\left.\\mathbf{w}=(\\mathbf{X}\\mathbf{X}^\\top)^{-1}\\mathbf{X}\\mathbf{y}^{\\top}\\right.             \\left.\\mathbf{X}=[\\mathbf{x}_{1}, ..., \\mathbf{x}_{n}]\\right.             \\left.\\mathbf{y}=[y_{1},...,y_{n}]\\right.\n\nRidge Regression             \\min_{\\mathbf{w}} \\frac{1}{n}\\sum\\limits_{i=1}^n (\\mathbf{w}^{\\top}x_{i}-y_{i})^{2}+\\lambda|{w}|_{2}^{2}\n\nSquared Loss             l_{2}-Regularization                       \\left.\\mathbf{w}=(\\mathbf{X}\\mathbf{X}^{\\top}+\\lambda\\mathbb{I})^{-1}\\mathbf{X}\\mathbf{y}^{\\top}\\right.\n\nLasso             \\min_{\\mathbf{w}} \\frac{1}{n}\\sum\\limits_{i=1}^n  (\\mathbf{w}^{\\top}\\mathbf{x}_{i}-{y}_{i})^{2}+\\lambda|\\mathbf{w}|_{1}\n\n+ sparsity inducing (good for feature selection)            + Convex             - Not strictly convex (no unique solution)            - Not differentiable (at 0)             Solve with (sub)-gradient descent or                 \n\nSVEN\n\nElastic Net \\min_{\\mathbf{w}}  \\frac{1}{n}\\sum\\limits_{i=1}^n  (\\mathbf{w}^{\\top}\\mathbf{x}_{i}-{y}_{i})^{2}+\\left.\\alpha|\\mathbf{w}|_{1}+(1-\\alpha)|{\\mathbf{w}}|_{2}^{2}\\right.            \\left.\\alpha\\in[0, 1)\\right.\n\nADVANTAGE: Strictly convex (i.e. unique solution)             + sparsity inducing (good for feature selection)             + Dual of squared-loss SVM, see \n\nSVEN             DISADVANTAGE: - Non-differentiable\n\nLogistic Regression             \\min_{\\mathbf{w},b} \\frac{1}{n}\\sum\\limits_{i=1}^n \\log{(1+e^{-y_i(\\mathbf{w}^{\\top}\\mathbf{x}_{i}+b)})}\n\nOften l_{1} or l_{2} Regularized             Solve with gradient descent.             $\\left.\\Pr{(y\n\nLinear Support Vector Machine              \\min_{\\mathbf{w},b} C\\sum\\limits_{i=1}^n \\max[1-y_{i}(\\mathbf{w}^\\top{\\mathbf{x}_i+b}), 0]+|\\mathbf{w}|_2^2\n\nTypically l_2 regularized (sometimes l_1).              Quadratic program.              When \n\nkernelized leads to  sparse solutions.              Kernelized version can be solved very efficiently with specialized algorithms (e.g. \n\nSMO)\n\nTable 4.4: Special Cases","type":"content","url":"/2021-10-14-empirical-risk-minimization#famous-special-cases","position":11},{"hierarchy":{"lvl1":"Midterm Review"},"type":"lvl1","url":"/2021-10-19-midterm-review","position":0},{"hierarchy":{"lvl1":"Midterm Review"},"content":"It is more likely that there is a linear separating hyperplane if the data is high dimensional. So linear classifier usually performs well on high dimensional data.\n\nIf the Naive Bayes assumption holds, the Naive Bayes classifier becomes identical to the Bayes Optimal classifier:BO=argmax_y \\; P(Y|X) \n= argmax_y \\; \\frac{P(X|Y)P(Y)}{P(X)} \n= argmax_y \\; P(X|Y)P(Y)\n= argmax_y \\; P(Y) \\prod_{\\alpha=1}^d P([x]_\\alpha|Y)\n= NB\n\nThe KNN algorithm can be used for classification, but not regression. False\nKNN can be used for regression by averaging the labels of the k nearest neighbors.\n\nThe Bayes optimal error is the best classification error you could get if there was no noise. False\nIt is the best classification error you could get if you knew the data distribution. In fact, this error is due to label uncertainty, i.e. noise.\n\nAs your training data set size, n, approaches infinity, the k−nearest neighbor classifier is guaranteed to have an error no worse than twice the Bayes optimal error. True\nThis is true for both 1nn and knn (k>1).\n\nAs the validation set becomes extremely large, the validation error approaches the test error. True\n\nNot midterm point, but:\nif we remove all the activation function in a neural network (multiple-layer perceptron), the NN/MLP is simply equivalent to a linear regression model.","type":"content","url":"/2021-10-19-midterm-review","position":1},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff"},"type":"lvl1","url":"/2021-10-26-bias-variance-tradeoff","position":0},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff"},"content":"","type":"content","url":"/2021-10-26-bias-variance-tradeoff","position":1},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl2":"Setting Up"},"type":"lvl2","url":"/2021-10-26-bias-variance-tradeoff#setting-up","position":2},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl2":"Setting Up"},"content":"As usual, we are given a training dataset D = \\{(\\mathbf{x}_1, y_1), \\dots,  (\\mathbf{x}_n,y_n)\\}, drawn i.i.d. from some distribution P(X,Y).  Throughout this lecture we assume a regression setting, i.e. y \\in  \\mathbb{R}. In this lecture we will decompose the generalization error of a classifier into three rather interpretable terms.","type":"content","url":"/2021-10-26-bias-variance-tradeoff#setting-up","position":3},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl3":"Expected Label","lvl2":"Setting Up"},"type":"lvl3","url":"/2021-10-26-bias-variance-tradeoff#expected-label","position":4},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl3":"Expected Label","lvl2":"Setting Up"},"content":"Even though we have the same features \\mathbf{x}_1 = \\mathbf{x}_2, their labels can be different y_1 \\not= y_2. For example, if your vector \\mathbf{x} describes features of house (e.g. #bedrooms, square footage, ...) and the label y its price, you could imagine two houses with identical description selling for different prices. So for any given feature vector \\mathbf{x}, there is a distribution over possible labels. According to this idea, we define given \\mathbf{x} \\in \\mathbb{R}^d:\\bar{y}(\\mathbf{x}) = E_{y \\vert \\mathbf{x}} \\left[Y\\right] = \\int\\limits_y y \\, \\Pr(y \\vert \\mathbf{x}) \\partial y.\n\nis the expected label - the label you would expect to obtain, given a feature vector \\mathbf{x}.","type":"content","url":"/2021-10-26-bias-variance-tradeoff#expected-label","position":5},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl3":"Expected Model","lvl2":"Setting Up"},"type":"lvl3","url":"/2021-10-26-bias-variance-tradeoff#expected-model","position":6},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl3":"Expected Model","lvl2":"Setting Up"},"content":"After drawing a training set D i.i.d. from the distribution P, we will use some machine learning algorithm \\mathcal{A} on this data set D to train a model h. Formally, we denote this process as h_D = \\mathcal{A}(D). Similar to the reasoning above, a same learning algorithm \\mathcal{A} can give different models h_D based on different datasets D, so we want to find the expected model \\bar h.\\bar{h} = E_{D \\sim P^n} \\left[ h_D \\right] = \\int\\limits_D h_D \\Pr(D) dD\n\nwhere \\Pr(D) is the probability of drawing D from P^n. In practice, we cannot integrate over all datasets, so we just sample some datasets and take their average (taking average means we think the probability of drawing out each D is the same. We can assume this because each time we draw D from the same distribution P^n)","type":"content","url":"/2021-10-26-bias-variance-tradeoff#expected-model","position":7},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl3":"Expected Error","lvl2":"Setting Up"},"type":"lvl3","url":"/2021-10-26-bias-variance-tradeoff#expected-error","position":8},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl3":"Expected Error","lvl2":"Setting Up"},"content":"For a given h_D, learned on data set D with algorithm \\mathcal{A}, we can compute the generalization error as measured in squared loss here. (One can use other loss functions. We use squared loss because it is easier for the derivation later) as follows. This is the error of our model:\\epsilon_{h_D}\n= E_{(\\mathbf{x},y) \\sim P} \\left[ \\left(h_D (\\mathbf{x}) - y \\right)^2 \\right] \n= \\int\\limits_x \\! \\! \\int\\limits_y \\left( h_D(\\mathbf{x}) - y\\right)^2 \\Pr(\\mathbf{x},y) \\; d \\mathbf{x} \\; dy\n\nwhere (\\mathbf x, y) is a pair of test point.\n\nGiven the idea of “expected model” discussed above, we can compute the expected test error only given \\mathcal{A}, taking the expectation also over D. So now we have the error of an algorithm, which produces different models based on different training data:\\epsilon_{\\mathcal A}\n= E_{\\substack{(\\mathbf{x},y) \\sim P\\\\ D \\sim P^n}} \\left[\\left(h_{D}(\\mathbf{x}) - y\\right)^{2}\\right] \n= \\int_{D} \\int_{\\mathbf{x}} \\int_{y} \\left( h_{D}(\\mathbf{x}) - y\\right)^{2} \\mathrm{P}(\\mathbf{x},y) \\mathrm{P}(D) \\; d \\mathbf{x} \\;dy \\;dD\n\nwhere D is our training datasets and the (\\mathbf{x},y) pairs are the test points.\nWe are interested in exactly this expression, because it evaluates the quality of a machine learning algorithm \\mathcal{A} with respect to a data distribution P(X,Y). In the following we will show that this expression decomposes into three meaningful terms.","type":"content","url":"/2021-10-26-bias-variance-tradeoff#expected-error","position":9},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl2":"Derivation"},"type":"lvl2","url":"/2021-10-26-bias-variance-tradeoff#derivation","position":10},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl2":"Derivation"},"content":"\\begin{align}\n\tE_{\\mathbf{x},y,D}\\left[\\left[h_{D}(\\mathbf{x}) - y\\right]^{2}\\right] &= E_{\\mathbf{x},y,D}\\left[\\left[\\left(h_{D}(\\mathbf{x}) - \\bar{h}(\\mathbf{x})\\right) + \\left(\\bar{h}(\\mathbf{x}) - y\\right)\\right]^{2}\\right] \\nonumber \\\\\n    &= E_{\\mathbf{x}, D}\\left[(\\bar{h}_{D}(\\mathbf{x}) - \\bar{h}(\\mathbf{x}))^{2}\\right] + 2 \\mathrm{\\;} E_{\\mathbf{x}, y, D} \\left[\\left(h_{D}(\\mathbf{x}) - \\bar{h}(\\mathbf{x})\\right)\\left(\\bar{h}(\\mathbf{x}) - y\\right)\\right] + E_{\\mathbf{x}, y} \\left[\\left(\\bar{h}(\\mathbf{x}) - y\\right)^{2}\\right] \n\\end{align}\n\nThe middle term of the above equation is 0 as we show below:\\begin{align*}\n\tE_{\\mathbf{x}, y, D} \\left[\\left(h_{D}(\\mathbf{x}) - \\bar{h}(\\mathbf{x})\\right) \\left(\\bar{h}(\\mathbf{x}) - y\\right)\\right]\n\t&= E_{\\mathbf{x}, y} \\left\\{ E_{D}\\left[\\left(h_{D}(\\mathbf{x}) - \\bar{h}(\\mathbf{x})\\right) \\left(\\bar{h}(\\mathbf{x}) - y\\right)\\right] \\right\\} \\\\\n\t&= E_{\\mathbf{x}, y} \\left[E_{D} \\left[ h_{D}(\\mathbf{x}) - \\bar{h}(\\mathbf{x})\\right] \\left(\\bar{h}(\\mathbf{x}) - y\\right) \\right] \\\\\n    &= E_{\\mathbf{x}, y} \\left[ \\left( E_{D} \\left[ h_{D}(\\mathbf{x}) \\right] - \\bar{h}(\\mathbf{x}) \\right) \\left(\\bar{h}(\\mathbf{x}) - y \\right)\\right] \\\\\n    &= E_{\\mathbf{x}, y} \\left[ \\left(\\bar{h}(\\mathbf{x}) - \\bar{h}(\\mathbf{x}) \\right) \\left(\\bar{h}(\\mathbf{x}) - y \\right)\\right] \\\\\n    &= 0\n\\end{align*}\n\nReturning to the earlier expression, we’re left with the variance and another term:E_{\\mathbf{x}, y, D} \\left[ \\left( h_{D}(\\mathbf{x}) - y \\right)^{2} \\right] = \\underbrace{E_{\\mathbf{x}, D} \\left[ \\left(h_{D}(\\mathbf{x}) - \\bar{h}(\\mathbf{x}) \\right)^{2} \\right]}_\\mathrm{Variance} + E_{\\mathbf{x}, y}\\left[ \\left( \\bar{h}(\\mathbf{x}) - y \\right)^{2} \\right]\n\nWe can break down the second term in the above equation just as what we did to \\epsilon_\\mathcal A at first:\\begin{align}\n\tE_{\\mathbf{x}, y} \\left[ \\left(\\bar{h}(\\mathbf{x}) - y \\right)^{2}\\right] &= E_{\\mathbf{x}, y} \\left[ \\left(\\bar{h}(\\mathbf{x}) -\\bar y(\\mathbf{x}) )+(\\bar y(\\mathbf{x}) - y \\right)^{2}\\right]  \\\\\n  &=\\underbrace{E_{\\mathbf{x}, y} \\left[\\left(\\bar{y}(\\mathbf{x}) - y\\right)^{2}\\right]}_\\mathrm{Noise} + \\underbrace{E_{\\mathbf{x}} \\left[\\left(\\bar{h}(\\mathbf{x}) - \\bar{y}(\\mathbf{x})\\right)^{2}\\right]}_\\mathrm{Bias^2} + 2 \\mathrm{\\;} E_{\\mathbf{x}, y} \\left[ \\left(\\bar{h}(\\mathbf{x}) - \\bar{y}(\\mathbf{x})\\right)\\left(\\bar{y}(\\mathbf{x}) - y\\right)\\right] \n\\end{align}\n\nThe third term in the equation above is 0, as we show in the same way as before, but this time decomposes E_{\\mathbf{x}, y} into E_{\\mathbf{x}}E_{y \\mid \\mathbf{x}} (can show correctness if you write out the definition of expectation):\\begin{align*}\n\tE_{\\mathbf{x}, y} \\left[\\left(\\bar{h}(\\mathbf{x}) - \\bar{y}(\\mathbf{x})\\right)\\left(\\bar{y}(\\mathbf{x}) - y\\right)\\right] &= E_{\\mathbf{x}}\\left[E_{y \\mid \\mathbf{x}} \\left[\\bar{y}(\\mathbf{x}) - y \\right] \\left(\\bar{h}(\\mathbf{x}) - \\bar{y}(\\mathbf{x}) \\right) \\right]  = 0\n\\end{align*}\n\nThis gives us the decomposition of expected test error:\\underbrace{E_{\\mathbf{x}, y, D} \\left[\\left(h_{D}(\\mathbf{x}) - y\\right)^{2}\\right]}_\\mathrm{Expected\\;Test\\;Error} = \\underbrace{E_{\\mathbf{x}, D}\\left[\\left(h_{D}(\\mathbf{x}) - \\bar{h}(\\mathbf{x})\\right)^{2}\\right]}_\\mathrm{Variance} + \\underbrace{E_{\\mathbf{x}, y}\\left[\\left(\\bar{y}(\\mathbf{x}) - y\\right)^{2}\\right]}_\\mathrm{Noise} + \\underbrace{E_{\\mathbf{x}}\\left[\\left(\\bar{h}(\\mathbf{x}) - \\bar{y}(\\mathbf{x})\\right)^{2}\\right]}_\\mathrm{Bias^2}","type":"content","url":"/2021-10-26-bias-variance-tradeoff#derivation","position":11},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl2":"Interpretation"},"type":"lvl2","url":"/2021-10-26-bias-variance-tradeoff#interpretation","position":12},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl2":"Interpretation"},"content":"Variance: Literally, it is how much our classifier on training data D deviates from the expected classifier. This value captures how much your classifier changes if you train on a different training set. How “over-specialized” is your classifier to a particular training set (overfitting)?\n\nBias: Literally, this is how much error the best classifier \\bar h(x) still makes. It captures the inherent error that you obtain from your classifier even with infinite training data. This is due to your classifier being  “biased” to a particular kind of solution (e.g. linear classifier). In other words, bias is inherent to your model and dependent on the algorithm \\mathcal A.\n\nNoise: How big is the difference between the test point’s label and the expected label? This is a data-intrinsic noise that measures ambiguity due to your data distribution and feature representation. You can never beat this, it is an aspect of the data.\n\n\n\nFig 1: Graphical illustration of bias and variance.\nIf we have bias, it will be the board constantly shaking, so you can hardly ever hit it.","type":"content","url":"/2021-10-26-bias-variance-tradeoff#interpretation","position":13},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl2":"Detecting High Bias and High Variance"},"type":"lvl2","url":"/2021-10-26-bias-variance-tradeoff#detecting-high-bias-and-high-variance","position":14},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl2":"Detecting High Bias and High Variance"},"content":"If a classifier is under-performing (test or training error is too high), the first step is to determine the root of the problem.\n\n\n\nFigure 3: Test and training error as the number of training instances increases.\n\nThe graph above plots the training error and the test error and can  be divided into two overarching regimes. In the first regime (on the  left side of the graph), training error is below the desired error  threshold (denoted by \\epsilon), but test error is significantly  higher. In the second regime (on the right side of the graph), test  error is remarkably close to training error, but both are above the  desired tolerance of \\epsilon.","type":"content","url":"/2021-10-26-bias-variance-tradeoff#detecting-high-bias-and-high-variance","position":15},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl3":"Regime 1 (High Variance)","lvl2":"Detecting High Bias and High Variance"},"type":"lvl3","url":"/2021-10-26-bias-variance-tradeoff#regime-1-high-variance","position":16},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl3":"Regime 1 (High Variance)","lvl2":"Detecting High Bias and High Variance"},"content":"In the first regime, the cause of the poor performance is high variance.\n\nSymptoms:\n\nTraining error is much lower than test error\n\nTraining error is lower than \\epsilon\n\nTest error is above \\epsilon\n\nRemedies:\n\nAdd more training data\n\nReduce model complexity, usually by adding a regularization term (complex models are prone to high variance)\n\nBagging (will be covered later in the course)\n\nEarly Stopping","type":"content","url":"/2021-10-26-bias-variance-tradeoff#regime-1-high-variance","position":17},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl3":"Regime 2 (High Bias)","lvl2":"Detecting High Bias and High Variance"},"type":"lvl3","url":"/2021-10-26-bias-variance-tradeoff#regime-2-high-bias","position":18},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl3":"Regime 2 (High Bias)","lvl2":"Detecting High Bias and High Variance"},"content":"Unlike the first regime, the second regime indicates high bias: the  model being used is not robust enough to produce an accurate prediction.\n\nSymptoms:\n\nTraining error is higher than \\epsilon\n\nRemedies:\n\nUse more complex model (e.g. kernelize, use non-linear models)\n\nAdd features\n\nBoosting (will be covered later in the course)","type":"content","url":"/2021-10-26-bias-variance-tradeoff#regime-2-high-bias","position":19},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl3":"Thought Process to determine whether a model is high bias / variance:","lvl2":"Detecting High Bias and High Variance"},"type":"lvl3","url":"/2021-10-26-bias-variance-tradeoff#thought-process-to-determine-whether-a-model-is-high-bias-variance","position":20},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl3":"Thought Process to determine whether a model is high bias / variance:","lvl2":"Detecting High Bias and High Variance"},"content":"High Variance:\n\nImagine you train several models on different dataset you draw from the original distribution P, each dataset is very small. How do the results compare with each other? If differ by a lot, high var.\n\nIs this model overfitting? If yes, high var.\n\nHigh Bias:\n\nImagine you train this model on all data from the original distribution P, if it still performs badly, it has a high bias.\n\nIs this model underfitting? If yes, high bias.\n\nExample:\n\n\n\nBias\n\nVariance\n\nkNN with small k\n\nlow\n\nhigh\n\nkNN with large k\n\nhigh\n\nlow","type":"content","url":"/2021-10-26-bias-variance-tradeoff#thought-process-to-determine-whether-a-model-is-high-bias-variance","position":21},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl3":"Ideal Case","lvl2":"Detecting High Bias and High Variance"},"type":"lvl3","url":"/2021-10-26-bias-variance-tradeoff#ideal-case","position":22},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl3":"Ideal Case","lvl2":"Detecting High Bias and High Variance"},"content":"Training and test error are both below the acceptable test error line.","type":"content","url":"/2021-10-26-bias-variance-tradeoff#ideal-case","position":23},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl3":"Reduce Noise","lvl2":"Detecting High Bias and High Variance"},"type":"lvl3","url":"/2021-10-26-bias-variance-tradeoff#reduce-noise","position":24},{"hierarchy":{"lvl1":"Bias-Variance Tradeoff","lvl3":"Reduce Noise","lvl2":"Detecting High Bias and High Variance"},"content":"Two reasons can cause noise and we introduce the corresponding solution:\n\nLabels are just wrongly assigned <- no solution but to assign the correct label\n\nFeatures are not expressive enough <- add more features","type":"content","url":"/2021-10-26-bias-variance-tradeoff#reduce-noise","position":25},{"hierarchy":{"lvl1":"Model Selection Tricks"},"type":"lvl1","url":"/2021-10-28-model-selection-tricks","position":0},{"hierarchy":{"lvl1":"Model Selection Tricks"},"content":"","type":"content","url":"/2021-10-28-model-selection-tricks","position":1},{"hierarchy":{"lvl1":"Model Selection Tricks","lvl2":"Overfitting and Underfitting"},"type":"lvl2","url":"/2021-10-28-model-selection-tricks#overfitting-and-underfitting","position":2},{"hierarchy":{"lvl1":"Model Selection Tricks","lvl2":"Overfitting and Underfitting"},"content":"There are two problematic cases which can arise when learning a classifier on a data set: underfitting and overfitting, each of which relate to the degree to which the data in the training set is extrapolated to apply to unknown data:\n\nUnderfitting: The classifier learned on the training set is not expressive enough (i.e. too simple) to even account for the data provided. In this case, both the training error and the test error will be high, as the classifier does not account for relevant information present in the training set.\n\nOverfitting: The classifier learned on the training set is too specific, and cannot be used to accurately infer anything about unseen data. Although training error continues to decrease over time, test error will begin to increase again as the classifier begins to make decisions based on patterns which exist only in the training set and not in the broader distribution.","type":"content","url":"/2021-10-28-model-selection-tricks#overfitting-and-underfitting","position":3},{"hierarchy":{"lvl1":"Model Selection Tricks","lvl2":"Identify Regularizer \\lambda"},"type":"lvl2","url":"/2021-10-28-model-selection-tricks#identify-regularizer-lambda","position":4},{"hierarchy":{"lvl1":"Model Selection Tricks","lvl2":"Identify Regularizer \\lambda"},"content":"\n\nFigure 1: overfitting and underfitting. Note the x-axis is \\lambda:\n\nthe bigger \\lambda is, the simpler the model is, so is likely to underfit.\n\nthe smaller \\lambda is, the more complex the model is, so is more likely to overfit","type":"content","url":"/2021-10-28-model-selection-tricks#identify-regularizer-lambda","position":5},{"hierarchy":{"lvl1":"Model Selection Tricks","lvl3":"K-Fold Cross Validation","lvl2":"Identify Regularizer \\lambda"},"type":"lvl3","url":"/2021-10-28-model-selection-tricks#k-fold-cross-validation","position":6},{"hierarchy":{"lvl1":"Model Selection Tricks","lvl3":"K-Fold Cross Validation","lvl2":"Identify Regularizer \\lambda"},"content":"When we train the data, we can use the k-fold cross validation: divide the training data into k partitions. Train on k-1 of them and leave one out as validation set. Do this k times (i.e. leave out every partition exactly once) and average the validation error across runs. This gives you a good estimate of the validation error (even with standard deviation).\n\nIn the extreme case, you can have k=n, i.e. you only leave a single data point out (this is often referred to as LOOCV - Leave One Out Cross Validation). LOOCV is important if your data set is small and cannot afford to leave out many data points for evaluation.\n\nWe can also use k-fold cross validation solely to determine the hyperparameter \\lambda: divide data into k partitions, each with a specific \\lambda; and we will select the \\lambda that gives the best validation error.\n\nNo matter what we use k-fold for, after selecting the best model/parameter based on k-fold, we will have to train our model on the whole dataset for another time (remember we divided it into k-1 training set and 1 validation set), and use that as our final model.","type":"content","url":"/2021-10-28-model-selection-tricks#k-fold-cross-validation","position":7},{"hierarchy":{"lvl1":"Model Selection Tricks","lvl3":"Telescopic search","lvl2":"Identify Regularizer \\lambda"},"type":"lvl3","url":"/2021-10-28-model-selection-tricks#telescopic-search","position":8},{"hierarchy":{"lvl1":"Model Selection Tricks","lvl3":"Telescopic search","lvl2":"Identify Regularizer \\lambda"},"content":"Do two searches:\n\nfind the best order of magnitude for \\lambda. 先搜索数量级\n\ndo a more fine-grained search around the best \\lambda found so far. 再搜索此数量级下的最好值\n\nFor example, first you try \\lambda=0.01,0.1,1,10,100. It turns out 10 is the best performing value. Then you try out \\lambda=5,10,15,20,25,...,95 to test values “around” 10.","type":"content","url":"/2021-10-28-model-selection-tricks#telescopic-search","position":9},{"hierarchy":{"lvl1":"Model Selection Tricks","lvl3":"Grid and Random search","lvl2":"Identify Regularizer \\lambda"},"type":"lvl3","url":"/2021-10-28-model-selection-tricks#grid-and-random-search","position":10},{"hierarchy":{"lvl1":"Model Selection Tricks","lvl3":"Grid and Random search","lvl2":"Identify Regularizer \\lambda"},"content":"Grid Search: If you have multiple parameters (e.g. \\lambda and also the kernel width \\sigma in case you are using a \n\nkernel) a simple way to find the best value of both of them is to fix a set of values for each hyper-parameter and try out every combination. 在 grid 上顺序选择\n\nRandom Search: Instead of selecting hyper-parameters on a pre-defined grid, we select them randomly within pre-defined intervals. 在区间(也可以理解成 grid)上随机选择\n\n\n\nFigure 2: Grid Search vs Random search (red indicates lower loss, blue indicates high loss).","type":"content","url":"/2021-10-28-model-selection-tricks#grid-and-random-search","position":11},{"hierarchy":{"lvl1":"Model Selection Tricks","lvl2":"Early Stopping"},"type":"lvl2","url":"/2021-10-28-model-selection-tricks#early-stopping","position":12},{"hierarchy":{"lvl1":"Model Selection Tricks","lvl2":"Early Stopping"},"content":"Stop your optimization after M (>= 0) number of gradient steps, even if optimization has not converged yet, because our final model almost always overfits.\n\n实际操作中，边训练边存模型，找到有最小 validation error 的模型（即我们不一定用最终的模型，相当于我们提早结束了训练)\n\n\n\nFigure 3: Early stopping，注意这里和前图 1 不一样，横轴是 M","type":"content","url":"/2021-10-28-model-selection-tricks#early-stopping","position":13},{"hierarchy":{"lvl1":"Kernels"},"type":"lvl1","url":"/2021-11-02-kernels","position":0},{"hierarchy":{"lvl1":"Kernels"},"content":"Linear classifiers are great, but what if there exists no linear decision boundary? We should observe that nonlinear classifier in our (usually low) dimensional space is just a linear classifier on a higher dimensional space. When we project this higher-dimensional linear classifier into our lower-dimensional space, it appears nonlinear. As it turns out, there is an elegant way to incorporate non-linearities into most linear classifiers.","type":"content","url":"/2021-11-02-kernels","position":1},{"hierarchy":{"lvl1":"Kernels","lvl2":"Handcrafted Feature Expansion"},"type":"lvl2","url":"/2021-11-02-kernels#handcrafted-feature-expansion","position":2},{"hierarchy":{"lvl1":"Kernels","lvl2":"Handcrafted Feature Expansion"},"content":"We can make linear classifiers non-linear by expanding the existing features. Formally, for a data vector \\mathbf{x}\\in\\mathbb{R}^d, we apply the transformation \\mathbf{x} \\rightarrow \\phi(\\mathbf{x}) where \\phi(\\mathbf{x})\\in\\mathbb{R}^D. Usually D \\gg d because we add dimensions that capture non-linear interactions among the original features.\nl\n\nAdvantage: It is simple, and your problem stays convex and well behaved. (i.e. you can still use your original gradient descent code, just with the higher dimensional representation)\n\nDisadvantage: \\phi(\\mathbf{x}) might be very high dimensional.\n\nConsider the following example: \\mathbf{x}=\\begin{pmatrix}x_1\\\\  x_2\\\\ \\vdots \\\\ x_d \\end{pmatrix}, and define \\phi(\\mathbf{x})=\\begin{pmatrix}1\\\\ x_1\\\\ \\vdots \\\\x_d \\\\ x_1x_2 \\\\  \\vdots \\\\ x_{d-1}x_d\\\\ \\vdots \\\\x_1x_2\\cdots x_d \\end{pmatrix}.\n\nThis new representation, \\phi(\\mathbf{x}), is very expressive and allows for complicated non-linear decision boundaries - but the dimensionality is extremely high D = 2^d. This makes our algorithm unbearable (and quickly prohibitively) slow.","type":"content","url":"/2021-11-02-kernels#handcrafted-feature-expansion","position":3},{"hierarchy":{"lvl1":"Kernels","lvl2":"The Kernel Trick"},"type":"lvl2","url":"/2021-11-02-kernels#the-kernel-trick","position":4},{"hierarchy":{"lvl1":"Kernels","lvl2":"The Kernel Trick"},"content":"","type":"content","url":"/2021-11-02-kernels#the-kernel-trick","position":5},{"hierarchy":{"lvl1":"Kernels","lvl3":"Gradient Descent with Squared Loss","lvl2":"The Kernel Trick"},"type":"lvl3","url":"/2021-11-02-kernels#gradient-descent-with-squared-loss","position":6},{"hierarchy":{"lvl1":"Kernels","lvl3":"Gradient Descent with Squared Loss","lvl2":"The Kernel Trick"},"content":"The kernel trick is a way to get around this dilemma by learning a function in the much higher dimensional space, without explicitly computing the value of a single vector \\phi(\\mathbf{x}) or ever computing the full vector \\mathbf{w}. We will represent these values only with \\mathbf x and y.\n\nIt is based on the following observation: If we use gradient descent with any one of our standard \n\nloss functions, the gradient is a linear combination of the input samples. For example, in squared loss:\\ell(\\mathbf{w}) = \\sum_{i=1}^n (\\mathbf{w}^\\top  \\mathbf{x}_i-y_i)^2\n\nand the gradient descent rule updates \\mathbf w over time with step size / learning rate s > 0.w_{t+1} \\leftarrow w_t - s(\\frac{\\partial \\ell}{\\partial \\mathbf{w}})\\ \\textrm{ where: }\n  \\frac{\\partial \\ell}{\\partial \\mathbf{w}}=\\sum_{i=1}^n \\underbrace{2(\\mathbf{w}^\\top  \\mathbf{x}_i-y_i)}_{\\gamma_i\\ :\\ \\textrm{function of $\\mathbf{x}_i, y_i$}} \\mathbf{x}_i = \\sum_{i=1}^n\\gamma_i \\mathbf{x}_i\n\nwhere \\gamma_i is the gradient of loss function: \\gamma_i = 2(h(\\mathbf x_i)-y_i)\n\nWe will now show by induction that we can express \\mathbf{w} as a linear combination of all input vectors, namely\\mathbf{w}=\\sum_{i=1}^n \\alpha_i {\\mathbf{x}}_i.\n\nBase Case: Since the loss is convex, the final solution is independent of the initialization, and we can initialize \\mathbf{w}_0 to be whatever we want. For convenience, set \\mathbf{w}_0=\\begin{pmatrix}0 \\\\  \\vdots \\\\ 0\\end{pmatrix}. This is trivially a linear combination of \\mathbf x.\n\nInductive Step:\n\n$$\\begin{align}\n\\mathbf{w}_1=&\\mathbf{w}_0-s\\sum_{i=1}^n2(\\mathbf{w}_0^\\top  \\mathbf{x}_i-y_i)\\mathbf{x}_i=\\sum_{i=1}^n \\alpha_i^0 {\\mathbf{x}}_i-s\\sum_{i=1}^n\\gamma_i^0\\mathbf{x}_i=\\sum_{i=1}^n\\alpha_i^1\\mathbf{x}_i&(\\textrm{with $\\alpha_i^1=\\alpha_i^0-s\\gamma_i^0$})\\nonumber\\\\\n\n\\mathbf{w}_2=&\\mathbf{w}_1-s\\sum_{i=1}^n2(\\mathbf{w}_1^\\top  \\mathbf{x}_i-y_i)\\mathbf{x}_i=\\sum_{i=1}^n \\alpha_i^1\\mathbf{x}_i-s\\sum_{i=1}^n\\gamma_i^1\\mathbf{x}_i=\\sum_{i=1}^n\\alpha_i^2\\mathbf{x}_i&(\\textrm{with $\\alpha_i^2=\\alpha_i^1\\mathbf{x}_i-s\\gamma_i^1$})\\nonumber\\\\\n\n\\cdots & \\qquad\\qquad\\qquad\\cdots &\\cdots\\nonumber\\\\\n\n\\mathbf{w}_t=&\\mathbf{w}_{t-1}-s\\sum_{i=1}^n2(\\mathbf{w}_{t-1}^\\top  \\mathbf{x}_i-y_i)\\mathbf{x}_i=\\sum_{i=1}^n \\alpha_i^{t-1}\\mathbf{x}_i-s\\sum_{i=1}^n\\gamma_i^{t-1}\\mathbf{x}_i=\\sum_{i=1}^n\\alpha_i^t\\mathbf{x}_i&(\\textrm{with $\\alpha_i^t=\\alpha_i^{t-1}-s\\gamma_i^{t-1}$})\\nonumber\n\\end{align}\n\n$$\n\nThe update-rule for \\alpha_i^t is thus\\alpha_i^t=\\alpha_i^{t-1}-s\\gamma_i^{t-1}\n\nSince a_i^0 = 0, we can write the closed form\\alpha_i^t=-s\\sum_{r=0}^{t-1}\\gamma_i^{r}\n\nTherefore, we have shown that we can perform the entire gradient descent update rule without ever expressing \\mathbf{w} explicitly. We just keep track of the n coefficients \\alpha_1,\\dots,\\alpha_n.\n\nNow that \\mathbf{w} can be written as a linear combination of the training set, we can also express the prediction result purely in terms of inner-products between training inputs:h({\\mathbf{x}}_t)=\\mathbf{w}^\\top {\\mathbf{x}}_t=\\sum_{j=1}^n\\alpha_j{\\mathbf{x}}_j^\\top {\\mathbf{x}}_t.\n\nConsequently, we can also re-write the squared-loss from \\ell(\\mathbf{w}) = \\sum_{i=1}^n (\\mathbf{w}^\\top  \\mathbf{x}_i-y_i)^2 entirely in terms of inner-product between training inputs:\\ell(\\mathbf{\\alpha}) = \\sum_{i=1}^n \\left(\\sum_{j=1}^n\\alpha_j\\mathbf{x}_j^\\top  \\mathbf{x}_i-y_i\\right)^2\n\nDo you notice a theme? The only information we ever need in order to learn a hyper-plane classifier with the squared-loss is inner-products between all pairs of data vectors.","type":"content","url":"/2021-11-02-kernels#gradient-descent-with-squared-loss","position":7},{"hierarchy":{"lvl1":"Kernels","lvl2":"Inner-Product Computation"},"type":"lvl2","url":"/2021-11-02-kernels#inner-product-computation","position":8},{"hierarchy":{"lvl1":"Kernels","lvl2":"Inner-Product Computation"},"content":"Let’s go back to the previous example, \\phi(\\mathbf{x})=\\begin{pmatrix}1\\\\ x_1\\\\ \\vdots \\\\x_d \\\\ x_1x_2 \\\\  \\vdots \\\\ x_{d-1}x_d\\\\ \\vdots \\\\x_1x_2\\cdots x_d \\end{pmatrix}.\n\nThe inner product \\phi(\\mathbf{x})^\\top \\phi(\\mathbf{z}) can be formulated as:\\phi(\\mathbf{x})^\\top \\phi(\\mathbf{z})=1\\cdot 1+x_1z_1+x_2z_2+\\cdots +x_1x_2z_1z_2+ \\cdots +x_1\\cdots x_dz_1\\cdots z_d=\\prod_{k=1}^d(1+x_kz_k)\\text{.}\n\nThe sum of 2^d terms becomes the product of d terms. Define the kernel function \\mathsf k as:\\mathsf{k}(\\mathbf{x}_i,\\mathbf{x}_j) =\\phi(\\mathbf{x}_i)^\\top  \\phi(\\mathbf{x}_j)\n\nWith a finite training set of n samples, inner products are often pre-computed and stored in a Kernel Matrix:\\mathsf{K}_{ij} = \\mathsf{k}(\\mathbf{x}_i,\\mathbf{x}_j) = \\phi(\\mathbf{x}_i)^\\top \\phi(\\mathbf{x}_j)\n\nObviously, \\mathsf K is symmetric. If we store the matrix \\mathsf{K}, we only need to do simple matrix look-ups and low-dimensional computations throughout the gradient descent algorithm. To make the formula more readable, we sill use the “kernel function” representation instead of the “matrix” representation. The final classifier becomes:h(\\mathbf{x}_t)=\\sum_{i=1}^n\\alpha_i \\mathsf{k}(\\mathbf{x}_i,\\mathbf{x}_t)\n\nSo we can rewrite function \\gamma as\\gamma_i = 2(h(\\mathbf x_i)-y_i) = 2 \\left[\\left(\\sum_{j=1}^n \\alpha_j \\mathsf{k}(\\mathbf{x}_i,\\mathbf{x}_j) \\right)-y_i \\right]\n\nThe gradient update becomes:\\alpha_i^t =\n\t\\alpha_i^{t-1} - s\\gamma_i^{t-1} =\n\t\\alpha_i^{t-1} - 2s\\left[\\left(\\sum_{j=1}^n \\alpha_j \\mathsf{k}(\\mathbf{x}_i,\\mathbf{x}_j) \\right)-y_i \\right]","type":"content","url":"/2021-11-02-kernels#inner-product-computation","position":9},{"hierarchy":{"lvl1":"Kernels","lvl2":"General Kernels"},"type":"lvl2","url":"/2021-11-02-kernels#general-kernels","position":10},{"hierarchy":{"lvl1":"Kernels","lvl2":"General Kernels"},"content":"Linear: \\mathsf{K}(\\mathbf{x},\\mathbf{z})=\\mathbf{x}^\\top \\mathbf{z}, \\phi(\\mathbf{x}) = \\mathbf{x}: The linear kernel is equivalent to just using a linear classifier, but it actually runs faster because it is a kernel and updates in a kernel way (covered in the next lecture what means to “update in a kernel way”)\n\nPolynomial: \\mathsf{K}(\\mathbf{x},\\mathbf{z})=(1+\\mathbf{x}^\\top \\mathbf{z})^d, \\phi(\\mathbf x) is something similar to the \\phi above, but in even higher dimension. This kernel contains all polynomials of up to d, including something like \\mathbf x^2 \\mathbf z ^3, which is not covered by the kernel function in the previous section (See Equation 8).\n\nRadial Basis Function (RBF) (aka Gaussian Kernel): \\mathsf{K}(\\mathbf{x},\\mathbf{z})= e^\\frac{-\\|\\mathbf{x}-\\mathbf{z}\\|^2}{\\sigma^2}: Though the polynomial kernel is in very high dimensional, it is still finite, but the dimension of RBF’s corresponding feature vector \\phi(\\mathbf x) is infinite and cannot be computed. However, an effective low dimensional approximation exists.\n\nExponential Kernel: \\mathsf{K}(\\mathbf{x},\\mathbf{z})= e^\\frac{-\\| \\mathbf{x}-\\mathbf{z}\\|}{2\\sigma^2}\n\nLaplacian Kernel: \\mathsf{K}(\\mathbf{x},\\mathbf{z})= e^\\frac{-| \\mathbf{x}-\\mathbf{z}|}{\\sigma}\n\nSigmoid Kernel: \\mathsf{K}(\\mathbf{x},\\mathbf{z})=\\tanh(\\mathbf{a}\\mathbf{x}^\\top  + c)","type":"content","url":"/2021-11-02-kernels#general-kernels","position":11},{"hierarchy":{"lvl1":"Kernels","lvl2":"Kernel functions"},"type":"lvl2","url":"/2021-11-02-kernels#kernel-functions","position":12},{"hierarchy":{"lvl1":"Kernels","lvl2":"Kernel functions"},"content":"Can any function \\mathsf{K}(\\cdot,\\cdot) be used as a kernel?\n\nNo, the matrix \\mathsf{K}(\\mathbf{x}_i,\\mathbf{x}_j) has to correspond to real inner-products of \\phi({\\mathbf{x}}) - \\mathbf x after some transformation. This is the case if and only if \\mathsf{K} is positive semi-definite.\n\nProve: recall the definition of positive semi-definite: A matrix A\\in \\mathbb{R}^{n\\times n} is positive semi-definite iff \\forall \\mathbf{q}\\in\\mathbb{R}^n, \\mathbf{q}^\\top A\\mathbf{q}\\geq 0.\n\nAll Kernel functions are positive semi-definite:\n\nRemember \\mathsf{K}_{ij}=\\phi(\\mathbf{x}_i)^\\top  \\phi(\\mathbf{x}_j). So \\mathsf{K}=\\Phi^\\top\\Phi, where \\Phi=[\\phi(\\mathbf{x}_1),\\dots,\\phi(\\mathbf{x}_n)]. It follows that \\mathsf{K} is p.s.d., because \\mathbf{q}^\\top\\mathsf{K}\\mathbf{q}=(\\Phi^\\top \\mathbf{q})^2\\geq 0.\n\nAll positive semi-definite matrix produces a kernel:\n\nif any matrix \\mathbf{A} is p.s.d., it can be decomposed as A=\\Phi^\\top\\Phi for some realization of \\Phi.","type":"content","url":"/2021-11-02-kernels#kernel-functions","position":13},{"hierarchy":{"lvl1":"More on Kernels"},"type":"lvl1","url":"/2021-11-04-more-on-kernels","position":0},{"hierarchy":{"lvl1":"More on Kernels"},"content":"","type":"content","url":"/2021-11-04-more-on-kernels","position":1},{"hierarchy":{"lvl1":"More on Kernels","lvl2":"Well-defined kernels"},"type":"lvl2","url":"/2021-11-04-more-on-kernels#well-defined-kernels","position":2},{"hierarchy":{"lvl1":"More on Kernels","lvl2":"Well-defined kernels"},"content":"We can build kernels by recursively combining one or more of the following rules:\n\n\\mathsf{k}(\\mathbf{x}, \\mathbf{z})=c\\mathsf{k_1}(\\mathbf{x},\\mathbf{z})\n\n\\mathsf{k}(\\mathbf{x}, \\mathbf{z})=\\mathsf{k_1}(\\mathbf{x},\\mathbf{z})+\\mathsf{k_2}(\\mathbf{x},\\mathbf{z})\n\n\\mathsf{k}(\\mathbf{x}, \\mathbf{z})=g(\\mathsf{k}(\\mathbf{x},\\mathbf{z}))\n\n\\mathsf{k}(\\mathbf{x}, \\mathbf{z})=\\mathsf{k_1}(\\mathbf{x},\\mathbf{z})\\mathsf{k_2}(\\mathbf{x},\\mathbf{z})\n\n\\mathsf{k}(\\mathbf{x}, \\mathbf{z})=f(\\mathbf{x})\\mathsf{k_1}(\\mathbf{x},\\mathbf{z})f(\\mathbf{z})\n\n\\mathsf{k}(\\mathbf{x}, \\mathbf{z})=e^{\\mathsf{k_1}(\\mathbf{x},\\mathbf{z})}\n\nwhere k_1,k_2 are well-defined kernels, c\\geq 0, g is a polynomial function with positive coefficients, f is any function and \\mathbf{A}\\succeq 0 is positive semi-definite.","type":"content","url":"/2021-11-04-more-on-kernels#well-defined-kernels","position":3},{"hierarchy":{"lvl1":"More on Kernels","lvl2":"Kernel Machines"},"type":"lvl2","url":"/2021-11-04-more-on-kernels#kernel-machines","position":4},{"hierarchy":{"lvl1":"More on Kernels","lvl2":"Kernel Machines"},"content":"","type":"content","url":"/2021-11-04-more-on-kernels#kernel-machines","position":5},{"hierarchy":{"lvl1":"More on Kernels","lvl3":"Kernalizing an Algorithm","lvl2":"Kernel Machines"},"type":"lvl3","url":"/2021-11-04-more-on-kernels#kernalizing-an-algorithm","position":6},{"hierarchy":{"lvl1":"More on Kernels","lvl3":"Kernalizing an Algorithm","lvl2":"Kernel Machines"},"content":"An algorithm can be kernelized in 3 steps:\n\nProve that the solution lies in the span of the training points (i.e. \\mathbf{w}=\\sum_{i=1}^n \\alpha_i \\mathbf{x}_i for some \\alpha_i)\n\nReplace \\mathbf w with \\sum\\alpha_i \\mathbf{x}_i in the algorithm, so we have prediction h(\\mathbf{x}_t)= \\mathbf w \\mathbf{x}_t = \\sum_{i=1}^n\\alpha_i \\mathbf{x}_i^T  \\mathbf{x}_i\n\nDefine a kernel function and substitute \\mathsf{k}(\\mathbf{x}_i,\\mathbf{x}_j) for \\mathbf{x}_i^\\top \\mathbf{x}_j, so h(\\mathbf{x}_t) = \\sum_{i=1}^n\\alpha_i \\mathbf{x}_i^T  \\mathbf{x}_i=  \\sum_{i=1}^n\\alpha_i \\mathsf{k}(\\mathbf{x}_i,\\mathbf{x}_t)","type":"content","url":"/2021-11-04-more-on-kernels#kernalizing-an-algorithm","position":7},{"hierarchy":{"lvl1":"More on Kernels","lvl3":"Example: Algorithm with Squared Loss","lvl2":"Kernel Machines"},"type":"lvl3","url":"/2021-11-04-more-on-kernels#example-algorithm-with-squared-loss","position":8},{"hierarchy":{"lvl1":"More on Kernels","lvl3":"Example: Algorithm with Squared Loss","lvl2":"Kernel Machines"},"content":"","type":"content","url":"/2021-11-04-more-on-kernels#example-algorithm-with-squared-loss","position":9},{"hierarchy":{"lvl1":"More on Kernels","lvl4":"Recap","lvl3":"Example: Algorithm with Squared Loss","lvl2":"Kernel Machines"},"type":"lvl4","url":"/2021-11-04-more-on-kernels#recap","position":10},{"hierarchy":{"lvl1":"More on Kernels","lvl4":"Recap","lvl3":"Example: Algorithm with Squared Loss","lvl2":"Kernel Machines"},"content":"linear regression minimizes the following squared loss:\\min_\\mathbf{w} \\sum_{i=1}^{n} (\\mathbf{w}^\\top \\mathbf{x}_i -y_i)^2\n\nThe solution of OLS can be written in closed form:\\mathbf{w}=(\\mathbf{X}\\mathbf{X}^\\top)^{-1} \\mathbf{X} \\mathbf{y}\n\nNote here our \\mathbf x has already gone through the transformation \\phi into the feature space, so \\mathbf x = \\phi(\\mathbf x_{original \\;data}). Therefore, the conclusion we make here generalizes to all kinds of kernel with \\mathsf{k}(\\mathbf{x}_{og \\;i},\\mathbf{x}_{og \\;j}) =\\phi(\\mathbf{x}_{og \\;i})^\\top  \\phi(\\mathbf{x}_{og \\;j})  and \\mathsf K = \\mathbf X ^T \\mathbf X.","type":"content","url":"/2021-11-04-more-on-kernels#recap","position":11},{"hierarchy":{"lvl1":"More on Kernels","lvl4":"Kernelization","lvl3":"Example: Algorithm with Squared Loss","lvl2":"Kernel Machines"},"type":"lvl4","url":"/2021-11-04-more-on-kernels#kernelization","position":12},{"hierarchy":{"lvl1":"More on Kernels","lvl4":"Kernelization","lvl3":"Example: Algorithm with Squared Loss","lvl2":"Kernel Machines"},"content":"We begin by expressing the solution \\mathbf{w} as a linear combination of the training inputs\\mathbf{w}=\\sum_{i=1}^{n} \\alpha_i\\mathbf{x}_i=\\mathbf{X}\\vec{\\alpha}\n\nWe derived in the \n\nprevious lecture that such a vector \\vec \\alpha must always exists. Also rewrite the prediction result h as:h(\\mathbf{z})=\\mathbf{w}^\\top \\mathbf{z} = \\sum_{i=1}^n\\alpha_i \\mathbf{x}_i^\\top\\mathbf{z}.\n\nRevisit our minimization problem:\\begin{align}\n& \\min_\\mathbf{w} \\sum_{i=1}^{n} (\\mathbf{w}^\\top \\mathbf{x}_i -y_i)^2 \\\\\n=& \\min_\\mathbf{w} || \\mathbf X^Tw - y ||_2^2 \\\\\n=& \\min_\\mathbf{w} || \\mathbf X^T \\mathbf{X}{\\alpha} - y ||_2^2 \\\\\n=& \\min_\\mathbf{w} || \\mathsf K \\alpha - y ||_2^2\n\\end{align}\n\nWe obtain a min value when \\mathsf K \\alpha - y  = 0, so when \\alpha = \\mathsf K^{-1}y , but this is only true when \\mathsf K is invertible (only happens when all pivots in \\mathsf K are non-zero, so only happens when \\mathsf K is positive definite) Since \\mathsf K is merely positive semi-definite, its invertible is not guaranteed, so we generalize it to:(\\mathsf K + \\tau^2 I) \\alpha - y  = 0\n\nand the solution becomes\\alpha = (\\mathsf K + \\tau^2 I)^{-1} y","type":"content","url":"/2021-11-04-more-on-kernels#kernelization","position":13},{"hierarchy":{"lvl1":"More on Kernels","lvl4":"Testing","lvl3":"Example: Algorithm with Squared Loss","lvl2":"Kernel Machines"},"type":"lvl4","url":"/2021-11-04-more-on-kernels#testing","position":14},{"hierarchy":{"lvl1":"More on Kernels","lvl4":"Testing","lvl3":"Example: Algorithm with Squared Loss","lvl2":"Kernel Machines"},"content":"The prediction of a test point \\mathbf{z} then becomesh(\\mathbf{z})=\\mathbf{z}^\\top \\mathbf{w} =\\mathbf{z}^\\top\\underbrace{\\mathbf{X}\\vec{\\alpha}}_{\\mathbf{w}} =\\underbrace{\\mathbf{k}_*}_{\\mathbf{z}^\\top\\mathbf{X}}\\underbrace{(\\mathbf{K}+\\tau^2\\mathbf{I})^{-1}\\mathbf{y}}_{\\vec{\\alpha}}=\\mathbf{k}_*\\vec{\\alpha}\n\nwhere \\mathbf{k}_* is the kernel (vector) of the test point with the training points after the mapping into feature space through \\phi, i.e. the i^{th} dimension corresponds to [\\mathbf{k}_*]_{i}=\\phi(\\mathbf{z})^\\top\\phi(\\mathbf{x}_i).","type":"content","url":"/2021-11-04-more-on-kernels#testing","position":15},{"hierarchy":{"lvl1":"Decision Tree"},"type":"lvl1","url":"/2021-11-09-decision-tree","position":0},{"hierarchy":{"lvl1":"Decision Tree"},"content":"","type":"content","url":"/2021-11-09-decision-tree","position":1},{"hierarchy":{"lvl1":"Decision Tree","lvl2":"Motivation for Decision Trees"},"type":"lvl2","url":"/2021-11-09-decision-tree#motivation-for-decision-trees","position":2},{"hierarchy":{"lvl1":"Decision Tree","lvl2":"Motivation for Decision Trees"},"content":"Recall the \n\nkD tree data structure. Decision trees also divide the data into different regions, except that it stops dividing when current region is pure (all have the same label). Therefore, if a test point falls into any bucket, we can just return the label of that bucket. Our new goal becomes to build a tree that is:\n\nMaximally compact (of minimal size)\n\nOnly has pure leaves\n\nFinding such a minimum size tree turns out to be NP-Hard. But the good news is: We can approximate it very effectively with a greedy strategy. We keep splitting the data to minimize an impurity function that measures label purity amongst the children.","type":"content","url":"/2021-11-09-decision-tree#motivation-for-decision-trees","position":3},{"hierarchy":{"lvl1":"Decision Tree","lvl2":"Impurity Functions"},"type":"lvl2","url":"/2021-11-09-decision-tree#impurity-functions","position":4},{"hierarchy":{"lvl1":"Decision Tree","lvl2":"Impurity Functions"},"content":"Data: S=\\left\\{ \\left( \\mathbf{x}_1,y_1 \\right),\\dots,\\left(  \\mathbf{x}_n,y_n \\right) \\right\\}, y_i\\in \\left\\{ 1,\\dots,c \\right\\}, where c is the number of classes.\n\nLet S_k\\subseteq S be the inputs with labels k. Formally, S_k=\\left \\{ \\left ( \\mathbf{x},y \\right )\\in S:y=k \\right \\}, so S=S_1\\cup \\dots \\cup S_c. Definep_k=\\frac{\\left | S_k \\right |}{\\left | S \\right |}\\leftarrow \\textrm{fraction of inputs in } S \\textrm{ with label } k\n\nWe know what we don’t want (Uniform Distribution): p_1=p_2=\\dots=p_c=\\frac{1}{c}. This is the worst case since each class is equally likely. Making decision based on some algorithm is no better than random guessing.","type":"content","url":"/2021-11-09-decision-tree#impurity-functions","position":5},{"hierarchy":{"lvl1":"Decision Tree","lvl3":"Gini impurity","lvl2":"Impurity Functions"},"type":"lvl3","url":"/2021-11-09-decision-tree#gini-impurity","position":6},{"hierarchy":{"lvl1":"Decision Tree","lvl3":"Gini impurity","lvl2":"Impurity Functions"},"content":"So Gini impurity on a leaf is:G(S) = \\sum_{k=1}^{c}p_k(1-p_k)\n\n\n\nFig: The Gini Impurity Function in the binary case reaches its maximum at p=0.5\n\nThis corresponds with the idea that uniform distribution is the least we want to see.\n\nGini impurity of a tree is:G^T(S)=\\frac{\\left | S_L \\right |}{\\left | S \\right |}G^T(S_L)+\\frac{\\left | S_R \\right |}{\\left | S \\right |}G^T(S_R)","type":"content","url":"/2021-11-09-decision-tree#gini-impurity","position":7},{"hierarchy":{"lvl1":"Decision Tree","lvl3":"Entropy","lvl2":"Impurity Functions"},"type":"lvl3","url":"/2021-11-09-decision-tree#entropy","position":8},{"hierarchy":{"lvl1":"Decision Tree","lvl3":"Entropy","lvl2":"Impurity Functions"},"content":"Define the impurity as how close we are to uniform. Use KL-Divergence to describe “closeness” from p to q. (Note: KL-Divergence is not a metric because it is not symmetric, i.e. KL(p||q)\\neq KL(q||p))\\begin{align}\nKL(p||q)=\\sum_{k=1}^{c}p_klog\\frac{p_k}{q_k}\n\\end{align}\n\nIn our case, p is our distribution and let q_1,\\dots,q_c be the uniform label/distribution. i.e. \\forall k, \\; q_k=\\frac{1}{c}. We will calculate the KL divergence between p and q.\\begin{align}\nKL(p||q)\n= &\\sum_{k=1}^{c}p_klog\\frac{p_k}{q_k}\\\\\n= &\\sum_{k}p_klog(p_k)-p_klog(q_k) &q_k=\\frac{1}{c}\\\\\n= &\\sum_{k}p_klog(p_k)+p_klog(c)\\\\\n= &\\sum_{k}p_klog(p_k)+log(c)\\sum_{k}p_k\n\\end{align}\n\nIf we are uniform (our least wanted case), \\log \\frac {p_k} {q_k} = \\log 1 = 0, so KL = 0. From this, we see that at the end we want to maximize the distance KL.\n\nSince log(c) is constant and \\sum_{k}p_k=1,\\begin{align}\n\\max_{p}KL(p||q)\n= &\\max_{p}\\sum_{k}p_klog(p_k) \\\\\n= &\\min_{p}-\\sum_{k}p_klog(p_k) \\\\\n= &\\min_{p}H(s)\n\\end{align}\n\nDefine entropy of set S is H(s) = -\\sum_{k}p_klog(p_k). We have therefore translated the problem of maximizing distance to minimizing entropy.\n\nEntropy on a tree is defined the same as Gini impurity.H(S)=\\frac{|S^L|}{|S|} H(S^L) + \\frac{|S^R|}{|S|} H(S^R)","type":"content","url":"/2021-11-09-decision-tree#entropy","position":9},{"hierarchy":{"lvl1":"Decision Tree","lvl2":"ID3-Algorithm"},"type":"lvl2","url":"/2021-11-09-decision-tree#id3-algorithm","position":10},{"hierarchy":{"lvl1":"Decision Tree","lvl2":"ID3-Algorithm"},"content":"","type":"content","url":"/2021-11-09-decision-tree#id3-algorithm","position":11},{"hierarchy":{"lvl1":"Decision Tree","lvl3":"Base Cases","lvl2":"ID3-Algorithm"},"type":"lvl3","url":"/2021-11-09-decision-tree#base-cases","position":12},{"hierarchy":{"lvl1":"Decision Tree","lvl3":"Base Cases","lvl2":"ID3-Algorithm"},"content":"In two situations, we say that we have reached the base case:\n\nthere is no need to split: all the data points in a subset have the same label, so return that label\n\nthere is no way to split: there are no more attributes could be used to split the subset (all the data points have the same value across all entries), so return the most common label\\textrm{ID3}(S):\\left\\{ \\begin{array}{ll}\n\\textrm{if } \\exists \\bar{y}\\textrm{ s.t. }\\forall(x,y)\\in S, y=\\bar{y}\\Rightarrow \\textrm{return leaf } \\textrm{ with label } \\bar{ y}\\\\\n\\textrm{if } \\exists\\bar{x}\\textrm{ s.t. }\\forall(x,y)\\in S, x=\\bar{x}\\Rightarrow \\textrm{return leaf } \\textrm{ with mode}(y:(x,y)\\in S)\\textrm{ or mean (regression)}\\end{array} \\right.\n\nNote, we do not stop when if no split can improve impurity:\n\nWhy? Look at the following XOR example\n\n\n\nDecision trees are myopic: ID3 (and possibly all algorithms used to find decision trees) is a greedy algorithm and only looks at one feature at a time. In the example, the first split does NOT decrease the impurity measure, however, if combining two feature splits, we reduce the impurity measure to 0.","type":"content","url":"/2021-11-09-decision-tree#base-cases","position":13},{"hierarchy":{"lvl1":"Decision Tree","lvl3":"Recursion Step and Time Analysis","lvl2":"ID3-Algorithm"},"type":"lvl3","url":"/2021-11-09-decision-tree#recursion-step-and-time-analysis","position":14},{"hierarchy":{"lvl1":"Decision Tree","lvl3":"Recursion Step and Time Analysis","lvl2":"ID3-Algorithm"},"content":"At each recursion, we find the split that would minimize impurity based on these two parameters:\n\nf - the feature to use\n\nt - threshold (where to draw the line)\n\nFor a specific feature/dimension i, there are n-1 = O(n) possible places to draw the line (the same as putting dividers between balls). When we draw such a line at t, we are basically dividing the set into x_f\\le t and x_f \\gt t, so\\textrm{Define: }\\begin{bmatrix}\nS^L=\\left \\{ (x,y)\\in S: x_f\\leq t \\right \\}\\\\\nS^R=\\left \\{ (x,y)\\in S: x_f> t \\right \\}\n\\end{bmatrix}\n\nTo draw such a line based on dimension i, we first have to sort all data points by x_i - their value on this dimension. This sort takes O(n\\;\\log n) time and we need test all O(n) possible line positions to determine where to draw the line. Therefore, it takes O(n\\;\\log n) time for each feature.\n\nFor all features and all corresponding places to draw the line, choose the division that minimizes impurity function. There are a total d features, so each final drawing takes O(dn\\;\\log n)","type":"content","url":"/2021-11-09-decision-tree#recursion-step-and-time-analysis","position":15},{"hierarchy":{"lvl1":"Decision Tree","lvl2":"Regression Trees - CART"},"type":"lvl2","url":"/2021-11-09-decision-tree#regression-trees-cart","position":16},{"hierarchy":{"lvl1":"Decision Tree","lvl2":"Regression Trees - CART"},"content":"CART stands for Classification and Regression Trees.\n\nAssume labels are continuous: y_i\\in\\mathbb{R}, we can just use squared loss as our impurity function, so\\begin{align}\nL(S)=\\frac{1}{|S|}\\sum_{(x,y)\\in S}(y-\\bar{y}_S)^2 \\leftarrow &\\textrm{Average squared difference from average label}\\\\\n&\\textrm{where }\\bar{y}_S=\\frac{1}{|S|}\\sum_{(x,y)\\in S}y\\leftarrow\\textrm{Average label}\n\\end{align}\n\nThis model has the following characteristics:\n\nCART are very light weight classifiers\n\nVery fast during testing\n\nUsually not competitive in accuracy but can become very strong through bagging (Random Forests) and boosting (Gradient Boosted Trees)","type":"content","url":"/2021-11-09-decision-tree#regression-trees-cart","position":17},{"hierarchy":{"lvl1":"Decision Tree","lvl2":"Parametric vs. Non-parametric algorithms"},"type":"lvl2","url":"/2021-11-09-decision-tree#parametric-vs-non-parametric-algorithms","position":18},{"hierarchy":{"lvl1":"Decision Tree","lvl2":"Parametric vs. Non-parametric algorithms"},"content":"So far we have introduced a variety of algorithms. One can categorize these into different families, such as generative vs. discriminative, or probabilistic vs. non-probabilistic. Here we will introduce another one: parametric vs. non-parametric.\n\nA parametric algorithm is one that has a constant set of parameters, which is independent of the number of training samples. You can think of it as the amount of much space you need to store the trained classifier. An examples for a parametric algorithm is the Perceptron algorithm, or logistic regression. Their parameters consist of \\mathbf{w},b, which define the separating hyperplane. The dimension of \\mathbf{w} depends on the dimension of the training data, but not on how many training samples you use for training.\n\nIn contrast, the number of parameters of a non-parametric algorithm scales as a function of the number of training samples. An example of a non-parametric algorithm is the k-Nearest Neighbors classifier. Here, during “training” we store the entire training data -- so the parameters that we learn are identical to the training set and the number of parameters (/the storage we require) grows linearly with the training set size.\n\nDecision Tree is an interesting edge case. If they are trained to full depth they are non-parametric, as the depth of a decision tree scales as a function of the training data (in practice O(\\log_2(n))). If we however limit the tree depth by a maximum value they become parametric (as an upper bound of the model size is now known prior to observing the training data).","type":"content","url":"/2021-11-09-decision-tree#parametric-vs-non-parametric-algorithms","position":19},{"hierarchy":{"lvl1":"Bagging"},"type":"lvl1","url":"/2021-11-16-bagging","position":0},{"hierarchy":{"lvl1":"Bagging"},"content":"Bagging is an ensemble method.","type":"content","url":"/2021-11-16-bagging","position":1},{"hierarchy":{"lvl1":"Bagging","lvl2":"Bagging Reduces Variance"},"type":"lvl2","url":"/2021-11-16-bagging#bagging-reduces-variance","position":2},{"hierarchy":{"lvl1":"Bagging","lvl2":"Bagging Reduces Variance"},"content":"Remember the Bias-Variance decomposition:\\underbrace{\\mathbb{E}[(h_D(x) - y)^2]}_\\mathrm{Error} = \\underbrace{\\mathbb{E}[(h_D(x)-\\bar{h}(x))^2]}_\\mathrm{Variance} + \\underbrace{\\mathbb{E}[(\\bar{h}(x)-\\bar{y}(x))^2]}_\\mathrm{Bias} + \\underbrace{\\mathbb{E}[(\\bar{y}(x)-y(x))^2]}_\\mathrm{Noise}\n\nOur goal is to reduce the variance term: \\mathbb{E}[(h_D(x)-\\bar{h}(x))^2]. For this, we want h_D \\to \\bar{h}.","type":"content","url":"/2021-11-16-bagging#bagging-reduces-variance","position":3},{"hierarchy":{"lvl1":"Bagging","lvl3":"Weak law of large numbers","lvl2":"Bagging Reduces Variance"},"type":"lvl3","url":"/2021-11-16-bagging#weak-law-of-large-numbers","position":4},{"hierarchy":{"lvl1":"Bagging","lvl3":"Weak law of large numbers","lvl2":"Bagging Reduces Variance"},"content":"The weak law of large numbers says: for i.i.d. random variables x_i with mean \\bar{x}, we have\\lim_{m \\to \\infty} \\frac{1}{m}\\sum_{i = 1}^{m}x_i = \\bar{x}\n\nApply this to classifiers: Assume we have m training sets D_1, D_2, …,  D_n drawn from P^n. Train a classifier on each one and average result:\\lim_{m \\to \\infty} \\hat{h} = \\frac{1}{m}\\sum_{i = 1}^m h_{D_i} = \\bar{h}\n\nWe refer to this average of multiple classifiers as an ensemble of classifiers. Good news: If \\hat{h}\\rightarrow \\bar{h} the variance component of the error must also vanish, i.e. \\mathbb{E}[(\\hat{h}(x)-\\bar{h}(x))^2]\\rightarrow 0. However, the problem is that we don’t have m data sets D_1, …., D_m . We only have a single D.","type":"content","url":"/2021-11-16-bagging#weak-law-of-large-numbers","position":5},{"hierarchy":{"lvl1":"Bagging","lvl3":"Solution: Bagging (Bootstrap Aggregating)","lvl2":"Bagging Reduces Variance"},"type":"lvl3","url":"/2021-11-16-bagging#solution-bagging-bootstrap-aggregating","position":6},{"hierarchy":{"lvl1":"Bagging","lvl3":"Solution: Bagging (Bootstrap Aggregating)","lvl2":"Bagging Reduces Variance"},"content":"We need to sample a total m dataset D_i, each of size n = |D|.\n\nTo do this, simulate drawing from P by drawing uniformly with replacement from the set D.\n\nMathematically, let Q(X,Y|D) be a probability distribution that picks a training sample (\\mathbf{x}_i,y_i) from D uniformly at random. i.e. \\forall (\\mathbf{x_i},  y_i)\\in D, \\; Q((\\mathbf{x_i}, y_i)|D) = \\frac{1}{n} with n=|D|. We sample the set D_i\\sim Q^n, with |D_i| =n, so D_i is picked with replacement from distribution Q.\n\nNotice we cannot use the W.L.L.N here: \\hat{h}_D = \\frac{1}{m}\\sum_{i = 1}^{m}h_{D_i}\\nrightarrow \\bar{h} because these data points are not drawn i.i.d. from the original distribution P, where h_D comes from. Even though they are drawn i.i.d from the distribution Q (which is conditioned on D.) However, in practice bagging still reduces variance very effectively.","type":"content","url":"/2021-11-16-bagging#solution-bagging-bootstrap-aggregating","position":7},{"hierarchy":{"lvl1":"Bagging","lvl4":"Analysis","lvl3":"Solution: Bagging (Bootstrap Aggregating)","lvl2":"Bagging Reduces Variance"},"type":"lvl4","url":"/2021-11-16-bagging#analysis","position":8},{"hierarchy":{"lvl1":"Bagging","lvl4":"Analysis","lvl3":"Solution: Bagging (Bootstrap Aggregating)","lvl2":"Bagging Reduces Variance"},"content":"Although we cannot prove that the new samples are i.i.d., we can show that they are drawn from the original distribution P, namely Q(X=x_i)=P(X=x_i). A proof can be found at the original lecture notes.","type":"content","url":"/2021-11-16-bagging#analysis","position":9},{"hierarchy":{"lvl1":"Bagging","lvl4":"Bagging summarized","lvl3":"Solution: Bagging (Bootstrap Aggregating)","lvl2":"Bagging Reduces Variance"},"type":"lvl4","url":"/2021-11-16-bagging#bagging-summarized","position":10},{"hierarchy":{"lvl1":"Bagging","lvl4":"Bagging summarized","lvl3":"Solution: Bagging (Bootstrap Aggregating)","lvl2":"Bagging Reduces Variance"},"content":"Sample m data sets D_1,\\dots,D_m from D with replacement.\n\nFor each D_j train a classifier h_j()\n\nThe final classifier is h(\\mathbf{x})=\\frac{1}{m}\\sum_{j=1}^m h_j(\\mathbf{x}).\n\nIn practice larger m results in a better ensemble, but at some point you will obtain diminishing returns. Note that setting m unnecessarily high will only slow down your classifier but will not increase the error of your classifier.","type":"content","url":"/2021-11-16-bagging#bagging-summarized","position":11},{"hierarchy":{"lvl1":"Bagging","lvl2":"Advantages of Bagging"},"type":"lvl2","url":"/2021-11-16-bagging#advantages-of-bagging","position":12},{"hierarchy":{"lvl1":"Bagging","lvl2":"Advantages of Bagging"},"content":"Easy to implement\n\nReduces variance, so has a strong beneficial effect on high variance classifiers.\n\nWe can obtain a mean score and variance as the prediction is an average of many classifiers. Variance can be interpreted as the uncertainty of the prediction. Especially in regression tasks, such uncertainties are otherwise hard to obtain. For example, imagine the prediction of a house price is $300,000. If a buyer wants to decide how much to offer, it would be very valuable to know if this prediction has standard deviation +-$10,000 or +-$50,000.\n\nBagging provides an unbiased estimate of the test error- the out-of-bag error. The idea is that for each training point, there are very likely to be some data sets D_k that did not pick this point. If we average the classifiers h_k of all such data sets, we obtain a classifier that was not trained on (\\mathbf{x}_i,y_i) ever. To these classifiers, this point is equivalent to a test sample. If we compute the error of all these points, we obtain an estimate of the true test error.\n\nMore formally, for each training point (\\mathbf{x}_i,y_i)\\in D let S_i=\\{k| (\\mathbf{x}_i,y_i)\\notin D_k\\} - in other words S_i - the set of all the training sets D_k that do not contain (\\mathbf{x}_i,y_i). Let the averaged classifier over all these data sets be\\tilde h_i(\\mathbf{x})=\\frac{1}{|S_i|}\\sum_{k\\in  S_i}h_k(\\mathbf{x}).\n\nThe-of-bag error is the average error/loss that all these classifiers yield\\epsilon_\\mathrm{OOB}=\\frac{1}{n}\\sum_{(\\mathbf{x}_i, y_i) \\in  D}l(\\tilde h_i(\\mathbf{x_i}),y_i).\n\nThis is an estimate of the test error, because for each training point we used the subset of classifiers that never saw that training point during training. if m is sufficiently large, the fact that we take out some classifiers has no significant effect and the estimate is pretty reliable.","type":"content","url":"/2021-11-16-bagging#advantages-of-bagging","position":13},{"hierarchy":{"lvl1":"Bagging","lvl2":"Random Forest"},"type":"lvl2","url":"/2021-11-16-bagging#random-forest","position":14},{"hierarchy":{"lvl1":"Bagging","lvl2":"Random Forest"},"content":"A Random Forest is essentially bagged decision trees, with a slightly modified splitting criteria - don’t use all features.\n\nThe algorithm works as follows:\n\nSample m data sets D_1,\\dots,D_m from D with replacement.\n\nFor each D_j train a full decision tree h_j (max-depth=\\infty) with one small modification: before each split randomly subsample k\\leq d features (without replacement) and only consider these features for your split. This further increases the variance of the trees. A good choice for k is k=\\sqrt{d}\n\nThe final classifier is h(\\mathbf{x})=\\frac{1}{m}\\sum_{j=1}^m h_j(\\mathbf{x}).\n\nThe randomness in random forest comes from the random sampling in step 1 and the random feature selection in step 2.\n\nThe Random Forest is one of the best and easiest to use model. There are two reasons:\n\nIt only has two hyper-parameters, m and k and is extremely insensitive to both of these. You can set m as large as you can afford and a popular choice of k is \\sqrt d\n\nDecision trees do not require a lot of preprocessing, so the features can be of different scale, magnitude, or slope. This is highly advantageous with heterogeneous data, for example with features like blood pressure, age, gender, ... each of which is recorded in completely different units.\n\nUseful variants of Random Forests:\n\nSplit each training set into two partitions D_l=D_l^A\\cup D_l^B. Build the tree on D_l^A and estimate the leaf labels on D_l^B. You must stop splitting if a leaf has only a single point in D_l^B in it. This has the advantage that each tree and also the RF classifier become \n\nconsistent.\n\nDo not grow each tree to its full depth, instead prune based on the leave out samples. This can further improve your bias/variance trade-off. (An easy way to prune decision trees is to start from the bottom and then to remove splits and check to see if it affects our error too much. If it doesn’t we remove that split.)","type":"content","url":"/2021-11-16-bagging#random-forest","position":15},{"hierarchy":{"lvl1":"Boosting"},"type":"lvl1","url":"/2021-11-18-boosting","position":0},{"hierarchy":{"lvl1":"Boosting"},"content":"","type":"content","url":"/2021-11-18-boosting","position":1},{"hierarchy":{"lvl1":"Boosting","lvl2":"Generic Boosting"},"type":"lvl2","url":"/2021-11-18-boosting#generic-boosting","position":2},{"hierarchy":{"lvl1":"Boosting","lvl2":"Generic Boosting"},"content":"","type":"content","url":"/2021-11-18-boosting#generic-boosting","position":3},{"hierarchy":{"lvl1":"Boosting","lvl3":"Boosting Reduces Bias","lvl2":"Generic Boosting"},"type":"lvl3","url":"/2021-11-18-boosting#boosting-reduces-bias","position":4},{"hierarchy":{"lvl1":"Boosting","lvl3":"Boosting Reduces Bias","lvl2":"Generic Boosting"},"content":"Scenario: Hypothesis class \\mathbb{H}, whose set of classifiers has large bias and the training error is high (e.g. CART trees with very limited depth.)\n\nFamous question: Can weak learners (h) be combined to generate a strong learner with low bias?\n\nAnswer: Yes!\n\nSolution: Create ensemble classifier H_T(\\vec x) = \\sum_{t =  1}^{T}\\alpha_t h_t(\\vec{x}). This ensemble classifier is built in an iterative fashion. In iteration t we add the classifier \\alpha_th_t(\\vec x) to the ensemble. At test time we evaluate all classifier and return the weighted sum.\n\nLet \\ell denote a (convex and differentiable) loss function.\\ell(H)=\\frac{1}{n}\\sum_{i=1}^n \\ell(H(\\mathbf{x}_i),y_i)\n\nAssume we have already finished t iterations and already have an ensemble classifier H_t(\\vec{x}). Now at iteration t+1 we add one more weak learner h_{t+1} that minimizes the loss:h_{t+1} = \\textrm{argmin}_{h \\in \\mathbb{H}} \\; \\ell(H_t + \\alpha h).\n\nSo H_{t+1} := H_t + \\alpha h","type":"content","url":"/2021-11-18-boosting#boosting-reduces-bias","position":5},{"hierarchy":{"lvl1":"Boosting","lvl3":"Finding h - Gradient Descent in Functional Space","lvl2":"Generic Boosting"},"type":"lvl3","url":"/2021-11-18-boosting#finding-h-gradient-descent-in-functional-space","position":6},{"hierarchy":{"lvl1":"Boosting","lvl3":"Finding h - Gradient Descent in Functional Space","lvl2":"Generic Boosting"},"content":"As before, we find minimum by doing gradient descent. However, instead of finding a parameter that minimizes the loss function, we find a function h this time. Therefore we will do gradient descent in function space.\n\nGiven H, we want to find the step-size \\alpha and the weak learner h to minimize the loss \\ell(H+\\alpha h). Use Taylor Approximation on \\ell(H+\\alpha h):\\ell(H+\\alpha h) \\approx \\ell(H) + \\alpha<\\nabla \\ell(H),h>\n\nThis approximation only holds within a small region around \\ell(H). We therefore fix \\alpha to a small constant (e.g. \\alpha\\approx 0.1). With the step-size \\alpha fixed, we can use the approximation above to find an almost optimal h:\n\n$$h = \\textrm{argmin}_{h\\in H}\\ell(H+\\alpha h) \\approx \\textrm{argmin}_{h\\in H}<\\nabla \\ell(H),h>\n\n$$\n\nIn function space, inner product is defined as < f,g >=\\int\\limits_x f(x)g(x)dx. This is intractable most of the time because x comes from an infinite space. Since we only have training set, we can rewrite the inner product as < f,g >= \\sum_{i = 1}^{n} f(\\mathbf{x}_i)g(\\mathbf{x}_i), where f is the gradient, g is model h.h = \\textrm{argmin}_{h \\in \\mathbb{H}}\\sum_{i = 1}^{n}\n\\frac{\\partial \\ell}{\\partial H}(\\mathbf{x}_i)\nh(\\mathbf{x}_i)\n\nNote \\frac{\\partial \\ell}{\\partial H} is the derivative of a function with respect to another function, which is tricky, so we need a work-around. \\ell(H) = \\sum_{i = 1}^{n}\\ell(H(\\mathbf{x}_i)), so we can write \\frac{\\partial \\ell}{\\partial H}(\\mathbf{x}_i) = \\frac{\\partial \\ell}{\\partial [H(\\mathbf{x}_i)]}, the derivative of the loss with respect to a specific function value. Now the optimization problem becomes:h_{t+1} = \\textrm{argmin}_{h \\in \\mathbb{H}} \\sum_{i = 1}^{n} \\underbrace{\\frac{\\partial \\ell}{\\partial [H(\\mathbf{x}_i)]}}_{r_i} h(\\mathbf x_i)\n\nIn order to make progress this h does not have to be great (reach a minimum). We still make progress as long as \\sum_{i = 1}^{n} r_i h(\\mathbf{x}_i)<0. That is because\\begin{align}\n\\ell(H_{t+1})\n&= \\ell(H_t+\\alpha h_{t+1}) \\\\\n&\\approx \\ell(H_t) + \\alpha<\\nabla \\ell(H_t),h_{t+1}>\\\\\n&= \\ell(H_t) + \\alpha \\sum_{i = 1}^{n} r_i h(\\mathbf{x}_i)\\\\\n&< \\ell(H_{t})\n\\end{align}\n\nIn this way, we decrease the loss by adding this new model h_{t+1}. However, if the this inner product is >0 there is nothing we can do with gradient descent.","type":"content","url":"/2021-11-18-boosting#finding-h-gradient-descent-in-functional-space","position":7},{"hierarchy":{"lvl1":"Boosting","lvl3":"Generic boosting (a.k.a Anyboost) in Pseudocode","lvl2":"Generic Boosting"},"type":"lvl3","url":"/2021-11-18-boosting#generic-boosting-a-k-a-anyboost-in-pseudocode","position":8},{"hierarchy":{"lvl1":"Boosting","lvl3":"Generic boosting (a.k.a Anyboost) in Pseudocode","lvl2":"Generic Boosting"},"content":"注意，下图只是把我们上一节描述的东西用伪代码写下来了而已，实际是完全一样的东西。","type":"content","url":"/2021-11-18-boosting#generic-boosting-a-k-a-anyboost-in-pseudocode","position":9},{"hierarchy":{"lvl1":"Boosting","lvl2":"Case study #1: Gradient Boosted Regression Tree(GBRT)"},"type":"lvl2","url":"/2021-11-18-boosting#case-study-1-gradient-boosted-regression-tree-gbrt","position":10},{"hierarchy":{"lvl1":"Boosting","lvl2":"Case study #1: Gradient Boosted Regression Tree(GBRT)"},"content":"","type":"content","url":"/2021-11-18-boosting#case-study-1-gradient-boosted-regression-tree-gbrt","position":11},{"hierarchy":{"lvl1":"Boosting","lvl3":"Background and Setting","lvl2":"Case study #1: Gradient Boosted Regression Tree(GBRT)"},"type":"lvl3","url":"/2021-11-18-boosting#background-and-setting","position":12},{"hierarchy":{"lvl1":"Boosting","lvl3":"Background and Setting","lvl2":"Case study #1: Gradient Boosted Regression Tree(GBRT)"},"content":"Classification ( y_i \\in \\{+1,-1\\} ) or regression (y_i\\in\\mathcal{R}^k)\n\nWeak learners,h \\in \\mathbb{H}, are not too deep fixed-depth (e.g. depth=4) regression trees .\n\nStep size \\alpha is fixed to a small constant (hyper-parameter).\n\nLoss function: Any differentiable convex loss that decomposes over the samples \\mathcal{L}(H)=\\sum_{i=1}^{n} \\ell(H(\\mathbf{x}_i))","type":"content","url":"/2021-11-18-boosting#background-and-setting","position":13},{"hierarchy":{"lvl1":"Boosting","lvl3":"Goal","lvl2":"Case study #1: Gradient Boosted Regression Tree(GBRT)"},"type":"lvl3","url":"/2021-11-18-boosting#goal","position":14},{"hierarchy":{"lvl1":"Boosting","lvl3":"Goal","lvl2":"Case study #1: Gradient Boosted Regression Tree(GBRT)"},"content":"We want to find a tree h that maximizes h = \\textrm{argmin}_{h \\in \\mathbb{H}} \\sum_{i = 1}^{n} r_i  h(\\mathbf{x}_i) where r_i = \\frac{\\partial \\ell}{\\partial H(\\mathbf{x}_i)}.","type":"content","url":"/2021-11-18-boosting#goal","position":15},{"hierarchy":{"lvl1":"Boosting","lvl3":"Assumptions","lvl2":"Case study #1: Gradient Boosted Regression Tree(GBRT)"},"type":"lvl3","url":"/2021-11-18-boosting#assumptions","position":16},{"hierarchy":{"lvl1":"Boosting","lvl3":"Assumptions","lvl2":"Case study #1: Gradient Boosted Regression Tree(GBRT)"},"content":"First, we assume that \\sum_{i = 1}^{n} h^2(\\mathbf{x}_i) = constant. So we are essentially fixing the vector h in \\sum_{i=1}^n  h(\\mathbf{x}_i)r_i to lie on a circle, and we are only concerned with its direction but not its length.\n\nDefine the negative gradient as t_i = -r_i = -\\frac{\\partial \\ell}{\\partial H(\\mathbf{x}_i)}.","type":"content","url":"/2021-11-18-boosting#assumptions","position":17},{"hierarchy":{"lvl1":"Boosting","lvl3":"Algorithm","lvl2":"Case study #1: Gradient Boosted Regression Tree(GBRT)"},"type":"lvl3","url":"/2021-11-18-boosting#algorithm","position":18},{"hierarchy":{"lvl1":"Boosting","lvl3":"Algorithm","lvl2":"Case study #1: Gradient Boosted Regression Tree(GBRT)"},"content":"\\begin{align}\nh &= \\textrm{argmin}_{h \\in \\mathbb{H}} \\sum_{i = 1}^{n} r_i  h(\\mathbf{x}_i) &&\\text{ original AnyBoost formulation} \\\\\n&= \\textrm{argmin}_{h \\in \\mathbb{H}}-2\\sum_{i = 1}^{n} t_i  h(\\mathbf{x}_i) &&\\text{Swapping in $t_i$ for $-r_i$ and multiply by constant 2} \\\\\n&= \\textrm{argmin}_{h \\in \\mathbb{H}} \\sum_{i = 1}^{n}  \\underbrace{t_i^2}_{\\textrm{constant}} - 2t_i h(\\mathbf{x}_i) +  \\underbrace{(h(\\mathbf{x}_i))^2}_{\\textrm{constant}} &&\\text{Adding  constant $\\sum_i t_i^2+h(\\mathbf{x}_i)^2$} \\\\\n&=\\textrm{argmin}_{h \\in \\mathbb{H}}\\sum_{i = 1}^{n}(h(\\mathbf{x}_i)-t_i)^2\n\\end{align}\n\nAt the second last step, note t_i^2 is the negation of loss function with respect to the ensemble H we already chose, independent of the model h we are choosing, so it is a constant. On the other hand, we constrained in the assumptions part that \\sum_{i = 1}^{n} h^2(\\mathbf{x}_i) is a constant.\n\nTherefore, we translate the original problem about finding a regression tree that minimizes an arbitrary loss function to this problem of finding a regression tree that minimizes the squared loss with our new “label” t and we are fitting t now.\n\nIf the loss function \\ell is the squared loss, i.e. \\ell(H)=\\frac{1}{2}\\sum_{i=1}^n (H(\\mathbf{x}_i)-y_i)^2, there is a special meaning of this t: it is easy to show that t_i=-\\frac{\\partial \\ell}{H(\\mathbf{x}_i)}=y_i-H(\\mathbf{x}_i),\n\n which is simply the error between our current prediction and the correct label, so this newly added model is just fitting the error current ensemble model has. However, it is important to keep in mind that you can use any other differentiable and convex loss function \\ell and the solution for your next weak learner h will always be the regression tree minimizing the squared loss.","type":"content","url":"/2021-11-18-boosting#algorithm","position":19},{"hierarchy":{"lvl1":"Boosting","lvl3":"GBRT in Pseudo Code","lvl2":"Case study #1: Gradient Boosted Regression Tree(GBRT)"},"type":"lvl3","url":"/2021-11-18-boosting#gbrt-in-pseudo-code","position":20},{"hierarchy":{"lvl1":"Boosting","lvl3":"GBRT in Pseudo Code","lvl2":"Case study #1: Gradient Boosted Regression Tree(GBRT)"},"content":"","type":"content","url":"/2021-11-18-boosting#gbrt-in-pseudo-code","position":21},{"hierarchy":{"lvl1":"Boosting","lvl2":"Case Study #2: AdaBoost"},"type":"lvl2","url":"/2021-11-18-boosting#case-study-2-adaboost","position":22},{"hierarchy":{"lvl1":"Boosting","lvl2":"Case Study #2: AdaBoost"},"content":"Setting: Classification ( y_i \\in \\{+1,-1\\} )\n\nStep-size: We perform line-search to obtain best step-size \\alpha.\n\nLoss function: Exponential loss \\ell(H)=\\sum_{i=1}^{n} e^{-y_i H(\\mathbf{x}_i)}\n\nAll data points must have the correct label, since we are using the exponential loss here and will get a very bad result if concentrate all our weights on noise (wrongly labeled data).","type":"content","url":"/2021-11-18-boosting#case-study-2-adaboost","position":23},{"hierarchy":{"lvl1":"Boosting","lvl3":"Finding the best weak learner","lvl2":"Case Study #2: AdaBoost"},"type":"lvl3","url":"/2021-11-18-boosting#finding-the-best-weak-learner","position":24},{"hierarchy":{"lvl1":"Boosting","lvl3":"Finding the best weak learner","lvl2":"Case Study #2: AdaBoost"},"content":"At each update step, first we compute the gradient r_i=\\frac{\\partial \\ell}{\\partial H(\\mathbf{x}_i)}=-y_i {e^{-y_i H(\\mathbf{x}_i)}}. For notational convenience, define w_i= \\frac{1}{Z}e^{-y_iH(\\mathbf{x}_i)}, where Z=\\sum_{i=1}^{n} e^{-y_iH(\\mathbf{x}_i)} is a normalizing factor so that \\sum_{i=1}^{n} w_i=1. Note that the normalizing constant Z is identical to the loss function. Each weight w_i can now be interpreted as the relative contribution of the training point (\\mathbf{x}_i,y_i) towards the overall loss. Therefore, at each update step, we reweight the data points according to current loss and in later part of the algorithm will prioritize those points we got really wrong.\n\nWe can translate the optimization problem of minimizing loss into a optimization probelm of minimizing the weighted classification error.\\begin{align}\nh(\\mathbf{x}_i)&=\\textrm{argmin}_{h \\in \\mathbb{H}}\\sum_{i=1}^{n}r_ih(\\mathbf{x}_i)\n&& \\Big(\\textrm{substitute in: } r_i=e^{-H(\\mathbf{x}_i)y_i}\\Big)\\\\\n&=\\textrm{argmin}_{h \\in \\mathbb{H}}-\\sum_{i=1}^n y_i e^{-H(\\mathbf{x}_i)y_i}h(\\mathbf{x}_i)\n&& \\Big(\\textrm{substitute in: } w_i=\\frac{1}{Z}e^{-H(\\mathbf{x}_i)y_i}, \\textrm{Z is constant}\\Big)\\\\\n&=\\textrm{argmin}_{h \\in \\mathbb{H}}-\\sum_{i=1}^{n} w_i y_i h(\\mathbf{x}_i)\n&& \\Big(y_ih(\\mathbf{x}_i)\\in \\{+1,-1\\} \\textrm{ with } h(\\mathbf{x}_i)y_i=1 \\iff h(\\mathbf{x}_i)=y_i \\Big)\\\\\n&=\\textrm{argmin}_{h \\in \\mathbb{H}}\\sum_{i: h(\\mathbf{x}_i)\\neq y_i} w_i - \\sum_{i: h(\\mathbf{x}_i)= y_i} w_i\n&& \\Big(\\sum_{i: h(\\mathbf{x}_i)= y_i} w_i + \\sum_{i: h(\\mathbf{x}_i)\\neq y_i} w_i= 1\\Big)\\\\\n&=\\textrm{argmin}_{h \\in \\mathbb{H}}\\sum_{i: h(\\mathbf{x}_i)\\neq y_i} w_i\n&& \\Big(\\textrm{This is the weighted classification error.}\\Big)\n\\end{align}\n\nLet us denote this weighted classification error as \\epsilon=\\sum_{i:h(\\mathbf{x}_i)y_i=-1} w_i, or to say the weights of those wrongly classified points. So for AdaBoost, we only need a classifier that reduces this weighted classification error of these wrongly labeled training samples. It doesn’t have to do all that well, in order for the inner-product \\sum_i r_i  h(\\mathbf{x}_i) to be negative, it just needs less than \\epsilon<0.5 weighted training error.","type":"content","url":"/2021-11-18-boosting#finding-the-best-weak-learner","position":25},{"hierarchy":{"lvl1":"Boosting","lvl3":"Finding the stepsize \\alpha","lvl2":"Case Study #2: AdaBoost"},"type":"lvl3","url":"/2021-11-18-boosting#finding-the-stepsize-alpha","position":26},{"hierarchy":{"lvl1":"Boosting","lvl3":"Finding the stepsize \\alpha","lvl2":"Case Study #2: AdaBoost"},"content":"In the previous example, GBRT, we set the stepsize \\alpha to be a small constant. As it turns out, in the AdaBoost setting we can find the optimal stepsize (i.e. the one that minimizes \\ell the most) in closed form every time we take a “gradient” step.\n\nIf we take the derivative of loss function \\ell with respect to \\alpha, we will actually find out that there is a nice closed form for \\alpha:\\alpha = \\frac{1}{2}\\ln \\frac{1-\\epsilon}{\\epsilon}\n\nIt is unusual that we can find the optimal step-size in such a simple closed form. One consequence is that AdaBoost converges extremely fast.","type":"content","url":"/2021-11-18-boosting#finding-the-stepsize-alpha","position":27},{"hierarchy":{"lvl1":"Boosting","lvl3":"Re-normalization","lvl2":"Case Study #2: AdaBoost"},"type":"lvl3","url":"/2021-11-18-boosting#re-normalization","position":28},{"hierarchy":{"lvl1":"Boosting","lvl3":"Re-normalization","lvl2":"Case Study #2: AdaBoost"},"content":"After you take a step, i.e. H_{t+1}=H_{t}+\\alpha h, you need to re-compute all the weights and then re-normalize. However, if we update w using the formula below, w will remain normalized.{w}_i\\leftarrow w_i\\frac{e^{-\\alpha  h(\\mathbf{x}_i)y_i}}{2\\sqrt{\\epsilon(1-\\epsilon)}}","type":"content","url":"/2021-11-18-boosting#re-normalization","position":29},{"hierarchy":{"lvl1":"Boosting","lvl3":"AdaBoost Pseudo-code","lvl2":"Case Study #2: AdaBoost"},"type":"lvl3","url":"/2021-11-18-boosting#adaboost-pseudo-code","position":30},{"hierarchy":{"lvl1":"Boosting","lvl3":"AdaBoost Pseudo-code","lvl2":"Case Study #2: AdaBoost"},"content":"\n\nThe inner loop can terminate as the error \\epsilon=\\frac{1}{2}, and in most cases it will converge to \\frac{1}{2} over time. In that case the latest weak learner h is only as good as a coin toss and cannot benefit the ensemble (therefore boosting terminates). Also note that if \\epsilon=\\frac{1}{2} the step-size \\alpha would be zero.","type":"content","url":"/2021-11-18-boosting#adaboost-pseudo-code","position":31},{"hierarchy":{"lvl1":"Boosting","lvl3":"Further analysis","lvl2":"Case Study #2: AdaBoost"},"type":"lvl3","url":"/2021-11-18-boosting#further-analysis","position":32},{"hierarchy":{"lvl1":"Boosting","lvl3":"Further analysis","lvl2":"Case Study #2: AdaBoost"},"content":"We can in fact show that the training loss is decreasing exponentially! Even further, we can show that after O(\\log(n)) iterations your training error must be zero. In practice it often makes sense to keep boosting even after you make no more mistakes on the training set.","type":"content","url":"/2021-11-18-boosting#further-analysis","position":33},{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/2022-08-22-introduction","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"Three principles that allow us to scale Machine Learning:\n\nOptimization: Write learning task as an optimization problem and solve it with fast and canned gradient-based algorithm using linear algebra.\n\nRecall the perceptron: it was a prediction model + a specific learning algorithm to this model. Having a different learning algorithm for each model isn’t what we want. Optimization allows generalization.\n\nStatistics: To process a large dataset, we can just process a small random subsample instead.\n\nStochastic Gradient Descent: use a subset of loss to do GD\n\ncross-validation / train-validation-test split: use a subsample of whole dataset to represent the whole\n\nHardware: use algorithms that fit your hardware and use hardware that fits your algorithm\n\nrequest memory in a power of 2\n\nbuild TPU to accelerate computation","type":"content","url":"/2022-08-22-introduction","position":1},{"hierarchy":{"lvl1":"Linear Algebra and NumPy"},"type":"lvl1","url":"/2022-08-24-linear-algebra-and-numpy","position":0},{"hierarchy":{"lvl1":"Linear Algebra and NumPy"},"content":"In the following discussion, the vector space is of dimension d and has basis x_1, x_2, \\dots, x_d","type":"content","url":"/2022-08-24-linear-algebra-and-numpy","position":1},{"hierarchy":{"lvl1":"Linear Algebra and NumPy","lvl2":"Vector Space"},"type":"lvl2","url":"/2022-08-24-linear-algebra-and-numpy#vector-space","position":2},{"hierarchy":{"lvl1":"Linear Algebra and NumPy","lvl2":"Vector Space"},"content":"a vector space over the real numbers is a set V together with two binary operations + (mapping V \\times V to V) and \\cdot (mapping \\R \\times V to V) satisfying the following axioms for any x, y, z \\in V and a, b \\in \\R\n\nclosure: x + y \\in V and a \\cdot x = ax \\in V\n\nassociativity of addition: (x + y) + z = x + (y + z)\n\ntransitivity of addition: x + y = y + x\n\nnegation: there exists a (-x) such that x + (-x) = 0\n\nzero element: 0 \\in V such that 0 + x = x + 0 = x\n\nassociativity of scalar multiplication: a(bx) = b(ax) = (ab)x\n\nmultiplication by one: 1v = v\n\ndistributivity: a(x+y) = ax + ay and (a+b)x = ax + bx","type":"content","url":"/2022-08-24-linear-algebra-and-numpy#vector-space","position":3},{"hierarchy":{"lvl1":"Linear Algebra and NumPy","lvl2":"Vector"},"type":"lvl2","url":"/2022-08-24-linear-algebra-and-numpy#vector","position":4},{"hierarchy":{"lvl1":"Linear Algebra and NumPy","lvl2":"Vector"},"content":"Any vector v in the vector space can be written uniquely as the following for some real numbers \\alpha_1, \\alpha_2, \\ldotsv = \\alpha_1 x_1 + \\alpha_2 x_2 + \\cdots + \\alpha_d x_d\n\nTherefore, to represent v on a computer, it suffices to store \\alpha_1, \\alpha_2, \\ldots, and \\alpha_d. We store these alphas in an array and this gets us back to our CS-style notion of what a vector is.","type":"content","url":"/2022-08-24-linear-algebra-and-numpy#vector","position":5},{"hierarchy":{"lvl1":"Linear Algebra and NumPy","lvl2":"Linear Map"},"type":"lvl2","url":"/2022-08-24-linear-algebra-and-numpy#linear-map","position":6},{"hierarchy":{"lvl1":"Linear Algebra and NumPy","lvl2":"Linear Map"},"content":"We say a function F from a vector space U to a vector space V is a linear map if for any x, y \\in U and any a \\in \\R,F(ax + y) = a F(x) + F(y)\n\nNote if we want to define a linear map, it suffices to just write out F(x_i) for all basis. Say |x_i| = d =m, |F(x_i)| = n, we can then represent a linear map with m\\times n number: each F(x_i) needs n numbers and there are m F(x_i)","type":"content","url":"/2022-08-24-linear-algebra-and-numpy#linear-map","position":7},{"hierarchy":{"lvl1":"Linear Algebra and NumPy","lvl2":"Matrix"},"type":"lvl2","url":"/2022-08-24-linear-algebra-and-numpy#matrix","position":8},{"hierarchy":{"lvl1":"Linear Algebra and NumPy","lvl2":"Matrix"},"content":"We have basically introduced the “matrix” in above. We call this two-dimensional-array representation of a linear map a matrix.\n\nWe use multiplication to denote the effect of a matrix operating on a vector (this is equivalent to applying a multilinear map as a function). E.g. if F is the multilinear map corresponding to matrix A:y = F(x) \\iff y = Ax \\\\\nF:\\R^m \\to \\R^n \\iff A \\in \\R^{n \\times m}\n\nWe can add two matrices, scale a matrix by a scalar, and these two operations follow the “axioms” described above for a vector space. This means that the set of matrices \\R^{n \\times m} is itself a vector space. (This was covered in MATH3360)","type":"content","url":"/2022-08-24-linear-algebra-and-numpy#matrix","position":9},{"hierarchy":{"lvl1":"Linear Algebra and NumPy","lvl2":"Broadcasting"},"type":"lvl2","url":"/2022-08-24-linear-algebra-and-numpy#broadcasting","position":10},{"hierarchy":{"lvl1":"Linear Algebra and NumPy","lvl2":"Broadcasting"},"content":"When operate on vectors / matrices, numpy broadcasts by acting as if there are infinite 1\\times to the left of dimension:if dimension of array not same:\n    append newaxis to the lesser one until same\n    if on a specific axis, one array has dim 1 and the other has > 1:\n        broadcast that 1 axis to match dimension\n    else:\n        fail\n\nHowever, it is good practice for you to always manually add dimension before broadcasting.x = numpy.array([2,3])\nx1 = x[numpy.newaxis,:]","type":"content","url":"/2022-08-24-linear-algebra-and-numpy#broadcasting","position":11},{"hierarchy":{"lvl1":"Automatic Differentiation"},"type":"lvl1","url":"/2022-08-29-automatic-differentiation","position":0},{"hierarchy":{"lvl1":"Automatic Differentiation"},"content":"","type":"content","url":"/2022-08-29-automatic-differentiation","position":1},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl2":"Derivative"},"type":"lvl2","url":"/2022-08-29-automatic-differentiation#derivative","position":2},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl2":"Derivative"},"content":"","type":"content","url":"/2022-08-29-automatic-differentiation#derivative","position":3},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl3":"Gradients","lvl2":"Derivative"},"type":"lvl3","url":"/2022-08-29-automatic-differentiation#gradients","position":4},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl3":"Gradients","lvl2":"Derivative"},"content":"Suppose I have a function f from \\R^d to \\R. The gradient \\nabla f is is the vector of partial derivatives of the function. Mathematically, it is a function from \\R^d to \\R^d such that\\left(\\nabla f(w) \\right)_i = \\frac{\\partial}{\\partial w_i} f(w) = \\lim_{\\delta \\rightarrow 0} \\frac{f(w + \\delta e_i) - f(w)}{\\delta}\n\nAnother equivalent definition is that \\nabla f(w)^T is the linear map such that for any u \\in \\R^d\\nabla f(w)^T u = \\lim_{\\delta \\rightarrow 0} \\frac{f(w + \\delta u) - f(w)}{\\delta}\n\nMore informally, we say it uniquely defines w nearby w_0 in the following way:f(w) \\approx f(w_0) + (w - w_0)^T \\nabla f(w_0)","type":"content","url":"/2022-08-29-automatic-differentiation#gradients","position":5},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl3":"Gradient Operator","lvl2":"Derivative"},"type":"lvl3","url":"/2022-08-29-automatic-differentiation#gradient-operator","position":6},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl3":"Gradient Operator","lvl2":"Derivative"},"content":"Here we introduce something much more general.\n\nFor a function F from one vector space U to another vector space V, the derivative of F is a function DF which maps U to \\mathcal{L}(U,V), where \\mathcal{L}(U,V) denotes the set of linear maps from U to V. This means DF takes in an element x \\in U and returns the derivative of F at this point x. This derivative function DF(x) will take in another directional vector \\Delta \\in U and outputs the directional derivative of F at point x in direction \\Delta.\n\nThe derivative is defined as the unique function such that for any x and \\Delta in U,\\lim_{\\alpha \\rightarrow 0} \\frac{F(x + \\alpha \\Delta) - F(x)}{\\alpha} = (DF(x)) \\Delta\n\nAs a special case, note for a function \\mathbb R^n \\to \\mathbb R, we can always write it in the form of b^Tx. Therefore, if F: \\mathbb R^n \\to \\mathbb R, we can always write DF(x)\\Delta in the form of b^Tx.","type":"content","url":"/2022-08-29-automatic-differentiation#gradient-operator","position":7},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl2":"Symbolic differentiation"},"type":"lvl2","url":"/2022-08-29-automatic-differentiation#symbolic-differentiation","position":8},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl2":"Symbolic differentiation"},"content":"Write your function as a single mathematical expression.\n\nApply the chain rule, product rule, ..., to differentiate that expression.\n\nExecute the expression as code.","type":"content","url":"/2022-08-29-automatic-differentiation#symbolic-differentiation","position":9},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl3":"Problems","lvl2":"Symbolic differentiation"},"type":"lvl3","url":"/2022-08-29-automatic-differentiation#problems","position":10},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl3":"Problems","lvl2":"Symbolic differentiation"},"content":"Converting code into a mathematical expression is not trivial. We need humans to do it.\n\nThe differentiation can be very large and complicated, especially when it gets to chain rules.","type":"content","url":"/2022-08-29-automatic-differentiation#problems","position":11},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl2":"Numerical Differentiation"},"type":"lvl2","url":"/2022-08-29-automatic-differentiation#numerical-differentiation","position":12},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl2":"Numerical Differentiation"},"content":"Just take a small enough amount (like 1e-8) and use it as the infinitely small value \\epsilon","type":"content","url":"/2022-08-29-automatic-differentiation#numerical-differentiation","position":13},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl3":"Problems","lvl2":"Numerical Differentiation"},"type":"lvl3","url":"/2022-08-29-automatic-differentiation#problems-1","position":14},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl3":"Problems","lvl2":"Numerical Differentiation"},"content":"suffers from numerical imprecision\n\ncan have problems if the function we’re differentiating is not smooth\n\nwe aren’t sure what values to use for \\epsilon","type":"content","url":"/2022-08-29-automatic-differentiation#problems-1","position":15},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl2":"Automatic Differentiation (Forward Mode)"},"type":"lvl2","url":"/2022-08-29-automatic-differentiation#automatic-differentiation-forward-mode","position":16},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl2":"Automatic Differentiation (Forward Mode)"},"content":"Automatic Differentiation allows us to compute derivatives automatically without any overheads or loss of precision. There are two rough classes of methods: forward mode and reverse mode. We introduce the forward mode here.\n\nIt fixes one input variable x over \\mathbb{R}. At each step of the computation, as we’re computing some value y, also compute \\frac{\\partial y}{\\partial x}. We can do this with a dual numbers approach: each number y is replaced with a pair (y, \\frac{\\partial y}{\\partial x}).","type":"content","url":"/2022-08-29-automatic-differentiation#automatic-differentiation-forward-mode","position":17},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl3":"Demo","lvl2":"Automatic Differentiation (Forward Mode)"},"type":"lvl3","url":"/2022-08-29-automatic-differentiation#demo","position":18},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl3":"Demo","lvl2":"Automatic Differentiation (Forward Mode)"},"content":"def to_dualnumber(x):\n    if isinstance(x, DualNumber):\n        return x\n    elif isinstance(x, float):\n        return DualNumber(x)\n    elif isinstance(x, int):\n        return DualNumber(float(x))\n    else:\n        raise Exception(\"couldn't convert {} to a dual number\".format(x))\n\nclass DualNumber(object):\n    def __init__(self, y, dydx=0.0):\n        super().__init__()\n        self.y = y\n        self.dydx = dydx\n        \n    def __repr__(self):\n        return \"(y = {}, dydx = {})\".format(self.y, self.dydx)\n\n    # operator overloading\n    def __add__(self, other):\n        other = to_dualnumber(other)\n        return DualNumber(self.y + other.y, self.dydx + other.dydx)\n    def __sub__(self, other):\n        other = to_dualnumber(other)\n        return DualNumber(self.y - other.y, self.dydx - other.dydx)\n    def __mul__(self, other):\n        other = to_dualnumber(other)\n        return DualNumber(self.y * other.y, self.dydx * other.y + self.y * other.dydx)\n    def __truediv__(self, other):\n        return DualNumber(self.y / other.y, self.dydx / other.y - self.y * other.dydx / (other.y * other.y))\n    \n    def __radd__(self, other):\n        return to_dualnumber(other).__add__(self)\n    def __rsub__(self, other):\n        return to_dualnumber(other).__sub__(self)\n    def __rmul__(self, other):\n        return to_dualnumber(other).__mul__(self)\n    def __rtruediv__(self, other):\n        return to_dualnumber(other).__truediv__(self)\n    \ndef forward_mode_diff(f, xv):\n    \"\"\"\n    It computes the df/dx at x=xv\n    f is a function that may use +,-,*,/ or other operators we have overloaded\n    xv is where we want to calculate the derivative\n    \"\"\"\n\t# x is a variable that has value xv; dx/dx = 1.0\n    x = DualNumber(xv, 1.0)\n    # f(x) is a DualNumber, because x is a DualNumber and x goes through a bunch of operations like + or * in f, which are overloaded for DualNumber. x goes through these DualNumber specified operations so the result is also a DualNumber from which we can obtain the derivative directly. \n    return f(x).dydxdef f(x):\n    return 2*x*x - 1\ndef dfdx(x):\n    return 4*x\ndef numerical_derivative(f, x, eps = 1e-5):\n    return (f(x+eps) - f(x-eps))/(2*eps)\n\nprint(dfdx(3.0)) # 12.0\nprint(numerical_derivative(f, 3.0)) # 12.000000000078613\nprint(forward_mode_diff(f, 3.0)) # 12.0\n","type":"content","url":"/2022-08-29-automatic-differentiation#demo","position":19},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl3":"Benefits","lvl2":"Automatic Differentiation (Forward Mode)"},"type":"lvl3","url":"/2022-08-29-automatic-differentiation#benefits","position":20},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl3":"Benefits","lvl2":"Automatic Differentiation (Forward Mode)"},"content":"Simple in-place operations, Easy to extend to compute higher-order derivatives","type":"content","url":"/2022-08-29-automatic-differentiation#benefits","position":21},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl3":"Problem","lvl2":"Automatic Differentiation (Forward Mode)"},"type":"lvl3","url":"/2022-08-29-automatic-differentiation#problem","position":22},{"hierarchy":{"lvl1":"Automatic Differentiation","lvl3":"Problem","lvl2":"Automatic Differentiation (Forward Mode)"},"content":"We can only differentiate with respect to one scalar input. It can get a bit complicated if we are given a vector.","type":"content","url":"/2022-08-29-automatic-differentiation#problem","position":23},{"hierarchy":{"lvl1":"Back Propagation"},"type":"lvl1","url":"/2022-08-31-back-propagation","position":0},{"hierarchy":{"lvl1":"Back Propagation"},"content":"","type":"content","url":"/2022-08-31-back-propagation","position":1},{"hierarchy":{"lvl1":"Back Propagation","lvl2":"Reverse Mode Automatic Differentiation"},"type":"lvl2","url":"/2022-08-31-back-propagation#reverse-mode-automatic-differentiation","position":2},{"hierarchy":{"lvl1":"Back Propagation","lvl2":"Reverse Mode Automatic Differentiation"},"content":"We need to fix an output in the NN. For most cases, we just use the final output \\ell of the NN (\\ell stands for loss). For each intermediate output value y, we want to compute partial derivatives \\frac{\\partial \\ell}{\\partial y}.\n\nSame as before: we replace each number/array y with a pair, except this time the pair is not (y, \\frac{\\partial y}{\\partial x}) but instead (y, \\nabla_y \\ell) where \\ell is our final output (loss of NN). Observe that \\nabla_y \\ell always has the same shape as y.","type":"content","url":"/2022-08-31-back-propagation#reverse-mode-automatic-differentiation","position":3},{"hierarchy":{"lvl1":"Back Propagation","lvl2":"Deriving Backprop from the Chain Rule"},"type":"lvl2","url":"/2022-08-31-back-propagation#deriving-backprop-from-the-chain-rule","position":4},{"hierarchy":{"lvl1":"Back Propagation","lvl2":"Deriving Backprop from the Chain Rule"},"content":"","type":"content","url":"/2022-08-31-back-propagation#deriving-backprop-from-the-chain-rule","position":5},{"hierarchy":{"lvl1":"Back Propagation","lvl3":"Single Variable","lvl2":"Deriving Backprop from the Chain Rule"},"type":"lvl3","url":"/2022-08-31-back-propagation#single-variable","position":6},{"hierarchy":{"lvl1":"Back Propagation","lvl3":"Single Variable","lvl2":"Deriving Backprop from the Chain Rule"},"content":"Suppose that the output \\ell can be written as\\ell = f(u), \\; u = g(y)\n\nHere, g represents the immediate next operation that takes y (producing a new intermediate u), and f represents all the rest of the computation between u and \\ell. By the chain rule,\\begin{align}\n\\frac{\\partial \\ell}{\\partial y}\n&= \\frac{\\partial \\ell}{\\partial u} \\frac{\\partial u}{\\partial y} \\\\\n&= g'(y) \\cdot \\frac{\\partial \\ell}{\\partial u}\n\\end{align}\n\nTherefore, to compute \\frac{\\partial \\ell}{\\partial y}, in addition to \\frac{\\partial \\ell}{\\partial u} - the derivative of all the other intermediate computation, we only need to use “local” information g'(y) that’s available in the program around where y is computed and used. That is, if ignore \\frac{\\partial \\ell}{\\partial u}, we can compute \\frac{\\partial \\ell}{\\partial y} together when we compute y.","type":"content","url":"/2022-08-31-back-propagation#single-variable","position":7},{"hierarchy":{"lvl1":"Back Propagation","lvl3":"Multi Variable","lvl2":"Deriving Backprop from the Chain Rule"},"type":"lvl3","url":"/2022-08-31-back-propagation#multi-variable","position":8},{"hierarchy":{"lvl1":"Back Propagation","lvl3":"Multi Variable","lvl2":"Deriving Backprop from the Chain Rule"},"content":"More generally, suppose that the output \\ell can be written as\\ell = F(u_1, u_2, \\ldots, u_k), \\; u_i = g_i(y)\n\nHere, the g_1, g_2, \\ldots, g_k represent the immediate next operations that take y; the u_i are the output by these operations, and F represents all the rest of the computation between u and \\ell. By the multivariate chain rule,\\begin{align}\n\\frac{\\partial \\ell}{\\partial y} &= \\sum_{i=1}^k  \\frac{\\partial \\ell}{\\partial u_i} \\frac{\\partial u_i}{\\partial y}\\\\\n\\nabla_y \\ell &= \\sum_{i=1}^k D g_i(y)^T \\nabla_{u_i} \\ell\n\\end{align}\n\nNote here we are talking about the more general case, where \\ell, u, y are all vectors. So g_i takes in a vector y and outputs a vector u_i When we take the derivative of g_i at y, we get a matrix and we call this matrix D g_i(y)\n\nAgain, we see that we can compute \\nabla_y \\ell using only \\nabla_{u_i} \\ell and “local” information D g_i(y).","type":"content","url":"/2022-08-31-back-propagation#multi-variable","position":9},{"hierarchy":{"lvl1":"Back Propagation","lvl2":"Computing Backprop"},"type":"lvl2","url":"/2022-08-31-back-propagation#computing-backprop","position":10},{"hierarchy":{"lvl1":"Back Propagation","lvl2":"Computing Backprop"},"content":"","type":"content","url":"/2022-08-31-back-propagation#computing-backprop","position":11},{"hierarchy":{"lvl1":"Back Propagation","lvl3":"When to Compute?","lvl2":"Computing Backprop"},"type":"lvl3","url":"/2022-08-31-back-propagation#when-to-compute","position":12},{"hierarchy":{"lvl1":"Back Propagation","lvl3":"When to Compute?","lvl2":"Computing Backprop"},"content":"Note the \\ell value is computed as follows: y \\to u \\to \\ell So when we are computing y , which is also when we ideally want to compute \\frac{\\partial \\ell}{\\partial y}, the value of u is not yet available. Without u, we cannot compute \\frac{\\partial \\ell}{\\partial u}, which is a necessary part to get \\frac{\\partial \\ell}{\\partial y}. Therefore we cannot compute \\frac{\\partial \\ell}{\\partial y} along with y\n\nTherefore, we cannot compute \\frac{\\partial \\ell}{\\partial y} until after we’ve computed everything between y and \\ell. So the solution is to remember the order in which we computed stuff in between y and \\ell, and then compute their gradients in the reverse order.","type":"content","url":"/2022-08-31-back-propagation#when-to-compute","position":13},{"hierarchy":{"lvl1":"Back Propagation","lvl3":"Computational Graph","lvl2":"Computing Backprop"},"type":"lvl3","url":"/2022-08-31-back-propagation#computational-graph","position":14},{"hierarchy":{"lvl1":"Back Propagation","lvl3":"Computational Graph","lvl2":"Computing Backprop"},"content":"We draw a graph to indicate the computational dependencies, just like the y \\to u \\to \\ell above:\n\nnode represents an scalar/vector/tensor/array\n\nEdges represent dependencies, where y \\rightarrow u means that y was used to compute u","type":"content","url":"/2022-08-31-back-propagation#computational-graph","position":15},{"hierarchy":{"lvl1":"Back Propagation","lvl3":"Algorithm","lvl2":"Computing Backprop"},"type":"lvl3","url":"/2022-08-31-back-propagation#algorithm","position":16},{"hierarchy":{"lvl1":"Back Propagation","lvl3":"Algorithm","lvl2":"Computing Backprop"},"content":"Generally speaking,\n\nFor each tensor, initialize a gradient “accumulator” to 0. Set the gradient of \\ell with respect to itself to 1.\n\nFor each tensor y' that \\ell depends on, in the reverse order of computation, compute \\nabla_{y'} \\ell - derivative of \\ell with respect to y'. This is obtainable because when we get to y', all the intermediate value u between y' and \\ell are already computed.\n\nOnce this step is called for y, its gradient will be fully manifested in its gradient accumulator.\n\nIn a computational graph, this means\n\nFor each tensor, initialize a gradient “accumulator” to 0. Set the gradient of \\ell with respect to itself to 1.\n\nFor each tensor y' pointing to \\ell, start from \\ell and go back along the path, compute \\nabla_{y'} \\ell - derivative of \\ell with respect to y'. Specifically, we compute this value by looking at all its direct descendants u_i where \\nabla_{y'} \\ell = \\sum_{i=1}^k \\frac{\\partial \\ell}{\\partial u_i} \\frac{\\partial u_i}{\\partial y'} We can do this because u are the nodes between y' and \\ell, when we get to y', all u values are already computed.\n\nOnce this step is called for y, computation is done.","type":"content","url":"/2022-08-31-back-propagation#algorithm","position":17},{"hierarchy":{"lvl1":"Gradient Descent"},"type":"lvl1","url":"/2022-09-12-gradient-descent","position":0},{"hierarchy":{"lvl1":"Gradient Descent"},"content":"","type":"content","url":"/2022-09-12-gradient-descent","position":1},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"Optimization Methods"},"type":"lvl2","url":"/2022-09-12-gradient-descent#optimization-methods","position":2},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"Optimization Methods"},"content":"","type":"content","url":"/2022-09-12-gradient-descent#optimization-methods","position":3},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"Empirical Risk Minimization","lvl2":"Optimization Methods"},"type":"lvl3","url":"/2022-09-12-gradient-descent#empirical-risk-minimization","position":4},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"Empirical Risk Minimization","lvl2":"Optimization Methods"},"content":"For a model / hypothesis h and its associated parameters w \\in \\R^d, we can write out its loss in the following formf(w) = \\frac{1}{|\\mathcal{D}|} \\sum_{x \\in \\mathcal{D}} f(w; x)","type":"content","url":"/2022-09-12-gradient-descent#empirical-risk-minimization","position":5},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"Gradient Descent","lvl2":"Optimization Methods"},"type":"lvl3","url":"/2022-09-12-gradient-descent#gradient-descent","position":6},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"Gradient Descent","lvl2":"Optimization Methods"},"content":"With w_0 initialized to some value, we then update w in each step according to the following rulew_{t+1} = w_t - \\alpha_t \\cdot \\nabla f(w_t) = w_t - \\alpha_t \\cdot \\frac{1}{n} \\sum_{x \\in \\mathcal{D}} \\nabla f(w; x)\n\nGD takes O(nd) and O(d) memory.","type":"content","url":"/2022-09-12-gradient-descent#gradient-descent","position":7},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"Newton’s Method","lvl2":"Optimization Methods"},"type":"lvl3","url":"/2022-09-12-gradient-descent#newtons-method","position":8},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"Newton’s Method","lvl2":"Optimization Methods"},"content":"w_{t+1} = w_t -  H(w_t) \\nabla f(w_t).\n\nH(x) is the Hessian matrix, which is the second order derivatives matrix. Sometimes people directly write H(w_t) = \\left( \\nabla^2 f(w_t) \\right)^{-1} .\n\nNewton’s method takes O(nd^2 + d^3) time and O(d^2) memory. Despite the problem with existence of second order derivative, the enormous extra time we have to spend calculating Hessian matrix when d gets big is the main reason why people avoid Newton’s method.","type":"content","url":"/2022-09-12-gradient-descent#newtons-method","position":9},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"Gradient Descent Converges"},"type":"lvl2","url":"/2022-09-12-gradient-descent#gradient-descent-converges","position":10},{"hierarchy":{"lvl1":"Gradient Descent","lvl2":"Gradient Descent Converges"},"content":"","type":"content","url":"/2022-09-12-gradient-descent#gradient-descent-converges","position":11},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"L-Smooth","lvl2":"Gradient Descent Converges"},"type":"lvl3","url":"/2022-09-12-gradient-descent#l-smooth","position":12},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"L-Smooth","lvl2":"Gradient Descent Converges"},"content":"We can prove that gradient descent works under the assumption that the second derivative of the objective is bounded. Formally, such function is called L-smooth.\n\nWe call a function L-smooth when its gradient is L-Lipschitz continuous. That is\\| \\nabla f(w) - \\nabla f(u) \\|_2 \\le L \\| u-v \\|_2\n\nFrom the above definition, we can derive the following property, which guarantees that small change in weight only results in small change in gradient (the rate of gradient change is bounded by L) This is the intuition behind why we need L-smooth for GD to converge.\\forall \\mathbf w,\\mathbf u \\in \\R^d, \\alpha \\in \\R \\; s.t. \\|\\mathbf u\\|=1, \\; \\|\\frac{\\partial^2}{\\partial\\alpha^2} f(\\mathbf w+\\alpha \\mathbf u) \\| \\le L\n\nFor example, take f(x) = \\sin(x) as an example, we have |f''(x)| = |-\\sin(x)| \\le 1, so we call f is 1-smooth and f' is 1-Lipchitz continuous. Another example g(x) = x^3, so g'(x) = 3x^2, g''(x) = 6x and a small x change can result in arbitrary large gradient descent, so this is bad for gradient descent.","type":"content","url":"/2022-09-12-gradient-descent#l-smooth","position":13},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"Proof","lvl2":"Gradient Descent Converges"},"type":"lvl3","url":"/2022-09-12-gradient-descent#proof","position":14},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"Proof","lvl2":"Gradient Descent Converges"},"content":"Assume there exists a global minimum f^* \\; s.t. \\; \\forall w \\in \\R^d, \\; f(w) \\ge f^*\n\nFirst, we start with looking at how the loss changes from step t to step t+1.f(w_{t+1}) = f(w_t - \\alpha \\nabla f(w_t))\n\nBy Fundamental Theorem of Calculus: h(a) = h(0) + \\int^a_0 h'(t) dtf(w_{t+1}) = f(w_t) + \\int^\\alpha_0 \\frac{\\partial}{\\partial\\eta} f(w_t - \\eta \\nabla f(w_t)) \\; d\\eta\n\nWe rewrite this equation with \\hat w = w_t - \\eta \\nabla f(w_t) (refer to the appendix for a detailed derivation)f(w_{t+1}) = f(w_t) + \\int^\\alpha_0 -\\nabla f(\\hat w)^T \\nabla f(w_t) \\; d\\eta\n\nSubtract and add \\nabla f(w_t)^T \\nabla f(w_t)\\begin{align}\nf(w_{t+1}) &= f(w_t) + \\int^\\alpha_0 - \\nabla f(w_t)^T \\nabla f(w_t) +  \\nabla f(w_t)^T \\nabla f(w_t) - \\nabla f(\\hat w)^T \\nabla f(w_t) \\; d\\eta  \\\\\n&= f(w_t) + \\int^\\alpha_0 - \\nabla f(w_t)^T \\nabla f(w_t) +  (\\nabla f(w_t)^T - \\nabla f(\\hat w)^T)  \\nabla f(w_t) \\; d\\eta \\\\\n&= f(w_t) - \\int^\\alpha_0 \\nabla f(w_t)^T \\nabla f(w_t) \\; d\\eta\n + \\int^\\alpha_0  (\\nabla f(w_t) - \\nabla f(\\hat w))^T  \\nabla f(w_t) \\; d\\eta\n\\end{align}\n\nEvaluate the first integral and replace the second integral with the triangular inequalityf(w_{t+1}) \\le f(w_t) -\\alpha \\nabla f(w_t)^T \\nabla f(w_t) + \\int_{0}^{\\alpha} \\| \\nabla f(w_t) \\| \\cdot \\| \\nabla f(w_t) - \\nabla f(w_t - \\eta \\nabla f(w_t)) \\| \\; d \\eta \\\\\n\nSince our function is L-smooth,\\begin{align}\nf(w_{t+1}) &\\le f(w_t) - \\alpha \\nabla f(w_t)^T \\nabla f(w_t) + \\int_{0}^{\\alpha} \\| \\nabla f(w_t) \\| \\cdot L \\| w_t - w_t - \\eta \\nabla f(w_t) \\| \\; d \\eta \\\\\n&\\le f(w_t) - \\alpha \\nabla f(w_t)^T \\nabla f(w_t) + \\int_{0}^{\\alpha} \\| \\nabla f(w_t) \\| \\cdot L \\| \\eta \\nabla f(w_t) \\| \\; d \\eta \\\\\n&\\le f(w_t) - \\alpha \\| \\nabla f(w_t) \\|^2 + L \\| \\nabla f(w_t) \\|^2 \\int_{0}^{\\alpha} \\eta \\; d \\eta \\\\\n&\\le f(w_t) - \\alpha \\| \\nabla f(w_t) \\|^2 + \\frac{\\alpha^2 L}{2} \\| \\nabla f(w_t) \\|^2 \\\\\n&\\le f(w_t) - \\alpha \\left(1 - \\frac{\\alpha L}{2}  \\right) \\cdot \\| \\nabla f(w_t) \\|^2 \\\\\n\\end{align}\n\nNote Gradient Descent only converges when 1 - \\frac{\\alpha L}{2} \\gt 0, so the loss function is guaranteed to decrease at each step. We now make an assumption that \\alpha L \\le 1 and we can safely conclude that GD converges.","type":"content","url":"/2022-09-12-gradient-descent#proof","position":15},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"How fast it Converges & Where does it Go?","lvl2":"Gradient Descent Converges"},"type":"lvl3","url":"/2022-09-12-gradient-descent#how-fast-it-converges-where-does-it-go","position":16},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"How fast it Converges & Where does it Go?","lvl2":"Gradient Descent Converges"},"content":"From our assumption \\alpha L \\le 1 above, we can get rid of the \\frac{\\alpha L}{2} termf(w_{t+1}) \\le f(w_t) - \\frac{\\alpha}{2} \\| \\nabla f(w_t) \\|^2\n\nWe now move stuff a little bit and calculate the accumulated value when we run T iterations\\begin{align}\n\\frac{\\alpha}{2} \\| \\nabla f(w_t) \\|^2  &\\le f(w_t) - f(w_{t+1}) \\\\\n\\frac{\\alpha}{2} \\sum_{t=0}^{T-1} \\| \\nabla f(w_t) \\|^2 &\\le f(w_0) - f(w_{T})\n\\end{align}\n\nRecall we have a global minimum loss f^*. We can also find the minimum gradient norm \\| \\nabla f(w_t) \\|^2 when we run these T iterations.\\frac{\\alpha T}{2} \\min_{t \\in [0, T-1]} \\| \\nabla f(w_t) \\|^2\n\\le \\frac{\\alpha}{2} \\sum_{t=0}^{T-1} \\| \\nabla f(w_t) \\|^2\n\\le f(w_0) - f(w_{T})\n\\le f(w_0) - f^*\n\nWe now have this following equation. We started off looking at the change in loss from each iteration. Then we accumulated the difference to look at the change by running T iterations. Now we ended up here with this inequality with a \\min.\\min_{t \\in [0, T-1]} \\| \\nabla f(w_t) \\|^2  \\le 2\\frac{f(w_0) - f^*}{\\alpha T}\n\nWhat is the minimum norm of gradient we can get in theory? It’s 0 when we reach a minimum of loss function f, though we do not know whether that is a local minimum or a global one. No matter what kind of a minimum it is, if we reach it, we can call Gradient Descent has converged. Therefore, we want to make the bound on the right as tight as possible.\n\nWe can do this by either running more iterations T or have a faster learning rate \\alpha. Within limited iterations, let’s tweak \\alpha here. Recall that \\alpha L \\le 1, so replace \\alpha = \\frac 1  L, we have\\min_{t \\in [0, T-1]} \\| \\nabla f(w_t) \\|^2  \\le 2L \\frac{f(w_0) - f^*}{T}\n\nThis means that the smallest gradient we observe after T iterations is getting smaller proportional to 1/T and GD converges.\n\nNote this proof doesn’t show it reaches a global minimum for the reasons explained above.","type":"content","url":"/2022-09-12-gradient-descent#how-fast-it-converges-where-does-it-go","position":17},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"Appendix","lvl2":"Gradient Descent Converges"},"type":"lvl3","url":"/2022-09-12-gradient-descent#appendix","position":18},{"hierarchy":{"lvl1":"Gradient Descent","lvl3":"Appendix","lvl2":"Gradient Descent Converges"},"content":"The derivative of f at point w in the direction u is defined as the following\\nabla f(w)^T u = \\lim_{\\delta \\rightarrow 0} \\frac{f(w + \\delta u) - f(w)}{\\delta}\n\nAlso the definition of a partial derivative is\\frac{\\partial}{\\partial x}f(x,y,z) = \\lim_{\\delta\\to0} \\frac{f(x+\\delta,y,z) - f(x,y,z)}{\\delta}\n\nAccording to the definition of partial derivative, we add a little \\delta on the \\eta we are differentiating with:\n\n$$\\begin{align}\n\n\\frac{\\partial}{\\partial\\eta} f(w_t - \\eta \\nabla f(w_t))\n&= \\lim_{\\delta \\to 0} \\frac{f(w_t - (\\eta + \\delta) \\nabla f(w_t)) - f(w_t - \\eta \\nabla f(w_t))}{\\delta} && \\text{partial derivative definition} \\\\\n&= \\lim_{\\delta \\to 0} \\frac{f(w_t - \\eta \\nabla f(w_t) - \\delta \\nabla f(w_t)) - f(w_t - \\eta \\nabla f(w_t))}{\\delta}\\\\\n&= \\lim_{\\delta \\to 0} \\frac{f(\\hat w - \\delta \\nabla f(w_t)) - f(\\hat w)}{\\delta} && \\text{Call $\\hat w = w_t - \\eta \\nabla f(w_t)$} \\\\\n&= -\\nabla f(\\hat w)^T \\nabla f(w_t) && \\text{definition of gradient}\\\\\n&= -\\nabla f(w_t - \\eta \\nabla f(w_t))^T \\nabla f(w_t)\n\n\\end{align}\n\n$$\n\nSo we have translated this partial derivative to the gradient at point w_t - \\eta \\nabla f(w_t) in the direction of -\\nabla f(w_t).","type":"content","url":"/2022-09-12-gradient-descent#appendix","position":19},{"hierarchy":{"lvl1":"Gradient Descent and Convexity"},"type":"lvl1","url":"/2022-09-14-gradient-descent-and-convexity","position":0},{"hierarchy":{"lvl1":"Gradient Descent and Convexity"},"content":"Last lecture we proved that GD converges, but doesn’t guarantee it minimizes loss. So when can we say it minimizes loss? When \\ell = f(x) is convex.","type":"content","url":"/2022-09-14-gradient-descent-and-convexity","position":1},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl2":"Convexity"},"type":"lvl2","url":"/2022-09-14-gradient-descent-and-convexity#convexity","position":2},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl2":"Convexity"},"content":"","type":"content","url":"/2022-09-14-gradient-descent-and-convexity#convexity","position":3},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl3":"Definition","lvl2":"Convexity"},"type":"lvl3","url":"/2022-09-14-gradient-descent-and-convexity#definition","position":4},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl3":"Definition","lvl2":"Convexity"},"content":"To visualize graphically, a convex function is that if we draw a line segment between any two points in the graph of the function, that line segment will lie above the function.\n\nConsider the function f: \\R^d \\to\\R","type":"content","url":"/2022-09-14-gradient-descent-and-convexity#definition","position":5},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl4":"0th Order Definition","lvl3":"Definition","lvl2":"Convexity"},"type":"lvl4","url":"/2022-09-14-gradient-descent-and-convexity#id-0th-order-definition","position":6},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl4":"0th Order Definition","lvl3":"Definition","lvl2":"Convexity"},"content":"\\forall x,y \\in \\R^d, \\alpha \\in \\R, \\;  f(\\alpha x + (1 - \\alpha) y) \\le \\alpha f(x) + (1 - \\alpha) f(y)","type":"content","url":"/2022-09-14-gradient-descent-and-convexity#id-0th-order-definition","position":7},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl4":"1st Order Definition","lvl3":"Definition","lvl2":"Convexity"},"type":"lvl4","url":"/2022-09-14-gradient-descent-and-convexity#id-1st-order-definition","position":8},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl4":"1st Order Definition","lvl3":"Definition","lvl2":"Convexity"},"content":"f(x) + (y-x)^T \\nabla f(x) \\le f(y)\n\nA local linear approximation to it is always below it.","type":"content","url":"/2022-09-14-gradient-descent-and-convexity#id-1st-order-definition","position":9},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl4":"2nd Order Definition","lvl3":"Definition","lvl2":"Convexity"},"type":"lvl4","url":"/2022-09-14-gradient-descent-and-convexity#id-2nd-order-definition","position":10},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl4":"2nd Order Definition","lvl3":"Definition","lvl2":"Convexity"},"content":"\\forall x,u \\in \\R^d, \\alpha \\in \\R, \\; \\frac{\\partial}{\\partial \\alpha^2} f(x+\\alpha u) \\ge 0\n\nSecond derivative is always greater than 0. Or in terms of Hessian,u^T \\nabla^2 f(x) u \\ge 0\n\nWhich is equivalent to saying the Hessian matrix H(x) = \\nabla^2f(x) is positive semi-definite.","type":"content","url":"/2022-09-14-gradient-descent-and-convexity#id-2nd-order-definition","position":11},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl2":"Good Properties"},"type":"lvl2","url":"/2022-09-14-gradient-descent-and-convexity#good-properties","position":12},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl2":"Good Properties"},"content":"We want to optimize on convex function because it has this really nothing property that local minimum = global minimum.\n\nThere is a neat proof directly from the 1st order definition: take a point x s.t. \\nabla f(x) = 0, so x is a local minimum. From the definition, we have\\begin{align}\nf(x) + (y-x)^T \\nabla f(x) &\\le f(y)\\\\\nf(x) + (y-x)^T 0 &\\le f(y) \\\\\nf(x)  &\\le f(y)\n\\end{align}\n\nSo x is also a global minimum.","type":"content","url":"/2022-09-14-gradient-descent-and-convexity#good-properties","position":13},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl2":"\\mu-Strongly Convex"},"type":"lvl2","url":"/2022-09-14-gradient-descent-and-convexity#id-mu-strongly-convex","position":14},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl2":"\\mu-Strongly Convex"},"content":"","type":"content","url":"/2022-09-14-gradient-descent-and-convexity#id-mu-strongly-convex","position":15},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl3":"Definition","lvl2":"\\mu-Strongly Convex"},"type":"lvl3","url":"/2022-09-14-gradient-descent-and-convexity#definition-1","position":16},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl3":"Definition","lvl2":"\\mu-Strongly Convex"},"content":"For a \\mu \\in \\R, \\mu \\gt 0, we call the function f is \\mu-Strongly Convex when","type":"content","url":"/2022-09-14-gradient-descent-and-convexity#definition-1","position":17},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl4":"1st Order Definition","lvl3":"Definition","lvl2":"\\mu-Strongly Convex"},"type":"lvl4","url":"/2022-09-14-gradient-descent-and-convexity#id-1st-order-definition-1","position":18},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl4":"1st Order Definition","lvl3":"Definition","lvl2":"\\mu-Strongly Convex"},"content":"f(x) + (y-x)^T \\nabla f(x) + \\frac \\mu 2 \\|y-x\\|^2 \\le f(y)\n\nFor this stronger convex, f not only has to be greater than linear approximation, but also has to be greater than it plus a parabola with positive curvature \\mu.","type":"content","url":"/2022-09-14-gradient-descent-and-convexity#id-1st-order-definition-1","position":19},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl4":"2nd Order Definition","lvl3":"Definition","lvl2":"\\mu-Strongly Convex"},"type":"lvl4","url":"/2022-09-14-gradient-descent-and-convexity#id-2nd-order-definition-1","position":20},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl4":"2nd Order Definition","lvl3":"Definition","lvl2":"\\mu-Strongly Convex"},"content":"\\forall x,u \\in \\R^d, \\alpha \\in \\R, \\text{s.t. } \\|u\\|^2 = 1,  \\; \\frac{\\partial^2}{\\partial \\alpha^2} f(x+\\alpha u) \\ge \\mu\n\nThis is the most natural way to define it","type":"content","url":"/2022-09-14-gradient-descent-and-convexity#id-2nd-order-definition-1","position":21},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl4":"Another Very Important Definition","lvl3":"Definition","lvl2":"\\mu-Strongly Convex"},"type":"lvl4","url":"/2022-09-14-gradient-descent-and-convexity#another-very-important-definition","position":22},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl4":"Another Very Important Definition","lvl3":"Definition","lvl2":"\\mu-Strongly Convex"},"content":"\\exists g \\text{ s.t. } f(w) = g(w) + \\frac \\mu 2 \\|w\\|^2 \\text{ and $g$ is convex}\n\nThis is very useful in ML because from this, we can say that any function is \\mu-strictly convex as long as it can be written in the form of a convex function plus a L2 regularizer.","type":"content","url":"/2022-09-14-gradient-descent-and-convexity#another-very-important-definition","position":23},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl2":"PL property"},"type":"lvl2","url":"/2022-09-14-gradient-descent-and-convexity#pl-property","position":24},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl2":"PL property"},"content":"If a function is of \\mu-strong convexity, it has this Polyak-Lojasiewicz property, which says (f^* is the global minimum)\\left\\| \\nabla f(x) \\right\\|^2 \\ge 2 \\mu \\left( f(x) - f^* \\right)\n\nThis inequality says that our function’s gradient is only close to 0 when we are close to the global minimum.","type":"content","url":"/2022-09-14-gradient-descent-and-convexity#pl-property","position":25},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl3":"GD Converge Linearly on PL Function","lvl2":"PL property"},"type":"lvl3","url":"/2022-09-14-gradient-descent-and-convexity#gd-converge-linearly-on-pl-function","position":26},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl3":"GD Converge Linearly on PL Function","lvl2":"PL property"},"content":"At the end of the proof, we had this inequality belowf(w_{t+1}) \\le f(w_t) - \\alpha \\left(1 - \\frac{\\alpha L}{2}  \\right) \\cdot \\| \\nabla f(w_t) \\|^2\n\nBy making an assumption that \\alpha L \\le 1, we concluded that GD converges at a rate proportional to \\alpha and the norm of gradient. What happens when we have a PL function?\n\nTo simplify this inequality first, we get rid of this big term - (1 - \\frac{\\alpha L}{2}) \\le - \\frac 1 2f(w_{t+1}) \\le f(w_t) - \\frac \\alpha 2 \\| \\nabla f(w_t) \\|^2\n\nWith our PL property, we can replace the norm of gradient with global minimum.\\begin{align}\nf(w_{t+1}) &\\le f(w_t) - \\mu \\alpha \\left( f(w_t) - f^* \\right)\\\\\nf(w_{t+1}) - f^* &\\le (f(w_t) - f^*) - \\mu \\alpha \\left( f(w_t) - f^* \\right)\\\\\nf(w_{t+1}) - f^* &\\le (1 - \\mu \\alpha)  \\left( f(w_t) - f^* \\right)\n\\end{align}\n\nCall this multiplicative factor \\sigma = 1 - \\mu \\alpha, so we havef(w_{t+1}) - f^* \\le \\sigma \\left( f(w_t) - f^* \\right)\n\nSince the left of the inequality is a positive number and the right is also a positive number, we must have \\sigma is a positive number. From here, we can then safely recursively apply this inequality T times and getf(w_{T}) - f^* \\le \\sigma^T \\left( f(w_0) - f^* \\right)\n\nRecall \\sigma = 1 - \\mu \\alpha and It holds true for all x that 1-x \\le e^{-x}, so we can rewrite the above inequality as below, where e^x = exp(x):f(w_{T}) - f^* \\le exp(-\\mu \\alpha)^T \\left( f(w_0) - f^* \\right)\\\\\nf(w_{T}) - f^* \\le exp(-T \\mu \\alpha) \\left( f(w_0) - f^* \\right)\n\nThis shows that, for \\mu-strongly convex functions, gradient descent with a constant step size converges exponentially quickly to the optimum. This is sometimes called convergence at a linear rate (I know it’s confusing when we have exponential convergence rate but call it a “linear time”. This is just a name from numeric optimization)","type":"content","url":"/2022-09-14-gradient-descent-and-convexity#gd-converge-linearly-on-pl-function","position":27},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl3":"What really interesting is","lvl2":"PL property"},"type":"lvl3","url":"/2022-09-14-gradient-descent-and-convexity#what-really-interesting-is","position":28},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl3":"What really interesting is","lvl2":"PL property"},"content":"Now take a closer look at the value \\sigma = 1 - \\mu \\alpha. To get the quickest convergence, we know from last time that we need to set \\alpha = 1/L, so we have\\alpha = \\frac 1 L, \\sigma = 1 - \\frac \\mu L \\\\\nf(w_{T}) - f^* \\le exp(-\\frac {\\mu T} L) \\left( f(w_0) - f^* \\right)\n\nSet our goal as: for a given margin \\epsilon \\gt 0, by running GD T times, we can output a prediction \\hat w s.t. f(\\hat w) - f^* \\le \\epsilon, sof(w_{T}) - f^* \\le \\epsilon\n\nWe look at the one with an exponential term instead, soexp(-\\frac {\\mu T} L) \\left( f(w_0) - f^* \\right) \\le \\epsilon\n\nTo solve this, we haveT \\ge \\frac{L}{\\mu} \\log\\left( \\frac{f(w_0) - f^*}{\\epsilon} \\right)\n\nIn CS, when we see a \\log term, we tend to say we can ignore it, so this inequality actually tells us that the number of iterations we need for GD to converge doesn’t depend on initialization w_0 or \\epsilon how accurate we want our solution to be. It depends instead only on \\frac L \\mu.","type":"content","url":"/2022-09-14-gradient-descent-and-convexity#what-really-interesting-is","position":29},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl3":"Condition Number","lvl2":"PL property"},"type":"lvl3","url":"/2022-09-14-gradient-descent-and-convexity#condition-number","position":30},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl3":"Condition Number","lvl2":"PL property"},"content":"We will call \\kappa = \\frac L \\mu the condition number of our problem. The condition number encodes how hard a strongly convex problem is to solve. This ratio is invariant to scaling of the objective function f. Recall the definition of L and \\mu: L is the upperbound of f’s second gradient and \\mu is the lowerbound.\\forall x,u \\in \\R^d, \\alpha \\in \\R, \\text{s.t. } \\|u\\|^2 = 1, \\; \\|\\frac{\\partial^2}{\\partial\\alpha^2} f(\\mathbf w+\\alpha \\mathbf u) \\| \\le L \\\\\n\\forall x,u \\in \\R^d, \\alpha \\in \\R, \\text{s.t. } \\|u\\|^2 = 1,  \\; \\| \\frac{\\partial^2}{\\partial \\alpha^2} f(x+\\alpha u) \\| \\ge \\mu\n\nThe naming of condition number comes from numerical analysis where it describes the ratio between the largest singular value and the smallest singular value in the SVD of a matrix. This \\kappa we have here is indeed the condition number of the Hessian matrix of our loss. Since a Hessian matrix is always semi-definite and positive, the condition number of a Hessian matrix is just the ratio between biggest and smallest eigenvalue.\n\nThe in-class demo showed an example where when we run GD on linear regression and it works horrible in raw data but works well on normalized data. One thing nice about linear regression is that we can actually calculate the exact Hessian matrix. Therefore, if we look at the Hessian, we understand when we have unnormalized data, we had a very large condition number (10^14) and it will take forever for GD to converge.\\begin{align}\n\\ell(w) &= \\frac{1}{n} \\sum_{i=1}^n (x_i^T w - y_i)^2 = \\frac 1 n \\|Xw - Y\\|^2\\\\\n\\nabla \\ell(w) &= \\frac{2}{n} \\sum_{i=1}^n x_i (x_i^T w - y_i) = \\frac 2 n X^T (Xw - Y)\\\\\n\\nabla^2 \\ell(w) &= \\frac{2 }{n} \\sum_{i=1}^n x_i x_i^T = \\frac 2 n X^T X\n\\end{align}","type":"content","url":"/2022-09-14-gradient-descent-and-convexity#condition-number","position":31},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl3":"Time Complexity","lvl2":"PL property"},"type":"lvl3","url":"/2022-09-14-gradient-descent-and-convexity#time-complexity","position":32},{"hierarchy":{"lvl1":"Gradient Descent and Convexity","lvl3":"Time Complexity","lvl2":"PL property"},"content":"We conclude that to run gradient descent, it will take O(\\kappa) steps and O(nd \\kappa) time (nd time to calculate each gradient descent step)","type":"content","url":"/2022-09-14-gradient-descent-and-convexity#time-complexity","position":33},{"hierarchy":{"lvl1":"Stochastic Gradient Descent"},"type":"lvl1","url":"/2022-09-19-stochastic-gradient-descent","position":0},{"hierarchy":{"lvl1":"Stochastic Gradient Descent"},"content":"","type":"content","url":"/2022-09-19-stochastic-gradient-descent","position":1},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl2":"Introduction"},"type":"lvl2","url":"/2022-09-19-stochastic-gradient-descent#introduction","position":2},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl2":"Introduction"},"content":"Recall we defined the loss function asf(w) = \\frac 1 n \\sum^n_{i=1} f_i(w)\n\nGD: w_{t+1} = w_t - \\alpha \\nabla f(w_t) = w_t - \\frac \\alpha n \\sum^n_{i=1} \\nabla f_i(w_t)\n\nSGD: Sample a random i \\in [1,n], update w_{t+1} = w_t - \\alpha \\nabla f_i(w_t)\n\nMinibatch SGD: Sample B i_b i.i.d from [1,n] with replacement (in practice, we don’t do replacement, but here is to make the proof below easier) , update w_{t+1} = w_t - \\frac \\alpha B \\sum^B_{b=1} \\nabla f_{i_b}(w_t)","type":"content","url":"/2022-09-19-stochastic-gradient-descent#introduction","position":3},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl3":"Minibatch SGD Converges?","lvl2":"Introduction"},"type":"lvl3","url":"/2022-09-19-stochastic-gradient-descent#minibatch-sgd-converges","position":4},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl3":"Minibatch SGD Converges?","lvl2":"Introduction"},"content":"It’s not too difficult to show SGD converges just like GD through expectation, so we’ll focus on minibatch SGD. How do we know it converges? We want to show in expectation, it behaves the same as SGD, so if we perform one step minibatch SGD, we have\\mathbf E[w_{t+1} \\mid w_t] = w_t - \\alpha \\nabla f(w_t)\n\nNote in this current context, i_b are the base Random Variables, w_t, w_{t+1} are functions of i_b and are also RV. \\alpha, B are constants.","type":"content","url":"/2022-09-19-stochastic-gradient-descent#minibatch-sgd-converges","position":5},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl2":"The Easier Part"},"type":"lvl2","url":"/2022-09-19-stochastic-gradient-descent#the-easier-part","position":6},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl2":"The Easier Part"},"content":"Similar to what we did in the GD convergence proof, we make some assumptions on our function f:\n\nf is L-smooth: \\| \\nabla f(w) - \\nabla f(u) \\|_2 \\le L \\| u-v \\|_2\n\nglobal min exists: \\exists f^* \\; s.t.  \\; f(w) \\ge f^*\n\nCall the average batch gradientg_t = \\frac 1 B \\sum^B_{b=1} \\nabla f_{i_b}(w_t)\n\nFirst half of the proof is all the same as GD convergence proof, where we have\\begin{align}\nf(w_{t+1}) &= f(w_t - \\alpha \\nabla f(w_t)) \\\\\n&= f(w_t) + \\int^\\alpha_0 \\frac{\\partial}{\\partial\\eta} f(w_t - \\eta \\nabla f(w_t)) \\; d\\eta \\\\\n& \\dots \\\\\n&\\le f(w_t) - \\alpha \\nabla f(w_t)^T g_t + \\frac{\\alpha^2 L}{2} \\| g_t \\|^2 \\\\\n\\end{align}\n\nIn the GD proof, we had g_t = \\nabla f(w_t), so by adding some constraint \\alpha L \\le 1, we can conclude it decreases at each step so eventually converges. However, here we can only hope it does so in expectation. On both sides take expectation given w_t, note the only thing random here is g_t. All the other values are either originally deterministic (constant \\alpha, L) or deterministic after given w_t (\\nabla f(w_t))\\begin{align}\n\\mathbf E [ f(w_{t+1}) \\mid w_t ]\n& \\le\n\\mathbf E \\left[  f(w_t) - \\alpha \\nabla f(w_t)^T g_t + \\frac{\\alpha^2 L}{2} \\| g_t \\|^2  \\mid w_t \\right] \\\\\n\\mathbf E [ f(w_{t+1}) \\mid w_t ]\n&\\le\nf(w_t) - \\alpha \\nabla f(w_t)^T \\mathbf E [ g_t | w_t ] + \\frac{\\alpha^2 L}{2} \\mathbf E [ \\| g_t \\|^2 | w_t ]\n\\end{align}\n\nObserve that for an arbitrary sample i, its expected gradient is just the global gradient. To show this, apply definition of expectation.\\mathbf E[\\nabla f_i(w_t) \\mid w_t] = \\sum^n_{j=1} P(i=j) \\nabla f_j(w_t) = \\sum^n_{j=1} \\frac 1 n \\nabla f_j(w_t) =\\nabla f(w_t)\n\nSo the batch average gradient is also just the global gradient. We can see this by applying linearity of expectation.\\mathbf E[g_t \\mid w_t] = \\mathbf E\\left[\\frac 1 B \\sum^B_{b=1} \\nabla f_{i_b}(w_t) \\mid w_t\\right]  = \\frac 1 B \\sum^B_{b=1} \\mathbf E\\left[ \\nabla f_{i_b}(w_t) \\mid w_t\\right]  = \\nabla f(w_t)\n\nTherefore, we can rewrite the above inequality as\\begin{align}\n\\mathbf E [ f(w_{t+1}) \\mid w_t ] &\\le\nf(w_t) - \\alpha \\nabla f(w_t)^T \\nabla f(w_t) + \\frac{\\alpha^2 L}{2} \\mathbf E [ \\| g_t \\|^2 | w_t ]\\\\\n\\mathbf E [ f(w_{t+1}) \\mid w_t ] &\\le\nf(w_t) - \\alpha \\| \\nabla f(w_t) \\|^2 + \\frac{\\alpha^2 L}{2} \\mathbf E [ \\| g_t \\|^2 | w_t ]\n\\end{align}","type":"content","url":"/2022-09-19-stochastic-gradient-descent#the-easier-part","position":7},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl2":"Variance of Gradient"},"type":"lvl2","url":"/2022-09-19-stochastic-gradient-descent#variance-of-gradient","position":8},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl2":"Variance of Gradient"},"content":"","type":"content","url":"/2022-09-19-stochastic-gradient-descent#variance-of-gradient","position":9},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl3":"Assumption on Variance","lvl2":"Variance of Gradient"},"type":"lvl3","url":"/2022-09-19-stochastic-gradient-descent#assumption-on-variance","position":10},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl3":"Assumption on Variance","lvl2":"Variance of Gradient"},"content":"Therefore, now all we have different is this last square term. Compare\\begin{align}\n\\text{GD: } f(w_{t+1}) &\\le f(w_t) - \\alpha \\| \\nabla f(w_t) \\|^2 + \\frac{\\alpha^2 L}{2} \\| \\nabla f(w_t) \\|^2 \\\\\n\\text{SGD: }\\mathbf E [ f(w_{t+1}) \\mid w_t ] &\\le\nf(w_t) - \\alpha \\| \\nabla f(w_t) \\|^2 + \\frac{\\alpha^2 L}{2} \\mathbf E [ \\| g_t \\|^2 | w_t ]\n\\end{align}\n\nTo tweak this term, we need an extra assumption on the variance of the gradient:\n\nthe variance of the gradients is bounded: There exists a constant \\sigma > 0, such that for a uniformly randomly drawn sample i,\n\\mathbf{E}\\left[ \\| \\nabla f_i(w) - \\nabla f(w) \\|^2 \\right] \\le \\sigma^2","type":"content","url":"/2022-09-19-stochastic-gradient-descent#assumption-on-variance","position":11},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl3":"Meaning of this Bound","lvl2":"Variance of Gradient"},"type":"lvl3","url":"/2022-09-19-stochastic-gradient-descent#meaning-of-this-bound","position":12},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl3":"Meaning of this Bound","lvl2":"Variance of Gradient"},"content":"Note this is just sayingVar(\\nabla f(w)) \\le \\sigma^2\n\nBut we cannot use the Var notation because it is only for a scalar random variable, not for a vector RV. For the same reason, we need to prove the classic Var(X) = \\mathbf E[X^2] - \\mathbf E^2[X] before using it. We prove it in the following by employing the fact that each i will be drawn i.i.d.\\begin{align}\n\\mathbf{E}\\left[ \\| \\nabla f_i(w) - \\nabla f(w) \\|^2 \\right] &= \\frac{1}{n} \\sum_{i=1}^N \\| \\nabla f_i(w) - \\nabla f(w) \\|^2\\\\\n&= \\frac 1 n \\sum^N_{i=1} \\|\\nabla f_i(w)\\|^2\n- \\frac 2 n \\sum^N_{i=1} \\nabla f_i(w)^T \\nabla f(w)\n+ \\frac 1 n \\sum^N_{i=1} \\|\\nabla f(w)\\|^2 \\\\\n\\end{align}\n\nNote in the second term, from the expectation of a specific sample’s gradient \\mathbf E[\\nabla f_i(w_t)] = \\nabla f(w_t) we have2 \\cdot \\frac 1 n \\sum^N_{i=1} \\nabla f_i(w)^T \\text{ something} = 2 \\nabla f(w) ^T \\text{ something}\n\nso we can rewrite the above equation as\\begin{align}\n\\mathbf{E}\\left[ \\| \\nabla f_i(w) - \\nabla f(w) \\|^2 \\right]\n&= \\frac 1 n \\sum^N_{i=1} \\|\\nabla f_i(w)\\|^2\n- 2 \\nabla f(w) ^T  \\nabla f(w)\n+ \\|\\nabla f(w)\\|^2 \\\\\n&= \\left(\\frac 1 n \\sum^N_{i=1} \\|\\nabla f_i(w)\\|^2 \\right)\n- \\|\\nabla f(w)\\|^2 \\\\\n&= \\mathbf{E}\\left[ \\| \\nabla f_i(w) \\|^2 \\right] -  \\| \\nabla f(w) \\|^2 \\\\\n&= \\mathbf{E}\\left[ \\| \\nabla f_i(w) \\|^2 \\right] -  \\mathbf{E}^2\\left[ \\| \\nabla f(w) \\| \\right] \\\\\n\\end{align}","type":"content","url":"/2022-09-19-stochastic-gradient-descent#meaning-of-this-bound","position":13},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl3":"??? Add On to Variance ???","lvl2":"Variance of Gradient"},"type":"lvl3","url":"/2022-09-19-stochastic-gradient-descent#id-add-on-to-variance","position":14},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl3":"??? Add On to Variance ???","lvl2":"Variance of Gradient"},"content":"","type":"content","url":"/2022-09-19-stochastic-gradient-descent#id-add-on-to-variance","position":15},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl3":"Applying Variance Bound","lvl2":"Variance of Gradient"},"type":"lvl3","url":"/2022-09-19-stochastic-gradient-descent#applying-variance-bound","position":16},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl3":"Applying Variance Bound","lvl2":"Variance of Gradient"},"content":"Now we are prepared to look at this term\\begin{align}\n\\mathbf E [ \\| g_t \\|^2 | w_t ]\n&= \\mathbf E \\left[\\left\\| \\frac 1 B \\sum^B_{b=1} \\nabla f_{i_b}(w_t) \\right\\|^2 \\mid w_t \\right] \\\\\n&= \\frac 1 {B^2} \\mathbf E \\left[\\left( \\sum^B_{b=1} \\nabla f_{i_b}(w_t) \\right)^T \\left( \\sum^B_{c=1} \\nabla f_{i_c}(w_t) \\right) \\mid w_t \\right] \\\\\n&= \\frac 1 {B^2} \\sum^B_{b=1} \\sum^B_{c=1}\\mathbf E \\left[ \\nabla f_{i_b}(w_t)^T \\nabla f_{i_c}(w_t) \\mid w_t \\right] \\\\\n\\end{align}\n\nAmong these total B^2 pairs, we have B pairs with the same subscripts and B(B-1) pairs with distinct subscripts. For such pairs with distinct subscripts, we can apply linearity of expectation multiplication since they are drawn i.i.d. For pairs of the same subscript, note again expectation of \\| \\nabla f_{i}(w_t)\\|^2 is just the average across all N training data.\\begin{align}\n\\mathbf E [ \\| g_t \\|^2 | w_t ]\n&= \\frac 1 {B^2} \\left( \\sum^B_{b=1} \\sum^B_{c\\not=b}\\mathbf E \\left[ \\nabla f_{i_b}(w_t)^T \\nabla f_{i_c}(w_t) \\mid w_t \\right] +  \\sum^B_{b=1} \\mathbf E \\left[ \\nabla f_{i_b}(w_t)^T \\nabla f_{i_b}(w_t) \\mid w_t \\right] \\right) \\\\\n&= \\frac 1 {B^2} \\left( \\sum^B_{b=1} \\sum^B_{c\\not=b}\\mathbf E \\left[ \\nabla f_{i_b}(w_t) \\mid w_t \\right]^T \\mathbf E\\left[\\nabla f_{i_c}(w_t) \\mid w_t \\right] +  \\sum^B_{b=1} \\mathbf E \\left[ \\| \\nabla f_{i_b}(w_t)\\|^2 \\mid w_t \\right] \\right) \\\\\n&= \\frac 1 {B^2} \\left( (B^2-B) \\| \\nabla f(w_t)\\|^2 +  B \\; \\frac 1 n \\sum^N_{i=1} \\| \\nabla f_{i}(w_t)\\|^2 \\right) \\\\\n&= \\frac {B-1} {B} \\| \\nabla f(w_t)\\|^2 +  \\; \\frac 1 {B} \\frac 1 n \\sum^N_{i=1} \\| \\nabla f_{i}(w_t)\\|^2 \\\\\n\\end{align}\n\nFrom our assumption of bound on variance, we replace the last term with\\begin{align}\n\\mathbf E [ \\| g_t \\|^2 | w_t ]\n&= \\frac {B-1} {B} \\| \\nabla f(w_t)\\|^2 +  \\; \\frac 1 {B} \\frac 1 n \\sum^N_{i=1} \\mathbf \\| \\nabla f_{i}(w_t)\\|^2 \\\\\n&= \\frac {B-1} {B} \\| \\nabla f(w_t)\\|^2 +  \\; \\frac 1 {B} \\left( \\mathbf{E}\\left[ \\| \\nabla f_i(w) - \\nabla f(w) \\|^2 \\right]  + \\|\\nabla f(w)\\|^2 \\right) \\\\\n&\\le \\frac {B-1} {B} \\| \\nabla f(w_t)\\|^2 +  \\; \\frac 1 {B} \\left( \\sigma^2  + \\|\\nabla f(w)\\|^2 \\right) \\\\\n&\\le \\| \\nabla f(w_t)\\|^2 +  \\; \\frac {\\sigma^2} {B} \\\\\n\\end{align}","type":"content","url":"/2022-09-19-stochastic-gradient-descent#applying-variance-bound","position":17},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl3":"Finishing Up Derivation","lvl2":"Variance of Gradient"},"type":"lvl3","url":"/2022-09-19-stochastic-gradient-descent#finishing-up-derivation","position":18},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl3":"Finishing Up Derivation","lvl2":"Variance of Gradient"},"content":"We can finally look back to our definition\\begin{align}\n\\mathbf E [ f(w_{t+1}) \\mid w_t ] &\\le\nf(w_t) - \\alpha \\| \\nabla f(w_t) \\|^2 + \\frac{\\alpha^2 L}{2} \\mathbf E [ \\| g_t \\|^2 | w_t ]\\\\\n&\\le f(w_t) - \\alpha \\| \\nabla f(w_t) \\|^2 + \\frac{\\alpha^2 L}{2} \\left( \\| \\nabla f(w_t) \\|^2 + \\frac {\\sigma^2} {B} \\right)\\\\\n&\\le f(w_t) - \\alpha \\left( 1 - \\frac{\\alpha L}{2} \\right) \\| \\nabla f(w_t) \\|^2 + \\frac{\\alpha^2 \\sigma^2 L}{2B}\n\\end{align}\n\nRemember we have old assumption that \\alpha L \\le 1, so\\mathbf{E}[ f(w_{t+1}) \\mid w_t] \\le f(w_t) - \\frac{\\alpha}{2} \\| \\nabla f(w_t) \\|^2 + \\frac{\\alpha^2 \\sigma^2 L}{2B}","type":"content","url":"/2022-09-19-stochastic-gradient-descent#finishing-up-derivation","position":19},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl2":"PL Condition"},"type":"lvl2","url":"/2022-09-19-stochastic-gradient-descent#pl-condition","position":20},{"hierarchy":{"lvl1":"Stochastic Gradient Descent","lvl2":"PL Condition"},"content":"Let’s focus on function f that satisfies PL condition. So we add our 4th assumption:\n\n\\left\\| \\nabla f(x) \\right\\|^2 \\ge 2 \\mu \\left( f(x) - f^* \\right)\n\nWith this, we can write\\begin{align}\n\\mathbf{E}[ f(w_{t+1}) \\mid w_t]\n&\\le f(w_t) - \\frac{\\alpha}{2} \\| \\nabla f(w_t) \\|^2 + \\frac{\\alpha^2 \\sigma^2 L}{2B}\\\\\n&\\le f(w_t) - \\alpha \\mu  \\left( f(w_t) - f^* \\right) + \\frac{\\alpha^2 \\sigma^2 L}{2B}\\\\\n\\end{align}\n\nSubtract a f^* on both sides of the inequality. On the left side, it goes straight into the expectation because of linearity of expectation\\begin{align}\n\\mathbf{E}[ f(w_{t+1}) \\mid w_t]  - f^*\n&\\le (f(w_t)  - f^*) - \\alpha \\mu  \\left( f(w_t) - f^* \\right) + \\frac{\\alpha^2 \\sigma^2 L}{2B}\\\\\n\\mathbf{E}[ f(w_{t+1}) - f^*\\mid w_t]\n&\\le (1 - \\alpha \\mu)  \\left( f(w_t) - f^* \\right) + \\frac{\\alpha^2 \\sigma^2 L}{2B}\\\\\n\\end{align}\n\nTo get rid of the “given w_t” term, we apply the Law of Total Expectation \\mathbf E[\\mathbf E [X | Y]] = \\mathbf E[X] and take expected value on both sides\\begin{align}\n\\mathbf{E}\\left[ \\mathbf{E}[ f(w_{t+1}) - f^*\\mid w_t] \\right]\n&\\le \\mathbf{E}\\left[ (1 - \\alpha \\mu)  \\left( f(w_t) - f^* \\right) + \\frac{\\alpha^2 \\sigma^2 L}{2B} \\right] \\\\\n\\mathbf{E}[ f(w_{t+1}) - f^*]\n&\\le (1 - \\alpha \\mu)   \\mathbf{E}\\left[f(w_t) - f^* \\right] + \\frac{\\alpha^2 \\sigma^2 L}{2B}  \\\\\n\\end{align}\n\nCall \\rho_t  = \\mathbf{E}\\left[f(w_t) - f^* \\right]\\rho_{t+1}\n\\le (1 - \\alpha \\mu)   \\rho_t + \\frac{\\alpha^2 \\sigma^2 L}{2B}\n\nObserve when t \\to \\infin,\\rho_{\\infin} = (1 - \\alpha \\mu)   \\rho_\\infin + \\frac{\\alpha^2 \\sigma^2 L}{2B}\\\\\n\\rho_{\\infin} = \\frac{\\alpha \\sigma^2 L} {2 \\mu B}\n\nIf we subtract this value in both sides of the inequality\\begin{align}\n\\rho_{t+1} - \\frac{\\alpha \\sigma^2 L} {2 \\mu B}\n&\\le (1 - \\alpha \\mu)   \\rho_t + \\frac{\\alpha^2 \\sigma^2 L}{2B} - \\frac{\\alpha \\sigma^2 L} {2 \\mu B} \\\\\n&\\le (1 - \\alpha \\mu)   (\\rho_t - \\frac{\\alpha \\sigma^2 L} {2 \\mu B}) \\\\\n\\end{align}\n\nNow we are ready to look at what happens when we start from initialization t=0 and runs T iterations\\begin{align}\n\\rho_{t} - \\frac{\\alpha \\sigma^2 L} {2 \\mu B}\n&\\le (1 - \\alpha \\mu)^T   (\\rho_0 - \\frac{\\alpha \\sigma^2 L} {2 \\mu B}) \\\\\n&\\le (1 - \\alpha \\mu)^T   \\rho_0 \\\\\n\\end{align}\n\nWe apply the same trick of 1-x \\le e^{-x}\\begin{align}\n\\mathbf{E}\\left[f(w_T) - f^* \\right] - \\frac{\\alpha \\sigma^2 L} {2 \\mu B}\n&\\le (1 - \\alpha \\mu)^T   \\mathbf{E}\\left[f(w_0) - f^* \\right] \\\\\n&\\le exp(-\\alpha \\mu T) \\; \\mathbf{E}\\left[f(w_0) - f^* \\right] \\\\\n\\end{align}\n\nIn conclusion: (f(w_0) only depends on initialization)\\mathbf{E}\\left[f(w_T) - f^* \\right]\n\\le exp(-\\alpha \\mu T) \\; (f(w_0) - f^*) + \\frac{\\alpha \\sigma^2 L} {2 \\mu B}\n\nComparing this with what we had with normal Gradient Descentf(w_{T}) - f^* \\le exp(-\\frac {\\mu T} L) \\left( f(w_0) - f^* \\right)\n\nIn GD, the loss gap goes to 0 exponentially. In SGD, There is this \\frac{\\alpha \\sigma^2 L} {2 \\mu B} error term caused by running GD stochastically. The loss gap flattens out when reaches this noise ball.\n\nOne thing to notice that is if we have a fixed learning rate, this noise ball will evaluate to a constant and we can never reach the minimum. However, if we have an adaptive learning rate that decreases as a function of step, we can actually push this noise ball to 0 and reach the minimum at the end.","type":"content","url":"/2022-09-19-stochastic-gradient-descent#pl-condition","position":21},{"hierarchy":{"lvl1":"Stochastic Gradient Descent Improved"},"type":"lvl1","url":"/2022-09-21-stochastic-gradient-descent-improved","position":0},{"hierarchy":{"lvl1":"Stochastic Gradient Descent Improved"},"content":"","type":"content","url":"/2022-09-21-stochastic-gradient-descent-improved","position":1},{"hierarchy":{"lvl1":"Stochastic Gradient Descent Improved","lvl2":"Minibatch SGD Running Time"},"type":"lvl2","url":"/2022-09-21-stochastic-gradient-descent-improved#minibatch-sgd-running-time","position":2},{"hierarchy":{"lvl1":"Stochastic Gradient Descent Improved","lvl2":"Minibatch SGD Running Time"},"content":"Recall from last time, we have\\mathbf{E}\\left[f(w_T) - f^* \\right]\n\\le exp(-\\alpha \\mu T) \\; (f(w_0) - f^*) + \\frac{\\alpha \\sigma^2 L} {2 \\mu B}\n\nSimilar to our proof of GD converges linearly on PL-condition functions, set our goal as for a given margin \\epsilon \\gt 0, by running minibatch SGD T times, we can output a prediction \\hat w s.t. \\mathbf E [f(\\hat w) - f^*] \\le \\epsilon, so we can writeexp(-\\alpha \\mu T) \\; (f(w_0) - f^*) + \\frac{\\alpha \\sigma^2 L} {2 \\mu B} \\le \\epsilon\n\nTo make it easier for ourselves, we instead find at least how many T we need to run so thatexp(-\\alpha \\mu T) \\; (f(w_0) - f^*) \\le \\frac \\epsilon 2 \\\\\n\\frac{\\alpha \\sigma^2 L} {2 \\mu B} \\le \\frac \\epsilon 2\n\nThe first expression givesT \\ge \\frac 1 {\\alpha \\mu} \\log(\\frac {2(f(w_0) - f^*)} {\\epsilon})\n\nThe second expression gives\\alpha \\le \\frac {B \\mu \\epsilon} {L \\sigma^2}\n\nRecall we have another constraint on \\alpha that \\alpha L \\le 1, so\\alpha \\le \\frac 1 L \\min(\\frac {B \\mu \\epsilon} {\\sigma^2}, 1)\n\nThis is actually a pretty interesting inequality. It says if we have a big batch size and low variance, so the batch gradient is representative of the global gradient, we can set \\alpha to be big. Replace the \\alpha in T expression hereT \\ge \\max(\\frac {\\sigma^2} {B \\mu \\epsilon}, 1) \\; \\frac L \\mu \\; \\log(\\frac {2(f(w_0) - f^*)} {\\epsilon})\n\nNote \\frac L \\mu = \\kappa is the condition number. All the T satisfies this condition will produce a small enough \\epsilon, so the first / smallest one that satisfices this condition will simply be the floor. Therefore, the number of step for minibatch SGD to converge isT = \\left\\lceil \\max(\\frac {\\sigma^2} {B \\mu \\epsilon}, 1) \\; \\kappa\\log(\\frac {2(f(w_0) - f^*)} {\\epsilon}) \\right\\rceil\n\nEach step, we need B time to compute the batch gradient (assuming \\mathcal O(1) time to calculate each sample’s gradient. This time is usually \\mathcal O(d) but it depends, so we just ignore it for now). The running time of minibatch SGD to reach \\epsilon precision is\\mathcal O \\left(\\max(\\frac {\\sigma^2} {\\mu \\epsilon}, B) \\; \\kappa \\log(\\frac {2(f(w_0) - f^*)} {\\epsilon})\\right)\n\nFor most of the time, \\frac {\\sigma^2} {\\mu \\epsilon} is the bigger part, and we ignore the \\log value, so running time of SGD is in short\\mathcal O \\left(\\frac {\\sigma^2} {\\mu \\epsilon} \\kappa \\right)","type":"content","url":"/2022-09-21-stochastic-gradient-descent-improved#minibatch-sgd-running-time","position":3},{"hierarchy":{"lvl1":"Stochastic Gradient Descent Improved","lvl2":"Comparing to GD"},"type":"lvl2","url":"/2022-09-21-stochastic-gradient-descent-improved#comparing-to-gd","position":4},{"hierarchy":{"lvl1":"Stochastic Gradient Descent Improved","lvl2":"Comparing to GD"},"content":"Compare the full version of minibatch SGD running time with GD, we see that the n is basically replaced with the \\max(\\frac {\\sigma^2} {\\mu \\epsilon}, B)\\mathcal O \\left(n \\kappa \\log(\\frac {2(f(w_0) - f^*)} {\\epsilon})\\right)\n\nSo there are some situations more suitable to minibatch SGD we can immediately see:\n\nhuge n: GD runs too slowly to calculate gradient on the whole dataset\n\nnot too big precision \\epsilon needed\n\nIn what situation does GD runs better? Maybe when we have too large \\sigma^2, so the average batch gradient is not representative of the whole dataset? Is this true though? Think when we have a dataset of a very large variance (maybe all the data points are uniformly distributed on some space), so randomly sample a data point i, \\nabla f_i will have almost nothing to do with \\mathbf E [\\nabla f_i]. If this is the case, even GD cannot learn much and this learning / optimization problem just doesn’t hold itself.","type":"content","url":"/2022-09-21-stochastic-gradient-descent-improved#comparing-to-gd","position":5},{"hierarchy":{"lvl1":"Stochastic Gradient Descent Improved","lvl2":"Another Advantage of SGD - Hardware"},"type":"lvl2","url":"/2022-09-21-stochastic-gradient-descent-improved#another-advantage-of-sgd-hardware","position":6},{"hierarchy":{"lvl1":"Stochastic Gradient Descent Improved","lvl2":"Another Advantage of SGD - Hardware"},"content":"Minibatch SGD runs significantly faster than GD not only because of the decrease in computation cost, but also because when we have too large an n, we will blow up the memory and have to do a lot of memory swap and other overheads.\n\nMinibatch SGD is faster than vanilla SGD because minibatch can utilize parallelism of the hardware.","type":"content","url":"/2022-09-21-stochastic-gradient-descent-improved#another-advantage-of-sgd-hardware","position":7},{"hierarchy":{"lvl1":"Stochastic Gradient Descent Improved","lvl2":"Minibatch SGD in Practice"},"type":"lvl2","url":"/2022-09-21-stochastic-gradient-descent-improved#minibatch-sgd-in-practice","position":8},{"hierarchy":{"lvl1":"Stochastic Gradient Descent Improved","lvl2":"Minibatch SGD in Practice"},"content":"When analyzing convergence and running time, we drew the batch with replacement. However, in practice, drawing the batch without replacement gives us better result.\n\nThe best practice is random reshuffle, where we randomly shuffle the whole dataset and divide it into several minibatches. So there is no duplicate elements across all the batches. This method will make our previous convergence proof absolutely not hold, because everything the previous proof built on was we assumed each batch is drawn independently, so all those expectations only depend on w_t. However, now the expectation will depend on a whole lot of stuff - what was the previous batch, the order they were drawn, ...\n\nThe term related to random shuffling is epoch, which means 1 pass through the whole dataset.\n\nWe have other methods for doing minibatch SGD:\n\nwithout replacement within batch level, so there can be duplicates across different batches\n\nshuffle-once: only shuffle the dataset once before start training, so each epoch has the same order","type":"content","url":"/2022-09-21-stochastic-gradient-descent-improved#minibatch-sgd-in-practice","position":9},{"hierarchy":{"lvl1":"Stochastic Gradient Descent Improved","lvl2":"Improve: Non-Constant Learning Rate on PL Function"},"type":"lvl2","url":"/2022-09-21-stochastic-gradient-descent-improved#improve-non-constant-learning-rate-on-pl-function","position":10},{"hierarchy":{"lvl1":"Stochastic Gradient Descent Improved","lvl2":"Improve: Non-Constant Learning Rate on PL Function"},"content":"Now instead of having a constant learning rate \\alpha, we will have an adaptive (usually diminishing) learning rate specific for each step: \\alpha_t\\rho_{t+1}\n\\le (1 - \\alpha_t \\mu)   \\rho_t + \\frac{\\alpha_t^2 \\sigma^2 L}{2B}\n\nThink of \\rho_{t+1} as a function of \\alpha_t\\rho_{t+1} = g(\\alpha_t)\n= (1 - \\alpha_t \\mu)   \\rho_t + \\frac{\\alpha_t^2 \\sigma^2 L}{2B}\n\nSince we want \\rho_{t+1} to be as small as possible, we take derivative of g with respect to \\alpha_t, we solve g'(\\alpha_t) = 0, so see value of \\alpha when \\rho_{t+1} reaches its minimum.\\alpha_t = \\frac {\\rho_t \\mu B} {\\sigma^2 L}\n\nSubstitute the \\alpha_t into the above inequality and take its inverse\\begin{align}\n\\rho_{t+1} &\\le \\rho_t - \\frac {\\mu^2 B} {2 \\sigma^2 L} \\rho_t^2 \\\\\n\\frac 1 {\\rho_{t+1}} & \\ge \\frac 1 {\\rho_t - \\frac {\\mu^2 B} {2 \\sigma^2 L} \\rho_t^2} \\\\\n&= \\frac 1 {\\rho_t} (1  - \\frac {\\mu^2 B} {2 \\sigma^2 L} \\rho_t)^{-1} \\\\\n&\\ge \\frac 1 {\\rho_t} (1  + \\frac {\\mu^2 B} {2 \\sigma^2 L} \\rho_t) \\\\\n&= \\frac 1 {\\rho_t} + \\frac {\\mu^2 B} {2 \\sigma^2 L} \\\\\n\\end{align}\n\nThen we use the fact \\forall x \\lt 1, \\; (1-x)^{-1} \\ge 1+x. We can say \\frac {\\mu^2 B} {2 \\sigma^2 L} \\rho_t \\lt 1 because we can make \\rho_t really small. To do this, at the beginning of the training stage, we first run minibatch SGD at a constant step size for several epochs. In these constant \\alpha runs, we don’t care about this property, so nothing is violated. At the same time, we can make \\rho_t relatively small so we are ready to switch to a diminishing step size while making sure \\frac {\\mu^2 B} {2 \\sigma^2 L} \\rho_t \\lt 1 holds.\\begin{align}\n\\frac 1 {\\rho_{t+1}} & \\ge \\frac 1 {\\rho_t} (1  - \\frac {\\mu^2 B} {2 \\sigma^2 L} \\rho_t)^{-1} \\\\\n&\\ge \\frac 1 {\\rho_t} (1  + \\frac {\\mu^2 B} {2 \\sigma^2 L} \\rho_t) \\\\\n&= \\frac 1 {\\rho_t} + \\frac {\\mu^2 B} {2 \\sigma^2 L} \\\\\n\\end{align}\n\nNow look at a T iterations:\\begin{align}\n\\frac 1 {\\rho_{T}}\n&\\ge \\frac 1 {\\rho_0} + T \\frac {\\mu^2 B} {2 \\sigma^2 L} \\\\\n\\rho_T = \\mathbf{E}\\left[f(w_t) - f^* \\right] &\\le (\\frac 1 {f(w_0) - f^*} + T \\frac {\\mu^2 B} {2 \\sigma^2 L})^{-1}\\\\\n&= \\mathcal O(\\frac {\\sigma^2 L} {T \\mu^2 B})\\\\\n&= \\mathcal O(\\frac 1 T)\n\\end{align}\n\nNow we have sort of a value of \\rho_t, or at least we now know how it changes. Recall we solved what value of \\alpha can make the next iteration as good as possible.\\alpha_t = \\frac {\\rho_t \\mu B} {\\sigma^2 L} \\propto \\rho_t \\propto \\frac 1 T\n\nTherefore, we conclude that we should make \\alpha_t changes proportional to step’s inverse.","type":"content","url":"/2022-09-21-stochastic-gradient-descent-improved#improve-non-constant-learning-rate-on-pl-function","position":11},{"hierarchy":{"lvl1":"SGD with Momentum"},"type":"lvl1","url":"/2022-09-26-sgd-with-momentum","position":0},{"hierarchy":{"lvl1":"SGD with Momentum"},"content":"Recall from previous lecture, the running time of GD on a strongly convex (PL-condition) function depends only on condition number \\kappa. When the condition number is high, convergence can become slow.T \\ge \\kappa \\log\\left( \\frac{f(w_0) - f^*}{\\epsilon} \\right)\n\nSo how can we speed up gradient descent when the condition is high? There are three common solutions:\n\nMomentum (this lecture)\n\nAdaptive learning rates\n\nPreconditioning\n\nWe introduce momentum here. A direct analysis would be messy, so we use a very simple example to give some intuition.","type":"content","url":"/2022-09-26-sgd-with-momentum","position":1},{"hierarchy":{"lvl1":"SGD with Momentum","lvl2":"Simple Quadratic Function"},"type":"lvl2","url":"/2022-09-26-sgd-with-momentum#simple-quadratic-function","position":2},{"hierarchy":{"lvl1":"SGD with Momentum","lvl2":"Simple Quadratic Function"},"content":"The simplest possible setting with a non-1 condition number is a 2D quadratic. To get a high condition number, we want in the Hessian matrix, the biggest (bigger) value to be very big and the smallest (smaller) to be very small. Consider the following example with a \\gt b. Here a is just the curvature of the first dimension and b is the curvature of the second dimension.f(w) = f(w_1, w_2) = \\frac{a}{2} w_1^2 + \\frac{b}{2} w_2^2 = \\frac{1}{2} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}^T \\begin{bmatrix} a & 0 \\\\ 0 & b \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}\n\nThe second derivative matrix is the constant\\nabla^2 f(w) = \\begin{bmatrix} a & 0 \\\\ 0 & b \\end{bmatrix}\n\nBy definition of a condition number in linear algebra, we know \\kappa = \\frac a b. By its definition in ML optimization problem, we know \\kappa = \\frac L \\mu, so we can just set a = L, b = \\mu, so\\begin{align}\nf(w) = \\frac{L}{2} w_1^2 + \\frac{\\mu}{2} w_2^2 &= \\frac{1}{2} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}^T \\begin{bmatrix} L & 0 \\\\ 0 & \\mu \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\\\\n\\nabla f(w) &= \\begin{bmatrix} L & 0 \\\\ 0 & \\mu \\end{bmatrix} w\\\\\n\\nabla^2 f(w) &= \\begin{bmatrix} L & 0 \\\\ 0 & \\mu \\end{bmatrix}\n\\end{align}\n\nWith this, we can write our update step as\\begin{align}\n  w_{t+1} &= w_t - \\alpha \\nabla f(w) \\\\\n  &= w_t - \\alpha \\begin{bmatrix} L & 0 \\\\ 0 & \\mu \\end{bmatrix} w_t \\\\\n  &= \\begin{bmatrix} 1 - \\alpha L & 0 \\\\ 0 & 1- \\alpha \\mu \\end{bmatrix} w_t\n\\end{align}\n\nIf we run the update T steps, we will have (from now on, T will most likely denote number of iterations instead of transpose.)\\begin{align}\n  w_{T} &= \\begin{bmatrix} 1 - \\alpha L & 0 \\\\ 0 & 1- \\alpha \\mu \\end{bmatrix}^T w_0 \\\\\n  &= \\begin{bmatrix} (1 - \\alpha L)^T & 0 \\\\ 0 & (1- \\alpha \\mu)^T \\end{bmatrix} w_0 \\\\\n  & = \\begin{bmatrix} (1 - \\alpha L)^{T} (w_0)_1  \\\\ (1 - \\alpha \\mu)^{T} (w_0)_2  \\end{bmatrix}\n\\end{align}\n\nFeed it into f, we havef(w_T) = \\frac{L}{2} (1 - \\alpha L)^{2T} \\left( (w_0)_1 \\right)^2 + \\frac{\\mu}{2} (1 - \\alpha \\mu)^{2T} \\left( (w_0)_2 \\right)^2\n\nTherefore, the final value of f(w_t) will be dominated by the exponential term: one of (1 - \\alpha L)^{2T} or (1 - \\alpha \\mu)^{2T}\n\nTo minimize f(w_t), we have to minimize | 1 - \\alpha L| and |1 - \\alpha \\mu| at the same time. That is to minimize \\max(| 1 - \\alpha L|, |1 - \\alpha \\mu|) Note if we just look at the first dimension, so we just minimize | 1 - \\alpha L|, we set \\alpha = \\frac 1 L . With L \\gt \\mu, this will always be a small number. Therefore, for the first dimension with a respective larger curvature L, we want a smaller learning rate. On the other hand, if we only look at the second dimension, where the respective curvature is the smaller \\mu, we should want a higher learning rate.\n\nNow that we have to minimize both at the same time, we are forced to choose something in the middle. We will always reach minimum when we have \\alpha L - 1 = 1 - \\alpha \\mu, so \\alpha = \\frac 2 {L + \\mu}. Substitute this \\alpha in,\\max(|{1 - \\alpha L}|, |{1 - \\alpha \\mu}|) = \\frac{L - \\mu}{L + \\mu} = \\frac{\\kappa - 1}{\\kappa + 1} = 1 - \\frac{2}{\\kappa + 1}\n\nAs we said, the bigger of these two terms will dominate the final value of f(w_T), sof(w_T) = \\mathcal O\\left( (1 - \\frac{2}{\\kappa + 1})^{2T} \\right)\n\nWe know 1-x \\approx e^{-x} around x = 1, so (here 1 - \\frac{2}{\\kappa + 1} \\approx 1 with \\kappa being large)f(w_T) = \\mathcal O\\left( exp(- \\frac{4T}{\\kappa + 1}) \\right)\n\nTherefore, even in the simplest setting, we can’t get rid of this \\frac {} {\\kappa + 1} term.","type":"content","url":"/2022-09-26-sgd-with-momentum#simple-quadratic-function","position":3},{"hierarchy":{"lvl1":"SGD with Momentum","lvl2":"Polyak Momentum"},"type":"lvl2","url":"/2022-09-26-sgd-with-momentum#polyak-momentum","position":4},{"hierarchy":{"lvl1":"SGD with Momentum","lvl2":"Polyak Momentum"},"content":"We want to sort of detect high vs low curvature during GD. The idea is:\n\nIf this dim has high curvature, we tend to overshoot, so in next step, gradient will point in the opposite direction\n\nIf this dim has low curvature, we are more likely to stay in the same direction, which means the step size is too small\n\nTherefore, we want to make steps smaller when gradients reverse sign and larger when gradients are consistently in the same direction. Polyak momentum does thisw_{t+1} = w_t - \\alpha \\nabla f(w_t) + \\beta (w_t - w_{t-1})\n\nThe intuition is that\n\nIf the current gradient is in the same direction as the previous step, move a little further in the same direction\n\nIf it’s in the opposite direction, move a little less far\n\nThis is equivalent to\\begin{align*}\n  w_{t+1} &= w_t - \\alpha \\nabla f(w_t) + \\beta (w_t - w_{t-1})\\\\\n  w_{t+1} - w_t &=  - \\alpha \\nabla f(w_t) + \\beta (w_t - w_{t-1})\\\\\n  m_{t+1} &= -\\alpha \\nabla f(w_t) + \\beta m_t\\\\\n  w_{t+1} &= w_t + m_{t+1}\n\\end{align*}","type":"content","url":"/2022-09-26-sgd-with-momentum#polyak-momentum","position":5},{"hierarchy":{"lvl1":"SGD with Momentum","lvl2":"Stage Transition Matrix"},"type":"lvl2","url":"/2022-09-26-sgd-with-momentum#stage-transition-matrix","position":6},{"hierarchy":{"lvl1":"SGD with Momentum","lvl2":"Stage Transition Matrix"},"content":"Go back to our simple example, denote A = \\begin{bmatrix} L & 0 \\\\ 0 & \\mu \\end{bmatrix}, so\\begin{align}\nw_{t+1} &= w_t - \\alpha A w_t + \\beta (w_t - w_{t-1})\n\\end{align}\n\nWe can write this update process as a matrix operation too. The matrix here will be a block matrix, where each entry is actually a 2 \\times 2 matrix.\\begin{align}\n\\begin{bmatrix} w_{t+1} \\\\ w_t \\end{bmatrix} & = \\begin{bmatrix} w_t - \\alpha A w_t + \\beta (w_t - w_{t-1}) \\\\ w_t \\end{bmatrix}\\\\\n& = \\begin{bmatrix} (1 + \\beta) I - \\alpha A  & -\\beta I \\\\ I &0 \\end{bmatrix} \\begin{bmatrix} w_{t} \\\\ w_{t-1} \\end{bmatrix}\\\\\n\\begin{bmatrix} w_{T+1} \\\\ w_T \\end{bmatrix}\n& = \\begin{bmatrix} (1 + \\beta) I - \\alpha A  & -\\beta I \\\\ I &0 \\end{bmatrix}^T \\begin{bmatrix} w_{1} \\\\ w_{0} \\end{bmatrix}\n\\end{align}\n\nThis block matrix in whole is actually a 4 \\times 4 matrix that transforms a vector in \\R^4 to \\R^4, so if we write in the basis form\\begin{bmatrix} w_{1} \\\\ w_{0} \\end{bmatrix} = c_1u_1 + c_2u_2 + c_3u_3 + c_4u_4\n\nAnd if we find the eigenvalues of this block matrix, denote them as \\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4, we can write\\begin{bmatrix} w_{T+1} \\\\ w_T \\end{bmatrix} = \\lambda_1^Tc_1u_1 + \\lambda_2^Tc_2u_2 + \\lambda_3^Tc_3u_3 + \\lambda_4^Tc_4u_4\n\nAs before, what dominates the value of f(w_t) will be the biggest among the exponential \\lambda^T term. Therefore, we want to minimize all these eigenvalues at the same time.\n\nIn addition, recall the square term in f(w)f(w_T) = \\frac{L}{2} w_{T1}^2 + \\frac{\\mu}{2} w_{T2}^2\n\nTherefore, we actually only care about the magnitude of w_T. That is, we want to minimize the magnitude of all these eigenvalues at the same time.\\begin{align}\n&\\min \\|w_T\\| \\\\\n\\iff &\\min_{\\lambda_{1,2,3,4}} \\; \\max \\left(\\| \\lambda_1^Tc_1u_1 \\|, \\|\\lambda_2^Tc_2u_2\\| , \\|\\lambda_3^Tc_3u_3\\| , \\|\\lambda_4^Tc_4u_4 \\| \\right)\\\\\n\\iff &\\min_{\\lambda_{1,2,3,4}} \\max \\left( \\| \\lambda_1^T \\|, \\|\\lambda_2^T\\| , \\|\\lambda_3^T\\| , \\|\\lambda_4^T \\|\\right)\n\\end{align}","type":"content","url":"/2022-09-26-sgd-with-momentum#stage-transition-matrix","position":7},{"hierarchy":{"lvl1":"SGD with Momentum","lvl2":"Analyzing Eigenvalues"},"type":"lvl2","url":"/2022-09-26-sgd-with-momentum#analyzing-eigenvalues","position":8},{"hierarchy":{"lvl1":"SGD with Momentum","lvl2":"Analyzing Eigenvalues"},"content":"We start to analyze the eigenvalues of this block matrix.\n\nWe write this block matrix out:\\begin{bmatrix}\n1+\\beta - \\alpha L & 0 & -\\beta & 0\\\\\n0 & 1+\\beta - \\alpha \\mu & 0 & -\\beta\\\\\n1 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0\n\\end{bmatrix}\n\nSince we are analyzing the eigenvalues and the basis are not of any importance, we swap the 2nd column with 3rd column and swap 2nd row with 3rd row. This is equivalent to swap the 2nd and 3rd basis in domain vector space and also swap 2nd and 3rd basis in the codomain space. Again, this is fine because we only care about the eigenvalues.\\begin{bmatrix}\n1+\\beta - \\alpha L & -\\beta & 0 & 0\\\\\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 1+\\beta - \\alpha \\mu & -\\beta\\\\\n0 & 0 & 1 & 0\n\\end{bmatrix}\n\nWe write this new matrix also in block matrix form,\\begin{bmatrix}\nB & 0 \\\\\n0 & B\n\\end{bmatrix},\n\\text{where } B =\n\\begin{bmatrix}\n1+\\beta - \\alpha \\chi & -\\beta \\\\\n1 & 0 \\\\\n\\end{bmatrix},\n\\chi = \\text{$L$ or $\\mu$ repsectively}\n\nThis new block matrix is also in diagonal form, so to solve for its eigenvalues, we just have to solve for B’s eigen values.\n\nRecall that we want to minimize all of eigen values’ norms all at the same time. We achieve this when they all have the same norm. For B specifically, it has two eigenvalues \\lambda_1, \\lambda_2 and we want |\\lambda_1| = |\\lambda_2|.\n\nWrite out the characteristic polynomial of matrix B\\begin{align}\ndet(B - \\lambda I)\n&= 0\\\\\ndet(B - \\lambda I) &= det\\left( \\begin{bmatrix}\n1+\\beta - \\alpha \\chi  - \\lambda & -\\beta \\\\\n1 & -\\lambda \\\\\n\\end{bmatrix} \\right) \\\\\n&= \\lambda \\left( \\lambda - (1 + \\beta - \\alpha \\chi) \\right) + \\beta \\\\\n&= \\lambda^2 - (1 + \\beta - \\alpha \\chi) \\lambda + \\beta\n\\end{align}\n\nSolve this quadratic equation, we have\\lambda   =   \\frac{     (1 + \\beta - \\alpha \\chi)     \\pm     \\sqrt{       (1 + \\beta - \\alpha \\chi)^2       -       4 \\beta     }   }{     2   }\n\nTwo solutions to this equation have the same norm when we have them to be the same or they are complex numbers. That is when(1 + \\beta - \\alpha \\chi)^2 - 4 \\beta \\le 0\n\nTo find the exact value of |\\lambda|, we don’t have to go through the process of finding norm of a complex number. In fact, we just recall that product of all eigenvalues is equal to the determinant of this matrix, so\\begin{align}\n\\lambda^2 &= det(B) = \\beta\\\\\n|\\lambda|  &= \\sqrt \\beta\n\\end{align}\n\nTherefore, to minimize |\\lambda|, we actually need to minimize \\sqrt \\beta. So we have this new linear optimization problem to solve:\\begin{align}\n&\\min_{\\alpha, \\beta} \\sqrt \\beta & \\\\\n&\\textrm{s.t. }\n\\begin{matrix}\n(1 + \\beta - \\alpha L)^2 - 4 \\beta \\le 0 \\\\\n(1 + \\beta - \\alpha \\mu)^2 - 4 \\beta \\le 0\n\\end{matrix}\n\\end{align}\n\nThis is a special case of Karush–Kuhn–Tucker conditions (KTT). When we solve a linear programming problem of k variables and n inequalities, k of these n inequalities will actually hold as equality. In this case, this minimization problem is solved when 2 of these 2 inequality constraints achieve equality. . So we solve(1 + \\beta - \\alpha L)^2 - 4 \\beta = 0 \\\\\n(1 + \\beta - \\alpha \\mu)^2 - 4 \\beta = 0\n\nwhich gives us\\begin{align}\n2 \\sqrt{\\beta} &= | 1 + \\beta - \\alpha L | \\\\\n&= | 1 + \\beta - \\alpha \\mu |\n\\end{align}\n\nSince we have L \\gt \\mu, to make the two absolute values to be equal, we must have\\begin{align}\n-2 \\sqrt{\\beta} &= 1 + \\beta - \\alpha L  \\\\\n2 \\sqrt{\\beta} &= 1 + \\beta - \\alpha \\mu\n\\end{align}\n\nSolve for both \\alpha and \\beta, we have\\alpha = \\frac{2 + 2 \\beta}{L + \\mu}\n  \\hspace{1em}\\text{ and }\\hspace{1em}\n  \\sqrt{\\beta}\n  =\n  \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\n  =\n  1 - \\frac{2}{\\sqrt{\\kappa} + 1}.\n\nRecall that the norm of w_t will be dominated with the one with max eigenvalue (C = \\max \\|c\\|)w_t = \\max_{i \\in [1,\\dots,4]} \\| \\lambda_i^T c_i u_i\\|\n = \\max_{i \\in [1,\\dots,4]} \\| \\lambda_i^T\\|C\n = \\sqrt \\beta ^ T C\n\nTherefore, we have (C' = \\frac {\\max(L, \\mu)} 2 C^2)\\begin{align}\nf(w_t) &= C' |w_t|^2 \\\\\n&= C' \\sqrt \\beta ^{2T} \\\\\n&= C' (1 - \\frac 2 {\\sqrt \\kappa +1})^{2T}\\\\\n&\\le C' \\exp(- \\frac {2 \\cdot 2T} {\\sqrt \\kappa +1})\n\\end{align}\n\nTherefore, our function converges to a given interval \\epsilon (remember this is a quadratic function), when we have T satisfies the following conditionf(w_T) \\le \\epsilon \\\\\nT \\ge \\frac{\\sqrt{\\kappa} + 1}{4} \\log (\\frac C \\epsilon)\n\nSo\\mathcal O (T) = \\sqrt \\kappa\n\nRecall a normal GD convergence rate is\\mathcal O (T) = \\kappa\n\nTherefore, we have shown that using momentum with GD on this simple example of quadratic function has a faster convergence rate than a vanilla GD. The result does generalize to more general cases with other kinds of functions.\n\nWhen we use momentum with SGD, it is not guaranteed that it gives a better result, but people still use it.","type":"content","url":"/2022-09-26-sgd-with-momentum#analyzing-eigenvalues","position":9},{"hierarchy":{"lvl1":"SGD with Momentum","lvl2":"Nesterov Momentum"},"type":"lvl2","url":"/2022-09-26-sgd-with-momentum#nesterov-momentum","position":10},{"hierarchy":{"lvl1":"SGD with Momentum","lvl2":"Nesterov Momentum"},"content":"One disadvantage of Polyak momentum is that the momentum term is not guaranteed to point to the right direction. Also, it is only guaranteed to have this nice acceleration for quadratics. Therefore, we introduce Nesterov Momentum, which works for general strongly convex objectives.\n\nPolyak:\\begin{align*}\n  m_{t+1} &= \\beta m_t - \\alpha \\nabla f(w_t) \\\\\n  w_{t+1} &= w_t + m_{t+1}\n\\end{align*}\n\nNesterov:\\begin{align*}\n  m_{t+1} &= \\beta m_t - \\alpha \\nabla f(w_t + \\beta m_t) \\\\\n  w_{t+1} &= w_t + m_{t+1}.\n\\end{align*}\n\nDifference: instead of calculating the momentum term at the current position, we pretend to have already taken one step and calculate the momentum term there.","type":"content","url":"/2022-09-26-sgd-with-momentum#nesterov-momentum","position":11},{"hierarchy":{"lvl1":"Preconditioning and Element Specific Learning Rate"},"type":"lvl1","url":"/2022-09-28-preconditioning-and-element-specific-le","position":0},{"hierarchy":{"lvl1":"Preconditioning and Element Specific Learning Rate"},"content":"Last time, we saw how we can use momentum to speed up GD when \\kappa is large. We reduced the number of iterations from \\mathcal O(n \\kappa \\log \\frac 1 \\epsilon) to \\mathcal O (n \\sqrt \\kappa \\log \\frac 1 \\epsilon). This time, we will see how we can use preconditioning and adaptive learning rates to speed up computation.","type":"content","url":"/2022-09-28-preconditioning-and-element-specific-le","position":1},{"hierarchy":{"lvl1":"Preconditioning and Element Specific Learning Rate","lvl2":"Level Sets - Visualize Condition Number \\kappa"},"type":"lvl2","url":"/2022-09-28-preconditioning-and-element-specific-le#level-sets-visualize-condition-number-kappa","position":2},{"hierarchy":{"lvl1":"Preconditioning and Element Specific Learning Rate","lvl2":"Level Sets - Visualize Condition Number \\kappa"},"content":"We define a level set L_c of a function f: \\R^d \\rightarrow \\R asL_C = \\{ w \\mid f(w) = C \\}\n\nIt is just the contour line of function f at a level C.\n\nTake the same function as last lecture, where\\begin{align}\nf(w_1, w_2) &= \\frac{L}{2} w_1^2 + \\frac{\\mu}{2} w_2^2 &&\\kappa = \\frac L \\mu &&2C = Lw_1^2 + \\mu w_2^2\\\\\nf(w_1, w_2) &= \\frac{1}{2} w_1^2 + \\frac{1}{2} w_2^2 &&\\kappa = 1 &&2C = w_1^2 + w_2^2 \\text{ is a circle}\\\\\nf(w_1, w_2) &= \\frac{10}{2} w_1^2 + \\frac{1}{2} w_2^2 &&\\kappa = 10 &&2C = 10w_1^2 + w_2^2 \\text{ is an eclipse}\n\\end{align}\n\nWe say that the function is well-conditioned when we have a small condition number. And the level set will look like a circle.","type":"content","url":"/2022-09-28-preconditioning-and-element-specific-le#level-sets-visualize-condition-number-kappa","position":3},{"hierarchy":{"lvl1":"Preconditioning and Element Specific Learning Rate","lvl2":"Preconditioning"},"type":"lvl2","url":"/2022-09-28-preconditioning-and-element-specific-le#preconditioning","position":4},{"hierarchy":{"lvl1":"Preconditioning and Element Specific Learning Rate","lvl2":"Preconditioning"},"content":"","type":"content","url":"/2022-09-28-preconditioning-and-element-specific-le#preconditioning","position":5},{"hierarchy":{"lvl1":"Preconditioning and Element Specific Learning Rate","lvl3":"Transforming Space","lvl2":"Preconditioning"},"type":"lvl3","url":"/2022-09-28-preconditioning-and-element-specific-le#transforming-space","position":6},{"hierarchy":{"lvl1":"Preconditioning and Element Specific Learning Rate","lvl3":"Transforming Space","lvl2":"Preconditioning"},"content":"One nice thing to note is that when we stretch the space, we reserve the minimum value of a function. That is, as long as the matrix P is invertible, we have (this is to ensure \\forall w, \\exists u \\; s.t. u = P^{-1}w)\\min_w f(w) = \\min_u f(Pu)\n\nTherefore, if we only look at the minimum point, we have\\operatorname*{argmin}_w f(w) = P\\left(\\operatorname*{argmin}_u f(Pu) \\right)\n\nwhere \\operatorname*{argmin}_u f(Pu) solves for u in the transformed space and we get back w by multiplying it with P\n\nWhile reserving the minimum point, we actually changed how the function looks in everywhere else so the condition number in this new transformed space is also changed (hopefully smaller).","type":"content","url":"/2022-09-28-preconditioning-and-element-specific-le#transforming-space","position":7},{"hierarchy":{"lvl1":"Preconditioning and Element Specific Learning Rate","lvl3":"Problem Setup","lvl2":"Preconditioning"},"type":"lvl3","url":"/2022-09-28-preconditioning-and-element-specific-le#problem-setup","position":8},{"hierarchy":{"lvl1":"Preconditioning and Element Specific Learning Rate","lvl3":"Problem Setup","lvl2":"Preconditioning"},"content":"Define g(u) = f(Pu), we want to solve \\operatorname*{argmin}_u g(u) and map the result back to w = Pu\n\nFor example, take A as a symmetric positive definite matrix with all positive eigenvalues and definef(w) = \\frac 1 2 w^T A w\n\nWe can actually transform A to I as the Hessian matrix in another space so we can have a perfect condition number \\kappa = 1. To do this, we want to find a P such that P^TAP = I, so\\begin{align}\nf(Pu) &= \\frac 1 2 (Pu)^T A (Pu)\\\\\ng(u) &= \\frac 1 2 uIu\\\\\n\\end{align}\n\nSuppose P is also symmetric. Solve for P, we haveP^{-2} = A\n\nWe can denote P=A^{- \\frac 1 2}. This is just a notation and doesn’t have practical computation meaning.","type":"content","url":"/2022-09-28-preconditioning-and-element-specific-le#problem-setup","position":9},{"hierarchy":{"lvl1":"Preconditioning and Element Specific Learning Rate","lvl3":"In-place Transformation","lvl2":"Preconditioning"},"type":"lvl3","url":"/2022-09-28-preconditioning-and-element-specific-le#in-place-transformation","position":10},{"hierarchy":{"lvl1":"Preconditioning and Element Specific Learning Rate","lvl3":"In-place Transformation","lvl2":"Preconditioning"},"content":"So far, we first transform the problem to another space, solve it at that space, and transform the solution back to original space. Can we do everything in the original space, but pretend we are in the transformed space when we do the GD update?\n\nImagine we are again solving the minimization problem on the transformed function g(u)u_{t+1} = u_t - \\alpha \\nabla g(u_t)\n\nTo find out the value of \\nabla g(u_t), we go through a similar derivation as we did in \n\nGradient Descent Appendix: First noteg(u + \\eta v) = f(P (u + \\eta v)) = f(Pu + \\eta Pv)\n\nWe then look at the definition of gradient of g\\nabla_v g(u) = \\lim_{\\eta \\to 0} \\frac{d}{d \\eta} g(u + \\eta v) = \\lim_{\\eta \\to 0} \\frac{d}{d \\eta} f(Pu + \\eta Pv) = \\nabla_{Pv} f(Pu) = (Pv)^T \\nabla f(Pu) = v^T P^T \\nabla f(Pu)\n\nSubstitute the \\nabla g(u) with this equationu_{t+1} = u_t - \\alpha P^T  \\nabla f(P u_t).\n\nTo get back w_t, recall w_t = P u_t so we just multiply both sides by P, we get\\begin{align}\nw_{t+1} &= P u_{t+1} \\\\\n&= P u_t - \\alpha P P^T  \\nabla f(P u_t) \\\\\n&= w_t - \\alpha P P^T \\nabla f(w_t)\\\\\n&= w_t - \\alpha R \\nabla f(w_t)\n\\end{align}\n\nTherefore, this is just Gradient Descent with gradients scaled by a positive semidefinite matrix R = PP^T. We call this matrix R the preconditioner. In practice, we won’t labor ourselves finding P, we just find some positive semidefinite R, because we know any such matrix can be decomposed into R = PP^T form.\n\nIf we relate to our previous simple example, R = P^2 = A^{-1}\n\nA weird question: despite this derivation, what happens if we use a non-positive semidefinite matrix here? Let’s just imagine a very simple diagonal -1 matrix. It will simply blow up w_t by directing it to go to the step where f(w_t) increases at each step.","type":"content","url":"/2022-09-28-preconditioning-and-element-specific-le#in-place-transformation","position":11},{"hierarchy":{"lvl1":"Preconditioning and Element Specific Learning Rate","lvl3":"Finding Transformation","lvl2":"Preconditioning"},"type":"lvl3","url":"/2022-09-28-preconditioning-and-element-specific-le#finding-transformation","position":12},{"hierarchy":{"lvl1":"Preconditioning and Element Specific Learning Rate","lvl3":"Finding Transformation","lvl2":"Preconditioning"},"content":"How do we find this R then?\n\nuse statistics from the dataset: For example, for a linear model you could precondition based on the variance of the features in your dataset. Note this is is almost the same as normalization: we both want to transform our problem into some easier domain. One slight difference is that preconditioning scales regularizer too, while normalization doesn’t. This really doesn’t matter in practice.\n\nuse information from the matrix of second-partial derivatives. For example, you could use a preconditioning matrix that is a diagonal approximation of the Newton’s method update at some point. This is similar to what we did in the quadratic function example. These methods are sometimes called Newton Sketch methods.","type":"content","url":"/2022-09-28-preconditioning-and-element-specific-le#finding-transformation","position":13},{"hierarchy":{"lvl1":"Preconditioning and Element Specific Learning Rate","lvl2":"Element Specific Learning Rate"},"type":"lvl2","url":"/2022-09-28-preconditioning-and-element-specific-le#element-specific-learning-rate","position":14},{"hierarchy":{"lvl1":"Preconditioning and Element Specific Learning Rate","lvl2":"Element Specific Learning Rate"},"content":"Note this is computationally expensive: previously each update only takes \\mathcal O(d+t), where t is the time computing gradients. Now that we have a matrix multiplication term, the time has become \\mathcal O(d^2 + t). In addition, we’ll have to store a d \\times d matrix in memory. As we discussed before, this is really bad when we had high dimensional data.\n\nTherefore, we want to find an R that is a diagonal matrix so that when we perform this matrix multiplication, it would work as two vector doing an elementwise multiplication and it can keep our running time at \\mathcal O(d + t)\n\nIf we have such a diagonal matrix, the update step on i-th index will look like:\\begin{align}\n(w_{t+1})_i &= (w_t)_i - \\alpha R_{ii} (\\nabla f(w_t))_i\\\\\n&= (w_t)_i - \\alpha'_i (\\nabla f(w_t))_i\\\\\n\\end{align}\n\nWe actually have an element specific learning rate \\alpha'_i = \\alpha R_{ii} we can also rewrite it in vector form (added the vector sign to denote \\alpha is actually a vector)\\vec{w_{t+1}} = \\vec{w_t} - \\vec \\alpha \\nabla f(w_t)","type":"content","url":"/2022-09-28-preconditioning-and-element-specific-le#element-specific-learning-rate","position":15},{"hierarchy":{"lvl1":"Preconditioning and Element Specific Learning Rate","lvl2":"Adaptive Learning Rate"},"type":"lvl2","url":"/2022-09-28-preconditioning-and-element-specific-le#adaptive-learning-rate","position":16},{"hierarchy":{"lvl1":"Preconditioning and Element Specific Learning Rate","lvl2":"Adaptive Learning Rate"},"content":"However, if we make \\alpha a hyperparameter, we would have to choose d hyperparameters, which is a lot. Imagine when we want our model to be optimal so we have to do a hyperparameter search, this many hyperparameters are impossible to search through. Therefore, we want to find a way to change \\alpha intelligently.\n\nMaybe we can keep a running sum of gradient square at each dimension? That’ll be our topic for next class.","type":"content","url":"/2022-09-28-preconditioning-and-element-specific-le#adaptive-learning-rate","position":17},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction"},"type":"lvl1","url":"/2022-10-03-adaptive-learning-rate-and-variance-red","position":0},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction"},"content":"All operations in this section are element-wise.","type":"content","url":"/2022-10-03-adaptive-learning-rate-and-variance-red","position":1},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl2":"AdaGrad"},"type":"lvl2","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#adagrad","position":2},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl2":"AdaGrad"},"content":"Set the learning rate to be inversely proportional to square root of the sum of all gradients we observed so far squared.\\alpha' = \\frac \\alpha {\\sqrt{g_1^2 + g_2^2 + \\dots + g_t^2}}","type":"content","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#adagrad","position":3},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl3":"Algorithm","lvl2":"AdaGrad"},"type":"lvl3","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#algorithm","position":4},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl3":"Algorithm","lvl2":"AdaGrad"},"content":"Input scalar global learning rate factor \\alpha \\gt 0\n\nInitialize w \\in \\R^d, r \\leftarrow 0 \\in \\R^d\n\nloop\n\nget example gradient g \\in \\R^d\n\naccumulate second moment estimate r \\leftarrow r + g^2\n\nupdate model w \\leftarrow w - \\frac{\\alpha}{\\sqrt{r}} \\cdot g\n\nAdaGrad is an algorithm independent of what kind of Gradient Descent we use. The g can be calculated on a specific example, a minibatch, or the whole dataset.","type":"content","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#algorithm","position":5},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl3":"Advantage","lvl2":"AdaGrad"},"type":"lvl3","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#advantage","position":6},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl3":"Advantage","lvl2":"AdaGrad"},"content":"If we look at the i-th dimension:r_i \\approx T \\mathbb E[g_i^2] = T \\left (\\mathbb E[g_i]^2 + Var(g_i) \\right)\n\nImagine we’re near convergence, so \\mathbb E[g_i]^2 should be small. Then the dominant term would be Var(g_i), so we set our learning rate to be small when this term is big, and big when this term is small.\n\nIt has a diminishing LR that is proportional to \\frac 1 {\\sqrt T}\\alpha' = \\frac \\alpha {\\sqrt r} \\approx  \\frac \\alpha {\\sqrt {T\\mathbb E[g_i^2]}}\n\nWhen AdaGrad was popular, people only focused on strongly convergence problem. With these problems, as we discussed in \n\nSGD on PL function, people thought it was optimal to have a diminishing LR proportional to \\frac 1 T. AdaGrad sort of achieves that.","type":"content","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#advantage","position":7},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl3":"Downside","lvl2":"AdaGrad"},"type":"lvl3","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#downside","position":8},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl3":"Downside","lvl2":"AdaGrad"},"content":"The LR monotonically diminishes, so if we have a non-convex problem and there are a lot of peaks and valleys, it will fail. Because such situation requires it to sometimes employ a high LR and sometimes a low LR.","type":"content","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#downside","position":9},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl2":"RMSProp"},"type":"lvl2","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#rmsprop","position":10},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl2":"RMSProp"},"content":"Modifies AdaGrad to use an exponential moving average instead of a sum, so\\alpha' \\approx \\frac \\alpha {\\bar{g_{1:t}} }","type":"content","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#rmsprop","position":11},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl3":"Algorithm","lvl2":"RMSProp"},"type":"lvl3","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#algorithm-1","position":12},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl3":"Algorithm","lvl2":"RMSProp"},"content":"Input scalar global learning rate factor \\alpha \\gt 0 and decay rate \\rho \\in (0,1)\n\nInitialize w \\in \\R^d, r \\leftarrow 0 \\in \\R^d\n\nloop\n\nget example gradient g \\in \\R^d\n\naccumulate second moment estimate r \\leftarrow \\rho r + (1 - \\rho) g^2\n\nupdate model w \\leftarrow w - \\frac{\\alpha}{\\sqrt{r}} \\cdot g\n\nNote r_i is the exponential running average of g^2, so r = \\mathbb E[g_i^2]\n\nIn practice, we usually set \\rho = 0.99 / 0.999 so the window size of moving average is 100 / 1000.","type":"content","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#algorithm-1","position":13},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl3":"Advantage","lvl2":"RMSProp"},"type":"lvl3","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#advantage-1","position":14},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl3":"Advantage","lvl2":"RMSProp"},"content":"More effective for non-convex optimization problems","type":"content","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#advantage-1","position":15},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl3":"Exponential Moving Average - Bias Term","lvl2":"RMSProp"},"type":"lvl3","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#exponential-moving-average-bias-term","position":16},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl3":"Exponential Moving Average - Bias Term","lvl2":"RMSProp"},"content":"When we take the exponential moving average r_t of a series \\{x_1, x_2, \\dots, x_t\\} with decay rate \\rho, we follow this recurrence relation:\\begin{align}\nr_0 &= 0\\\\\nr_t &= \\rho r_{t-1} + (1 - \\rho) x_t\n\\end{align}\n\nWe can write out the closed form of r:r_t = \\sum^t_{i=1} \\rho^{t-i}(1-\\rho)x_i\n\nIf we assume the series all have the same value x and take the sum of this geometric series,\\begin{align}\nr_t &= \\sum^t_{i=1} \\rho^{t-i}(1-\\rho)x\\\\\n&= (1-\\rho)x \\sum^t_{i=1} \\rho^{t-i} \\\\\n&= (1-\\rho)x \\frac {1-\\rho^t} {1-\\rho}\\\\\n&= (1-\\rho^t)x\n\\end{align}\n\nIf we take \\rho = 0.999, we see that r_1 = (1-\\rho^1)x = 0.001x, r_2 = (1-\\rho^2)x \\approx 0.002x, which is not even near to the average of these terms. This 1-\\rho^t term here is really annoying and preventing us from getting the correct “average”. We will call it the bias correction term and divide our r_t with it to get the real average:r'_t = \\frac{\\rho r_{t-1} + (1 - \\rho) x_t} {1-\\rho^t}\n\nThe original RMSProp didn’t do this so it performs bad at the first steps","type":"content","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#exponential-moving-average-bias-term","position":17},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl2":"Adam"},"type":"lvl2","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#adam","position":18},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl2":"Adam"},"content":"In short, Adam used a correct average for everything.\n\nAdam is a combination of RMSProp and momentum. It also corrects for bias to estimate the first-order and second-order moments of the gradients","type":"content","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#adam","position":19},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl3":"Algorithm","lvl2":"Adam"},"type":"lvl3","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#algorithm-2","position":20},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl3":"Algorithm","lvl2":"Adam"},"content":"Since time step values actually matter here, we will use step update way instead of value assignment way to describe this algorithm. Note all operations are still elementwise and subscripts mean time step.\n\nInput scalar global learning rate factor \\alpha \\gt 0 and decay rates \\rho_1, \\rho_2 \\in (0,1)\n\nInitialize weight w \\in \\R^d, first moment and second moment estimator s, r \\leftarrow 0 \\in \\R^d, time step t \\leftarrow 0\n\nloop:\n\nWe are currently updating for time step t+1\n\nget example gradient g_{t+1} \\in \\R^d\n\naccumulate first and second moment estimate with bias correction:s_{t+1} = \\frac{\\rho_1 s_t + (1 - \\rho_1) g_{t+1}}{1 - \\rho_1^t}, \\;\n    r_{t+1} = \\frac{\\rho_2 r_t + (1 - \\rho_2) g_{t+1}^2}{1 - \\rho_2^t}\n\nupdate weightw_{t+1} = w_t - \\frac{\\alpha}{\\sqrt{r_{t+1}}} \\cdot s_{t+1}\n\nIn practice, we also make changes to \\alpha every some number of iterations, but here just treat it as a constant for simplicity.","type":"content","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#algorithm-2","position":21},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl3":"Relate to RMSProp","lvl2":"Adam"},"type":"lvl3","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#relate-to-rmsprop","position":22},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl3":"Relate to RMSProp","lvl2":"Adam"},"content":"If we set \\rho_1=0 so we only look at current gradient, this will be the same as RMSProp, where w_{t+1} = w_t - \\frac{\\alpha}{\\sqrt{r_{t+1}}} \\cdot g_{t+1}","type":"content","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#relate-to-rmsprop","position":23},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl3":"Relate to Momentum","lvl2":"Adam"},"type":"lvl3","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#relate-to-momentum","position":24},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl3":"Relate to Momentum","lvl2":"Adam"},"content":"Note with momentum, we have\\begin{align*}\n  m_{t+1} &= \\beta m_t - \\alpha g_t \\\\\n  w_{t+1} &= w_t + m_{t+1}\n\\end{align*}\n\nIf we discard the \\frac 1 {\\sqrt{r}}  term in Adam,\\begin{align*}\n  s_{t+1} &= \\frac{\\rho_1 s_t + (1 - \\rho_1) g_{t+1}}{1 - \\rho_1^t}\\\\\n  w_{t+1} &= w_t - \\alpha s_{t+1}\n\\end{align*}\n\nso Adam will become the same as Gradient Descent with momentum.","type":"content","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#relate-to-momentum","position":25},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl2":"SVRG - Stochastic Variance Reduction Gradient Descent"},"type":"lvl2","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#svrg-stochastic-variance-reduction-gradient-descent","position":26},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl2":"SVRG - Stochastic Variance Reduction Gradient Descent"},"content":"It is a good algorithm, but only works on convex function so we won’t go into details for it since deep learning is almost always not convex. The basic idea is to reduce the variance of the gradient estimators by using an infrequent full-gradient step.\n\nIt is very powerful on convex function that it reduces the running time of GD from\\underset {GD} { \\mathcal O (n \\kappa \\log{\\frac 1 \\epsilon})}\n\\rightarrow\n\\underset {SVRG} {\\mathcal O\\left((n+\\kappa) \\log{\\frac 1 \\epsilon} \\right)}","type":"content","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#svrg-stochastic-variance-reduction-gradient-descent","position":27},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl2":"Polyak–Ruppert Averaging"},"type":"lvl2","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#polyak-ruppert-averaging","position":28},{"hierarchy":{"lvl1":"Adaptive Learning Rate and Variance Reduction","lvl2":"Polyak–Ruppert Averaging"},"content":"Redefine the model weight as the average of all model weights that we have observed so far.\\bar w_T = \\frac{1}{T} \\sum_{t=1}^T w_t\n\nThe idea is that when we are near to convergence in SGD, we will encounter a noise ball around the optimum. If we take an average of the model weights at the periphery of that noise ball, we can then cancel out the noise and reach the minimum.\n\nOne issue with the simple setup we’ve used above: we are averaging with equal weight iterates from the very start of training, when we have not reached the noise ball. To address this, we vary the original algorithm by first using an initial warm-up period during which we do not average, and then only starting to average after the warm-up period is over.\n\nOn the other hand, if we make two other changes to the original Polyak Averaging algorithm\n\nInstead of using an average that assigns equal weight to all model weights, we use an exponential running average instead.\n\nNo average over all iterations or doing a warm-up period, we use a running window instead.\n\nIt will become the same algorithm as Stochastic Weight Averaging (SWA).","type":"content","url":"/2022-10-03-adaptive-learning-rate-and-variance-red#polyak-ruppert-averaging","position":29},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction"},"type":"lvl1","url":"/2022-10-05-sparsity-and-dimension-reduction","position":0},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction"},"content":"草啊，我搞错把第一页笔记扔了 (￣艸￣)","type":"content","url":"/2022-10-05-sparsity-and-dimension-reduction","position":1},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction","lvl2":"Dimension Reduction"},"type":"lvl2","url":"/2022-10-05-sparsity-and-dimension-reduction#dimension-reduction","position":2},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction","lvl2":"Dimension Reduction"},"content":"So far we’ve talked about what constraints the number of iterations we have to run for an optimization algorithm to converge. However, when we really talk about the running time, we will multiply the number of iterations by the time of doing one gradient update step. Take the case of Gradient Descent:\\underset{\\text{\\#iterations of GD}}{\\mathcal O(n\\kappa \\log{\\frac 1 \\epsilon})}\n\\rightarrow\n\\underset{\\text{running time of GD}}{\\mathcal O(n\\kappa d \\log{\\frac 1 \\epsilon})}\n\nThis is the same for other algorithms too. Therefore, to speed up computation, we also want to make d small. So we introduce dimension reduction. That is, we want to transform our original problem in dimension D into one that has a smaller dimension d\n\nUsually, dimensionality reduction is done using unsupervised learning (learning without using labels y) Some examples are:\n\nRandom projection: choose a d-dimensional linear subspace at random and project onto it.\n\nPrincipal Component Analysis: find the d components of the data with the largest variance (represent the data the best), keep those, and throw out all the other components.\n\nAutoencoders: learn a lower-dimension version of the data using a neural network\n\nLDA, t-SNE, word embeddings...","type":"content","url":"/2022-10-05-sparsity-and-dimension-reduction#dimension-reduction","position":3},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction","lvl3":"Random Projection - JL Transform","lvl2":"Dimension Reduction"},"type":"lvl3","url":"/2022-10-05-sparsity-and-dimension-reduction#random-projection-jl-transform","position":4},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction","lvl3":"Random Projection - JL Transform","lvl2":"Dimension Reduction"},"content":"This is also called the Johnson-Lindenstrauss transform or just JL transform because it is a result of Johnson-Lindenstrauss lemma.\n\nFor an arbitrary error tolerance 0 \\lt \\epsilon \\lt 1 , for arbitrary points x, y \\in \\R^d, for any target dimension d \\gt 8 \\frac {\\log n}{\\epsilon^2}, there exists a linear map / matrix A: \\R^D \\to \\R^d s.t.(1 - \\epsilon) \\| x - y \\|^2 \\le \\| Ax - Ay \\|^2 \\le (1 + \\epsilon) \\| x - y \\|^2.\n\nThis property is so nice exactly because it doesn’t depend on the original dimension D.","type":"content","url":"/2022-10-05-sparsity-and-dimension-reduction#random-projection-jl-transform","position":5},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction","lvl4":"Finding A","lvl3":"Random Projection - JL Transform","lvl2":"Dimension Reduction"},"type":"lvl4","url":"/2022-10-05-sparsity-and-dimension-reduction#finding-a","position":6},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction","lvl4":"Finding A","lvl3":"Random Projection - JL Transform","lvl2":"Dimension Reduction"},"content":"How can we find this matrix A though? We will use a random matrix where each entry is drawn i.i.d. from the normal distribution \\mathcal{N}(0, \\sigma^2)\n\nFor this random matrix A, calculate the expected value of two point’s distance after transformation: (note A is the only thing random here. x, y are just fixed constants)\\begin{align*}\n\\mathbb E \\left[ \\|Ax - Ay\\|^2 \\right]\n&= \\mathbb E \\left[ \\|A(x - y)\\|^2 \\right] \\\\\n&= \\mathbb E \\left[ (x - y)^T A^T A (x - y) \\right]\\\\\n&= (x - y)^T \\mathbb E \\left[A^T A\\right] (x - y)  \\\\\n\\mathbb E \\left[A^T A\\right]_{ij}\n&= \\sum^d_{k=1} \\mathbb E \\left[A^T_{ik} A_{kj} \\right]\\\\\n&= \\sum^d_{k=1} \\mathbb E \\left[A_{ki} A_{kj} \\right]\\\\\n&= \\begin{cases}\n\t\\sum^d_{k=1} \\mathbb E [A_{ki}] \\mathbb E [A_{kj}] &\\text{if $i \\not = j$}\\\\\n\t\\sum^d_{k=1} \\mathbb E [A_{ki}^2]  &\\text{if $i=j$}\n\t\\end{cases}\\\\\n&= \\begin{cases}\n\t\\sum^d_{k=1} 0 \\cdot 0 &\\text{if $i \\not = j$}\\\\\n\t\\sum^d_{k=1} \\sigma^2  &\\text{if $i=j$}\n\t\\end{cases}\\\\\n&= \\begin{cases}\n\t0 &\\text{if $i \\not = j$}\\\\\n\td\\sigma^2  &\\text{if $i=j$}\n\t\\end{cases}\\\\\n\\mathbb E \\left[A^T A\\right] &= d\\sigma^2 I\\\\\n\\mathbb E \\left[ \\|Ax - Ay\\|^2 \\right]  &= d\\sigma^2 \\|x - y\\|^2\n\\end{align*}\n\nFrom this derivation, we see that as long as we set \\sigma^2 = \\frac 1 d, we are guaranteed to draw an A that in expectation meets requirements.","type":"content","url":"/2022-10-05-sparsity-and-dimension-reduction#finding-a","position":7},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction","lvl4":"A Caveat","lvl3":"Random Projection - JL Transform","lvl2":"Dimension Reduction"},"type":"lvl4","url":"/2022-10-05-sparsity-and-dimension-reduction#a-caveat","position":8},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction","lvl4":"A Caveat","lvl3":"Random Projection - JL Transform","lvl2":"Dimension Reduction"},"content":"While using Gaussian random variables is sufficient, it’s not very computationally efficient to generate the matrix A, communicate it, and multiply by it. There are other methods that achieve the same effect, so if you want to use random projection at scale, you should consider using these methods.","type":"content","url":"/2022-10-05-sparsity-and-dimension-reduction#a-caveat","position":9},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction","lvl3":"Autoencoder","lvl2":"Dimension Reduction"},"type":"lvl3","url":"/2022-10-05-sparsity-and-dimension-reduction#autoencoder","position":10},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction","lvl3":"Autoencoder","lvl2":"Dimension Reduction"},"content":"We feed our data point x \\in \\R^D into an encoder to get a latent vector z \\in \\R^d, and feed this latent vector to a decoder to try to restore the original data point \\hat x \\in \\R^D. The training loss is just the mean squared reconstruction loss.\\mathcal L(x, \\hat x) = \\|x - \\hat x\\|^2\n\nThis is a classic example of unsupervised learning where you don’t have label associated with each training input.\n\nWhen we use an autoencoder for dimension compression, after training, we just throw away the decoder and just use the encoder.\n\nAutoencoder has no theoretical guarantee of why it works whatsoever.\n\nChristopher De Sa","type":"content","url":"/2022-10-05-sparsity-and-dimension-reduction#autoencoder","position":11},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction","lvl2":"Sparsity"},"type":"lvl2","url":"/2022-10-05-sparsity-and-dimension-reduction#sparsity","position":12},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction","lvl2":"Sparsity"},"content":"Previously with dimension reduction, we deal with high computational cost brought by high dimension through reducing to a lower dimension. With sparsity, we deal with it by working at the same dimension but with a lower cost. Define:\\operatorname*{density}(x) = \\frac {\\text{\\# nonzero entries of x}} {\\text{size of x}}\n\nSome common ways to make a model sparse:\n\nL1 regularization","type":"content","url":"/2022-10-05-sparsity-and-dimension-reduction#sparsity","position":13},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction","lvl3":"Sparse Vector","lvl2":"Sparsity"},"type":"lvl3","url":"/2022-10-05-sparsity-and-dimension-reduction#sparse-vector","position":14},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction","lvl3":"Sparse Vector","lvl2":"Sparsity"},"content":"If we have a sparse vector, we can represent it using a (non-zero-index, non-zero-value) pair\\begin{bmatrix} 0 & 0 & 0 & 2.8 & 0 & 0 & 3.7 & 0\\end{bmatrix} = \\{(4, 2.8), (7, 3.7) \\}\n\nIn computer, it’ll be just two arrays\n\nindices = [4, 7]\n\nvalues = [2.8, 3.7]\n\nIn this way, we have transformed a vector of 8 float into one length 2 integer array and one length 2 float array.","type":"content","url":"/2022-10-05-sparsity-and-dimension-reduction#sparse-vector","position":15},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction","lvl3":"Sparse Matrix - COO","lvl2":"Sparsity"},"type":"lvl3","url":"/2022-10-05-sparsity-and-dimension-reduction#sparse-matrix-coo","position":16},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction","lvl3":"Sparse Matrix - COO","lvl2":"Sparsity"},"content":"To store a sparse matrix, we use Coordinate List (COO), where we just add another column indices array:\\begin{bmatrix}\n0 & 0 & 5.4 & 0 & 0 & 3.6 \\\\\n0 & 1.7 & 0 & 0 & 0 & 0\n\\end{bmatrix}\n= \\{(1, 3, 5.4), (1, 6, 3.6), (2, 2, 1.7)\\}\n\nIn computer, it’ll be three arrays:\n\nrow = [1, 1, 2]\n\ncolumn = [3, 6, 2]\n\nvalues = [5.4, 3.6, 1.7]\n\nHere each number in matrix is represented by 3 numbers in sparse representation. So we need at least 1/3 density in the original matrix to get memory benefit. In fact, with all the memory overheads, we usually need 1/4 density to get memory benefit.\n\nHowever, this representation is hard to compute: we notice that there is no particular order of the values. The only thing to locate a value is to go through this sparse representation. Therefore, counting this computational overheads, we need 2% density to get real benefit.","type":"content","url":"/2022-10-05-sparsity-and-dimension-reduction#sparse-matrix-coo","position":17},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction","lvl3":"Sparse Matrix Improved - CSR","lvl2":"Sparsity"},"type":"lvl3","url":"/2022-10-05-sparsity-and-dimension-reduction#sparse-matrix-improved-csr","position":18},{"hierarchy":{"lvl1":"Sparsity and Dimension Reduction","lvl3":"Sparse Matrix Improved - CSR","lvl2":"Sparsity"},"content":"So we have an improved version go by Compressed Sparse Row (CSR). It basically stores each row as a sparse vector and concatenate them together, keeping track of the number of non-zero values in each row for quicker lookup using row offset.\\begin{bmatrix}\n0 & 0 & 5.4 & 0 & 0 & 3.6 \\\\\n0 & 1.7 & 0 & 0 & 0 & 0\n\\end{bmatrix}\n= \\{(3, 5.4), (6, 3.6)\\}, \\{(2, 1.7)\\}\n\nStore rows as sparse vectors:\n\nindices: [3, 6] [2]\n\nvalues: [5.4, 3.6] [1.7]\n\nConcat these row vectors with row offset:\n\nrow_offsets = [1, 3]\n\n(column) indices = [3, 6, 2]\n\nvalues = [5.4, 3.6, 1.7]\n\nFor consistency with matrix index, assume these arrays start with 1. s = row_offsets[i] means that the i-th row starts from the s-th element in indices-values pair (and end at the row_offsets[i+1]-th element in the indices-values pair)\n\nFor example, if we want to know what’s in row 1, we look up row_offsets[1] = 1, row_offsets[1+1] = 3 so row 1 starts at the 1st element in indices-values pair and the 3rd element is the first element in the second row, so row 1 is just the 1st and 2nd element in the indices-values pair. So we look at the 1st and 2nd elements and found them to be (3, 5.4) and (6, 3.6). That is in the 1st row, the 3rd column has a non-zero value 5.4 and the 6th column has a non-zero value 3.6\n\nThis representation format allows faster lookup and computation.","type":"content","url":"/2022-10-05-sparsity-and-dimension-reduction#sparse-matrix-improved-csr","position":19},{"hierarchy":{"lvl1":"Neural Network Review"},"type":"lvl1","url":"/2022-10-12-neural-network-review","position":0},{"hierarchy":{"lvl1":"Neural Network Review"},"content":"In a deep neural network, we have all those non linear layers and they contribute to the unconvexity of the loss function. In the optimization regime, when we have some problem as unconvex, we regard it as hard to solve. However in practice, the convex optimization algorithms we have discussed so far do perform well on Neural Networks.\n\nNN tends not to overfit. We have this nice property of double descent that after the regime of overfit, if we continue growing the model size to a super huge model, the testing error will actually go down again (as we see in GPT-3 level model)","type":"content","url":"/2022-10-12-neural-network-review","position":1},{"hierarchy":{"lvl1":"Accelerate DNN Training"},"type":"lvl1","url":"/2022-10-17-accelerate-dnn-training","position":0},{"hierarchy":{"lvl1":"Accelerate DNN Training"},"content":"","type":"content","url":"/2022-10-17-accelerate-dnn-training","position":1},{"hierarchy":{"lvl1":"Accelerate DNN Training","lvl2":"Describing Capacity"},"type":"lvl2","url":"/2022-10-17-accelerate-dnn-training#describing-capacity","position":2},{"hierarchy":{"lvl1":"Accelerate DNN Training","lvl2":"Describing Capacity"},"content":"Capacity of a model informally refers to the ability of the model to fit a wide range of possible functions.\n\nModels with high capacity tend to overfit.\n\nModels with low capacity tend to underfit.\n\nRepresentational Capacity of a model refers to the extent of functions this model can approximate well in theory. Deep neural networks have very high representational capacity. In fact, they’re universal approximators.\n\nEffective Capacity of a model refers to the extent of functions this model can approximate well in practice given a specific learning algorithm and dataset.\n\nFor convex optimization problem, a models’ effective capacity is just the representational capacity since local minimum is a global minimum. But for nonconvex problems, a model’s effective capacity also depends on whether the dataset is representative or whether the learning algorithm is powerful. These two factors can have great effect on effective capacity, though have zero effect on representational capacity.","type":"content","url":"/2022-10-17-accelerate-dnn-training#describing-capacity","position":3},{"hierarchy":{"lvl1":"Accelerate DNN Training","lvl2":"Early Stopping"},"type":"lvl2","url":"/2022-10-17-accelerate-dnn-training#early-stopping","position":4},{"hierarchy":{"lvl1":"Accelerate DNN Training","lvl2":"Early Stopping"},"content":"We stop the training when validation loss starts to get bigger.\n\npatience = K - terminate training when validation loss has no improvement in the past K epochs\n\nmin_delta=0.01 - only an improvement greater than 0.01 is considered as an improvement. Anything lower than that is considered as no improvement.","type":"content","url":"/2022-10-17-accelerate-dnn-training#early-stopping","position":5},{"hierarchy":{"lvl1":"Accelerate DNN Training","lvl2":"Dropout"},"type":"lvl2","url":"/2022-10-17-accelerate-dnn-training#dropout","position":6},{"hierarchy":{"lvl1":"Accelerate DNN Training","lvl2":"Dropout"},"content":"During training, randomly drop some neurons (force their activation value to 0) so they do not count as computation. For these neurons, we use 0 to back propagate their values.\n\nIn this way, we force all neurons to participate in training: imagine we have two neurons A and B. B always outputs a constant and it’s A always changing to approximate all the values. Now we force A to output 0, B can no longer remain constant and it has to also participate in learning.","type":"content","url":"/2022-10-17-accelerate-dnn-training#dropout","position":7},{"hierarchy":{"lvl1":"Accelerate DNN Training","lvl2":"Batch Normalization"},"type":"lvl2","url":"/2022-10-17-accelerate-dnn-training#batch-normalization","position":8},{"hierarchy":{"lvl1":"Accelerate DNN Training","lvl2":"Batch Normalization"},"content":"To avoid great variance in the output of each neuron, we restrain the output of one neuron on a minibatch to have 0 mean and unit standard deviation.\n\nThink of one specific neuron, let u = [u_1, u_2, \\dots, u_B] \\in \\mathbb R^B be its output for all samples in this batch. LetBN(u) = \\frac {u-\\mu} {\\sigma}\n\nwhere \\mu and \\sigma are the mean and standard deviation of u = [u_1, u_2, \\dots, u_B]. After BN layer, it has 0 mean and unit sd.\n\nThis is how we calculate BN during training. In training, we also keep an exponential running average of \\mu and \\sigma^2, so during evaluation, we use these averages as the batch statistics. Batch Norm is usually applied before activation.\n\nThere are two problems with Batch Norm:\n\nWe effectively reduced the representative capacity of our model by constraining mean to only be 0 and sd to only be 1. Note previously, each neuron can have different mean and sd, but now they all have the same.\n\nWe try to solve this problem by adding an affine term to assign a new non-zero mean \\beta and a new non-unit sd \\gamma to the activation value on this batch from this neuron. \\gamma, \\beta are two learnable terms here. (Note \\mu, \\sigma, \\gamma, \\beta are all scalars here)BN(u) = \\frac {u-\\mu} {\\sigma} \\gamma + \\beta\n\nHowever, doesn’t this defeat the purpose of normalizing in the first place? This considered, the above equation is still the common practice though we don’t know why it works\n\nAll previous proofs depend on the fact that calculation of gradient step on each sample in minibatch is independent. However, by adding the mean \\mu and sd \\sigma, we make the update step dependent of each other, since mean and sd depend on all samples. This nullifies all the proofs we did previously ensuring good properties of SGD. Despite this, it performs good and people use it.","type":"content","url":"/2022-10-17-accelerate-dnn-training#batch-normalization","position":9},{"hierarchy":{"lvl1":"Accelerate DNN Training","lvl2":"Residual Block / Skip Connection"},"type":"lvl2","url":"/2022-10-17-accelerate-dnn-training#residual-block-skip-connection","position":10},{"hierarchy":{"lvl1":"Accelerate DNN Training","lvl2":"Residual Block / Skip Connection"},"content":"L0 ---- L1 ---- sigma1 ---- L2 ---- sigma2 --+--\n     |                                       |\n     -----------------------------------------\n\nThe L here are some computation layers and sigma is the non-linear activation layer.\n\nSo why do we add this skip connection? Imagine L2 does something weird that prevent learning (maybe all its weights go to 0, so gradient of this point to earlier layers all go to 0 when we compute it in back propagation). By adding L0 value, we ensure that even if L2 doesn’t work, the whole network can still work (for this particular example, we can then make L0 gradient non-zero even what’s in between all have gradient 0)\n\nThe classic ResNet employs this architecture:L0 ---- conv1 ---- BN1 ---- sigma1 ---- conv2 ---- BN2 --+-- sigma2\n     |                                                   |\n     -----------------------------------------------------# https://github.com/akamaster/pytorch_resnet_cifar10/blob/master/resnet.py/ class BasicBlock\ndef forward(self, x):\n    out = F.relu(self.bn1(self.conv1(x)))\n    out = self.bn2(self.conv2(out))\n    out += self.shortcut(x)\n    out = F.relu(out)\n    return out\n\nPeople don’t usually relate this to RNN because an RNN takes in sequence of inputs and each token of it is passed into the same weight / model.","type":"content","url":"/2022-10-17-accelerate-dnn-training#residual-block-skip-connection","position":11},{"hierarchy":{"lvl1":"Beyond supervised learning"},"type":"lvl1","url":"/2022-10-19-beyond-supervised-learning","position":0},{"hierarchy":{"lvl1":"Beyond supervised learning"},"content":"","type":"content","url":"/2022-10-19-beyond-supervised-learning","position":1},{"hierarchy":{"lvl1":"Beyond supervised learning","lvl2":"Data Augmentation"},"type":"lvl2","url":"/2022-10-19-beyond-supervised-learning#data-augmentation","position":2},{"hierarchy":{"lvl1":"Beyond supervised learning","lvl2":"Data Augmentation"},"content":"When we don’t have enough data to make the model robust, sometimes we do data augmentation.\n\nIt’s mostly widely used in CV, where we do Translation, Shift, Rotation, Scaling, Recoloring. These are just transforming these problems with group actions.\n\nIn NLP, we can do synonym swapping\n\nAlso we do deletion in NLP and cropping in CV, these are like randomly projecting our data to a lower dimension.\n\nAdding Noise is also an approach\n\nYou can think of most data augmentation as adding a soft regularization term to the ERM goal so the model we trained can be invariant to some group action (shift, rotation, ...)","type":"content","url":"/2022-10-19-beyond-supervised-learning#data-augmentation","position":3},{"hierarchy":{"lvl1":"Beyond supervised learning","lvl2":"Semi-Supervised Learning"},"type":"lvl2","url":"/2022-10-19-beyond-supervised-learning#semi-supervised-learning","position":4},{"hierarchy":{"lvl1":"Beyond supervised learning","lvl2":"Semi-Supervised Learning"},"content":"This is when we have a lot of unlabeled data and only some labeled data. There are several ways to get around this and we make different assumptions for each:\n\nSmoothness assumption: similar points have similar labels (if two points are close, their label is also close; this is also the assumption of KNN)\nwe can just assign labels to each data point using KNN\n\nClustering assumption: the data is split into clusters and if two points are in the same cluster, their label is the same.\nWe run K-means or other clustering algorithm and assign labels according to clusters\n\nManifold assumption: there exists a low-dimensional manifold / curve such that all data points approximately lie on that surface\nWe run some dimension reduction algorithm on the whole dataset (PCA or encoder part of autoencoder) and learn an algorithm only on the labeled data. That is, we first try to obtain a lower-dim representation of the whole dataset and hope the labeled data alone is now enough to learn a good model on the reduced dimension.","type":"content","url":"/2022-10-19-beyond-supervised-learning#semi-supervised-learning","position":5},{"hierarchy":{"lvl1":"Beyond supervised learning","lvl2":"Weak Supervision Learning"},"type":"lvl2","url":"/2022-10-19-beyond-supervised-learning#weak-supervision-learning","position":6},{"hierarchy":{"lvl1":"Beyond supervised learning","lvl2":"Weak Supervision Learning"},"content":"Labelling is either too expensive or requires too much expertise. So we can only get labels that are noisy or imprecise through\n\nCrowdsourcing from non-experts\n\nData programming: use functions / models / external databases to heuristically label examples\n\nReference: \n\nhttp://​ai​.stanford​.edu​/blog​/weak​-supervision/","type":"content","url":"/2022-10-19-beyond-supervised-learning#weak-supervision-learning","position":7},{"hierarchy":{"lvl1":"Beyond supervised learning","lvl2":"Self-Supervised Learning"},"type":"lvl2","url":"/2022-10-19-beyond-supervised-learning#self-supervised-learning","position":8},{"hierarchy":{"lvl1":"Beyond supervised learning","lvl2":"Self-Supervised Learning"},"content":"Extract a supervision label from the unlabeled data itself.\n\nFor example, we have a “fill-in-the-blank” setting for computer vision: Take a image. Remove patches from the image. Train a DNN to recover the original image from the version with the patches removed.","type":"content","url":"/2022-10-19-beyond-supervised-learning#self-supervised-learning","position":9},{"hierarchy":{"lvl1":"Attention, Transformers, and Transfer Learning"},"type":"lvl1","url":"/2022-10-24-attention-transformers-and-transfer-lea","position":0},{"hierarchy":{"lvl1":"Attention, Transformers, and Transfer Learning"},"content":"","type":"content","url":"/2022-10-24-attention-transformers-and-transfer-lea","position":1},{"hierarchy":{"lvl1":"Attention, Transformers, and Transfer Learning","lvl2":"Sequence Models"},"type":"lvl2","url":"/2022-10-24-attention-transformers-and-transfer-lea#sequence-models","position":2},{"hierarchy":{"lvl1":"Attention, Transformers, and Transfer Learning","lvl2":"Sequence Models"},"content":"Pad to max length: works alright in most cases, but the following is a common failure case imagine max length is five\n\nHello Yao xx xx xx\n\nOh hello Yao xx xx\n\nThough these two sentences are similar in both looking (offset only by 1) and meaning, they look pretty differently after padding. So they are rather different in the padded space.\n\nCounting word appearance: this approach ignores order, so it is bad\n\nRecursive Neural Network: by design, it handles input in a sequential way, which has some limitations (forgetting previous reference, ambiguous reference, ...)\n\nTransformers: the superior choice, looks data in a parallel way","type":"content","url":"/2022-10-24-attention-transformers-and-transfer-lea#sequence-models","position":3},{"hierarchy":{"lvl1":"Attention, Transformers, and Transfer Learning","lvl2":"Recursive Neural Network"},"type":"lvl2","url":"/2022-10-24-attention-transformers-and-transfer-lea#recursive-neural-network","position":4},{"hierarchy":{"lvl1":"Attention, Transformers, and Transfer Learning","lvl2":"Recursive Neural Network"},"content":"RNN kind of resembles Finite State Machine, where you have the state transition functions_{i+1} = \\delta(s_{i}, x_i)\n\nexcept the state space RNN operates on is   the continuous real numbers.","type":"content","url":"/2022-10-24-attention-transformers-and-transfer-lea#recursive-neural-network","position":5},{"hierarchy":{"lvl1":"Attention, Transformers, and Transfer Learning","lvl2":"Attention"},"type":"lvl2","url":"/2022-10-24-attention-transformers-and-transfer-lea#attention","position":6},{"hierarchy":{"lvl1":"Attention, Transformers, and Transfer Learning","lvl2":"Attention"},"content":"The attention model wants the tokens to interact not by ordering or position (as in RNN), but by how they look similar to each other in the embedding space.\n\nYou can think of them as differentiable relaxation of the concept of “dictionary” -- Christian Szegedy","type":"content","url":"/2022-10-24-attention-transformers-and-transfer-lea#attention","position":7},{"hierarchy":{"lvl1":"Incomplete CS Notes @ Cornell"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Incomplete CS Notes @ Cornell"},"content":"\n\nThis is a collection of notes I took in CS classes at Cornell University, limited to the ones written in Markdown. \n\nVisit my personal website for more info\n\nThe author of these notes is actively looking for job right now. Feel free to reach out via \n\nemail or \n\nLinkedIn.\n\nLogo generated by ChatGPT","type":"content","url":"/","position":1}]}