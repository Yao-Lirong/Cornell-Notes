{"version":2,"kind":"Article","sha256":"8a1827e641d2d95732f2fde78e8bf725b296a1ad109efddfd7a7d28c185bc9ff","slug":"2022-10-24-attention-transformers-and-transfer-lea","location":"/CS4787/2022-10-24-Attention,-Transformers,-and-Transfer-Learning.md","dependencies":[],"frontmatter":{"title":"Attention, Transformers, and Transfer Learning","tags":["CS4787"],"date":"2022-10-24","authors":[{"nameParsed":{"literal":"Yao Lirong","given":"Yao","family":"Lirong"},"name":"Yao Lirong","affiliations":["Cornell University"],"url":"https://yao-lirong.github.io","linkedin":"https://www.linkedin.com/in/yao-lirong/","id":"contributors-myst-generated-uid-0"}],"keywords":["Cornell","CS","Yao Lirong"],"affiliations":[{"id":"Cornell University","name":"Cornell University"}],"numbering":{"title":{"offset":1}},"exports":[{"format":"md","filename":"2022-10-24-Attention,-Transformers,-and-Transfer-Learning.md","url":"/cornell-notes/build/2022-10-24-Attention-da1bbf98f425f8b81be5672fedc3d1db.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Sequence Models","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"WS1XvYPY4s"}],"identifier":"sequence-models","label":"Sequence Models","html_id":"sequence-models","implicit":true,"key":"qVTh8p7SIT"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Pad to max length: works alright in most cases, but the following is a common failure case imagine max length is five","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"ZM9GRH0lwJ"}],"key":"dOEO1A4kXl"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":12,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Hello Yao xx xx xx","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"KQ5b9hLP2L"}],"key":"CHOHJEW2lE"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Oh hello Yao xx xx","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"pXNAzCEirG"}],"key":"aJCPMu5iFR"}],"key":"xvQnyCeE6a"},{"type":"paragraph","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"Though these two sentences are similar in both looking (offset only by 1) and meaning, they look pretty differently after padding. So they are rather different in the padded space.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"pYdGBAfoiZ"}],"key":"y3wevqaOgV"}],"key":"X0jvHNThiN"},{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"Counting word appearance: this approach ignores order, so it is bad","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"SZPAqLpOzv"}],"key":"dEoPGvIgQL"}],"key":"i0Rv8t5mjX"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"Recursive Neural Network: by design, it handles input in a sequential way, which has some limitations (forgetting previous reference, ambiguous reference, ...)","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"DikwmPyxn1"}],"key":"ND32NtRreS"}],"key":"thzgd67gNw"},{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"Transformers: the superior choice, looks data in a parallel way","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"HHhera4bU5"}],"key":"qZRSIOAz8q"}],"key":"Na51mczsIv"}],"key":"mm1KNQ4b6W"},{"type":"heading","depth":2,"position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"Recursive Neural Network","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"z6Ne4G0Ryn"}],"identifier":"recursive-neural-network","label":"Recursive Neural Network","html_id":"recursive-neural-network","implicit":true,"key":"fEiyVnkrcw"},{"type":"paragraph","position":{"start":{"line":27,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"RNN kind of resembles Finite State Machine, where you have the state transition function","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"ZnfpI2vxtX"}],"key":"CQymR4CK54"},{"type":"math","value":"s_{i+1} = \\delta(s_{i}, x_i)","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"tight":true,"html":"<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>δ</mi><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo separator=\"true\">,</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">s_{i+1} = \\delta(s_{i}, x_i)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03785em;\">δ</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>","enumerator":"1","key":"wrniseFfVO"},{"type":"paragraph","position":{"start":{"line":27,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"except the state space RNN operates on is   the continuous real numbers.","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"JUxURKQqOA"}],"key":"upoHrqQYQV"},{"type":"heading","depth":2,"position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"Attention","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"m41ZucegUM"}],"identifier":"attention","label":"Attention","html_id":"attention","implicit":true,"key":"lF8pvVwkKy"},{"type":"paragraph","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"The attention model wants the tokens to interact not by ordering or position (as in RNN), but by how they look similar to each other in the embedding space.","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"c3pN1jaKgZ"}],"key":"dvZditHTYg"},{"type":"paragraph","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"You can think of them as ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"HQJaHvbDz7"},{"type":"strong","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"differentiable relaxation of the concept of “dictionary”","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"huQKNJpMWx"}],"key":"PaO2unFY1R"},{"type":"text","value":" -- Christian Szegedy","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"O0y32hVbrt"}],"key":"jrwsxb6ufK"}],"key":"GunR5L6Leg"}],"key":"FPqNwMhIVi"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Beyond supervised learning","url":"/2022-10-19-beyond-supervised-learning","group":"CS4787 Principles of Large-Scale Machine Learning"},"next":{"title":"CS4820 Intro to Analysis of Algorithms","url":"/2020-10-13-cs4820-intro-to-analysis-of-algorithms","group":"CS4787 Principles of Large-Scale Machine Learning"}}},"domain":"http://localhost:3000"}