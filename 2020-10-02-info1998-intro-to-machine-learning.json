{"version":2,"kind":"Article","sha256":"70cb7e27c03568a34b80deb0626cd6c1e07753fd1a205aa734c76165503f59ed","slug":"2020-10-02-info1998-intro-to-machine-learning","location":"/2020-10-02-INFO1998-Intro-to-Machine-Learning.md","dependencies":[],"frontmatter":{"title":"INFO1998 Intro to Machine Learning","tags":["Cornell","20FA","Python"],"date":"2020-10-02","authors":[{"nameParsed":{"literal":"Yao Lirong","given":"Yao","family":"Lirong"},"name":"Yao Lirong","affiliations":["Cornell University"],"url":"https://yao-lirong.github.io","linkedin":"https://www.linkedin.com/in/yao-lirong/","id":"contributors-myst-generated-uid-0"}],"keywords":["Cornell","CS","Yao Lirong"],"affiliations":[{"id":"Cornell University","name":"Cornell University"}],"exports":[{"format":"md","filename":"2020-10-02-INFO1998-Intro-to-Machine-Learning.md","url":"/cornell-notes/build/2020-10-02-INFO1998--e6bc3205fd7dafe80315ee3d66864e13.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"The goal of this course is to provide you with a high-level exposure to a wide range of Data Science techniques and Machine Learning models. From the basics of getting your Jupyter environment setup, to manipulating  and visualizing data, to building supervised and unsupervised models,  this class aims to give you the base intuition and skillset to continue  developing and working on ML projects. We hope you exit the course with  an understanding of how models and optimization techniques work, as well as have the confidence and tools to solve future problems on your own.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"Q7MS82RfN3"}],"key":"UjGyiIe79N"},{"type":"comment","value":"more","key":"ickCr1xZsX"},{"type":"heading","depth":2,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Lec2 Data Manipulation","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"vVopIY40Am"}],"identifier":"lec2-data-manipulation","label":"Lec2 Data Manipulation","html_id":"lec2-data-manipulation","implicit":true,"key":"E0KouW5EDS"},{"type":"heading","depth":3,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Introduction to Pandas","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"Suf6pd83M9"}],"identifier":"introduction-to-pandas","label":"Introduction to Pandas","html_id":"introduction-to-pandas","implicit":true,"key":"O5763ckSpz"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":18,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"inlineCode","value":"Series","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"swJeggk5Xm"},{"type":"text","value":": one dimensional array","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"eO4d95qGi0"}],"key":"EL7doyotCk"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"inlineCode","value":"DataFrame","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"XYAZWLCc7s"},{"type":"text","value":": 2-D table","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"jerbBfEPOa"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":20,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"Filtering DataFrames: ","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"a1PVpXQlbD"},{"type":"inlineCode","value":"loc","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"cRzg180sWj"}],"key":"tHBIe0D4YD"},{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"Cleaning-Up DataFrames: ","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"s05PwBnoLr"},{"type":"inlineCode","value":"df.dropna()","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"vNoFcY69ZK"},{"type":"text","value":", ","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"zJFwJ2fDvu"},{"type":"inlineCode","value":"df[df['Open'].notnull()]","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"xFXV8tvSLZ"},{"type":"text","value":" (These two methods both return a new DataFrame instead of modifying the existed one)","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"B4b56UpUsM"}],"key":"BzTHgTBnD6"},{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"View DataFrames: ","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"oqYZVNWcPa"},{"type":"inlineCode","value":"head","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"pBHM3RDS3o"},{"type":"text","value":", ","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"M3f6wVCEyo"},{"type":"inlineCode","value":"tail","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"Qa00QOf7Fe"},{"type":"text","value":", ...","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"xxq9BzbOwQ"}],"key":"NtORwO5v62"},{"type":"listItem","spread":true,"position":{"start":{"line":23,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"Summary Statistics: ","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"aUydkDnOHS"},{"type":"inlineCode","value":"mean","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"tpgsuypELH"},{"type":"text","value":", ","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"vl0OUdHzpe"},{"type":"inlineCode","value":"median","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"cL1FH7n3KJ"},{"type":"text","value":", ... ","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"ZqBdVyUXBA"},{"type":"inlineCode","value":"describe","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"jFj8IYko5r"}],"key":"oUliRAu3oI"}],"key":"kPmXYZAnUQ"}],"key":"TiBKAy6so9"}],"key":"SN74XTE7m6"},{"type":"heading","depth":3,"position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"Dealing with missing data","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"zCxUt203tP"}],"identifier":"dealing-with-missing-data","label":"Dealing with missing data","html_id":"dealing-with-missing-data","implicit":true,"key":"BGDZG0jEMu"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":27,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":27,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"Fill in some random info of our choice:","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"r7417UZb22"}],"key":"g9KAWmKKMU"},{"type":"code","lang":"python","value":"#if we there is no record about which cabin he is in, we assume he is on the Top Deck\ndf['Cabin']=df['Cabin'].fillna('Top Deck') ","position":{"start":{"line":29,"column":1},"end":{"line":32,"column":1}},"key":"yqYiKHWoQ1"}],"key":"boe5RRZRIo"},{"type":"listItem","spread":true,"position":{"start":{"line":34,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"text","value":"Using summary statistics: fill missing entries with median or mean","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"lCsCLJ9hdg"}],"key":"RGE8DlQnTX"},{"type":"paragraph","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"text","value":"works well with small set","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"key":"kPZmlTondt"}],"key":"oj8gztCnpr"}],"key":"x2vdhn6Iyi"},{"type":"listItem","spread":true,"position":{"start":{"line":38,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"text","value":"Use regression and clustering: will be covered later","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"THLK4jdj8n"}],"key":"VAjoJ8MZlZ"}],"key":"V9D4Uy8ByE"}],"key":"WRFlBp2vL7"},{"type":"heading","depth":2,"position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"text","value":"Lec3 Data Visualization","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"key":"GjQy2BmsZc"}],"identifier":"lec3-data-visualization","label":"Lec3 Data Visualization","html_id":"lec3-data-visualization","implicit":true,"key":"G7MG5M8yP8"},{"type":"heading","depth":3,"position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"children":[{"type":"text","value":"Types of Graphs","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"key":"xzeRrgkgQA"}],"identifier":"types-of-graphs","label":"Types of Graphs","html_id":"types-of-graphs","implicit":true,"key":"nJputZ4nXN"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":44,"column":1},"end":{"line":46,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":44,"column":1},"end":{"line":44,"column":1}},"children":[{"type":"text","value":"Heatmap","position":{"start":{"line":44,"column":1},"end":{"line":44,"column":1}},"key":"VpuYo63YKj"}],"key":"BMHt5N3EJv"},{"type":"listItem","spread":true,"position":{"start":{"line":45,"column":1},"end":{"line":46,"column":1}},"children":[{"type":"text","value":"Correlation Plots","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"key":"BvyCjKlUq3"}],"key":"gHWHBM0Vw8"}],"key":"hEj1YhobQV"},{"type":"heading","depth":3,"position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"Coloring Graphs","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"ptTjqn68Bd"}],"identifier":"coloring-graphs","label":"Coloring Graphs","html_id":"coloring-graphs","implicit":true,"key":"BBkSftpHGv"},{"type":"paragraph","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"children":[{"type":"inlineCode","value":"plt.scatter(Longitude, Latitude, c=Temp.values.ravel(),cmap=plt.cm.OrRd)","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"Kxo4NZVGZ3"},{"type":"text","value":" color a scattered plot based on values of ","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"iL188COxck"},{"type":"inlineCode","value":"Temp","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"gkIOXJ0PiD"},{"type":"text","value":" with color scheme ","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"Kf8iQi1BDo"},{"type":"inlineCode","value":"cm.OrRd","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"ULwmvQwcuU"},{"type":"text","value":". Find more color schemes from ","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"bcwp3ma55X"},{"type":"link","url":"https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"children":[{"type":"text","value":"matplotlib manual","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"oRTXwaR1El"}],"urlSource":"https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html","key":"lgJh8mjzEk"},{"type":"text","value":".","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"Y7qayoZY8Z"}],"key":"sPeziMl6tH"},{"type":"heading","depth":2,"position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"children":[{"type":"text","value":"Lec4 Linear Regression","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"key":"bb2jKUbQPy"}],"identifier":"lec4-linear-regression","label":"Lec4 Linear Regression","html_id":"lec4-linear-regression","implicit":true,"key":"VwcRRhFdfq"},{"type":"heading","depth":3,"position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"children":[{"type":"text","value":"Preparing Data","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"key":"RJtYmnSkWD"}],"identifier":"preparing-data","label":"Preparing Data","html_id":"preparing-data","implicit":true,"key":"mjRhSmUKfv"},{"type":"code","lang":"python","value":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n# X must be a table (in case there are multiple x in y = a1*x1 + a2*x2 + ... + k)\nX = data[['cost','compl_4']] \n# Y must be one column\nY = data['median_earnings'] \n\nfrom sklearn.model_selection import train_test_split\n# test is 20% of all data\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)","position":{"start":{"line":55,"column":1},"end":{"line":66,"column":1}},"key":"tkkQo0uXJp"},{"type":"heading","depth":3,"position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"children":[{"type":"text","value":"Predicting and Fitting","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"key":"bJinFDYbh7"}],"identifier":"predicting-and-fitting","label":"Predicting and Fitting","html_id":"predicting-and-fitting","implicit":true,"key":"tlqHVCThx5"},{"type":"code","lang":"python","value":"# creates Linear Regression model \nLR = LinearRegression()\n# note LR is an object by calling fit, we set all of its coefficients\nLR.fit(x_train, y_train)\n# predict() returns the predicted value\ny_predicted = LR.predict(x_test)\n# score(x,y') first computes the predicted value y based on x and our model, then compare it with y'\nscore = LR.score(x_test,y_test)","position":{"start":{"line":70,"column":1},"end":{"line":79,"column":1}},"key":"qVDpVR4XXw"},{"type":"heading","depth":3,"position":{"start":{"line":81,"column":1},"end":{"line":81,"column":1}},"children":[{"type":"text","value":"Describing the Model","position":{"start":{"line":81,"column":1},"end":{"line":81,"column":1}},"key":"iniVmCnlbg"}],"identifier":"describing-the-model","label":"Describing the Model","html_id":"describing-the-model","implicit":true,"key":"q3E5wXEI8P"},{"type":"code","lang":"python","value":"# Gives a comprehensive view of Y = a1*x1 + a2*x2 + ... + k\nLR?\n\n# coefficients of x (a1, a2, ...)\nLR.coef_\n\n# intercept k\nLR.intercept_","position":{"start":{"line":83,"column":1},"end":{"line":92,"column":1}},"key":"LD8iCtjjxD"},{"type":"heading","depth":2,"position":{"start":{"line":94,"column":1},"end":{"line":94,"column":1}},"children":[{"type":"text","value":"Lec5 Measuring Model’s Accuracy","position":{"start":{"line":94,"column":1},"end":{"line":94,"column":1}},"key":"gXRjsRidYq"}],"identifier":"lec5-measuring-models-accuracy","label":"Lec5 Measuring Model’s Accuracy","html_id":"lec5-measuring-models-accuracy","implicit":true,"key":"c9NEBNFkB2"},{"type":"paragraph","position":{"start":{"line":96,"column":1},"end":{"line":96,"column":1}},"children":[{"type":"text","value":"When determining accuracy, usually want to compare our model to a baseline. Therefore, instead of comparing our model’s prediction to each specific ","position":{"start":{"line":96,"column":1},"end":{"line":96,"column":1}},"key":"Jw9tgsLPGA"},{"type":"inlineCode","value":"y","position":{"start":{"line":96,"column":1},"end":{"line":96,"column":1}},"key":"e8gWNm8Iqm"},{"type":"text","value":" value, we compare it with the mean ","position":{"start":{"line":96,"column":1},"end":{"line":96,"column":1}},"key":"V662EaUHxL"},{"type":"inlineCode","value":"y","position":{"start":{"line":96,"column":1},"end":{"line":96,"column":1}},"key":"VdSMRNSLuI"},{"type":"text","value":" value.","position":{"start":{"line":96,"column":1},"end":{"line":96,"column":1}},"key":"ie9Ovl5TtG"}],"key":"h6XHIs3wR3"},{"type":"code","lang":"python","value":"from sklearn.metrics import mean_squared_error\ncelcius_MSE = mean_squared_error(y_test, celcius_predictions)\n\ntest_goal_mean = y_test.mean()\nbaseline = np.full((len(celcius_predictions),), test_goal_mean)\nbaseline_MSE = mean_squared_error(baseline, celcius_predictions)","position":{"start":{"line":98,"column":1},"end":{"line":105,"column":1}},"key":"bCfYQ0EwIV"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":107,"column":1},"end":{"line":109,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":107,"column":1},"end":{"line":107,"column":1}},"children":[{"type":"text","value":"overfitting: too specific to the data given, doesn’t predict any other data","position":{"start":{"line":107,"column":1},"end":{"line":107,"column":1}},"key":"AJCmkLD1Gs"}],"key":"y92ILmJq1U"},{"type":"listItem","spread":true,"position":{"start":{"line":108,"column":1},"end":{"line":109,"column":1}},"children":[{"type":"text","value":"underfitting: no matter what data you use to train this model, it gives the same curve, so it doesn’t have prediction power either because it doesn’t show any pattern of the data.","position":{"start":{"line":108,"column":1},"end":{"line":108,"column":1}},"key":"LEfCd7vW2P"}],"key":"uTEIGfc4Sf"}],"key":"iOCeMm4NAR"},{"type":"heading","depth":2,"position":{"start":{"line":110,"column":1},"end":{"line":110,"column":1}},"children":[{"type":"text","value":"Lec6 Classifiers","position":{"start":{"line":110,"column":1},"end":{"line":110,"column":1}},"key":"QauTVvTdYv"}],"identifier":"lec6-classifiers","label":"Lec6 Classifiers","html_id":"lec6-classifiers","implicit":true,"key":"I35T7WXDXD"},{"type":"paragraph","position":{"start":{"line":112,"column":1},"end":{"line":112,"column":1}},"children":[{"type":"text","value":"Linear regression is used to predict the value of a continuous variable. Classifiers are used to predict ","position":{"start":{"line":112,"column":1},"end":{"line":112,"column":1}},"key":"JABmmGy1VP"},{"type":"strong","position":{"start":{"line":112,"column":1},"end":{"line":112,"column":1}},"children":[{"type":"text","value":"categorical or binary variables","position":{"start":{"line":112,"column":1},"end":{"line":112,"column":1}},"key":"nM7C7JQmvB"}],"key":"pjmLvy69Ng"},{"type":"text","value":".","position":{"start":{"line":112,"column":1},"end":{"line":112,"column":1}},"key":"TAqhHWfRDa"}],"key":"dpOzNQPpKW"},{"type":"heading","depth":3,"position":{"start":{"line":114,"column":1},"end":{"line":114,"column":1}},"children":[{"type":"text","value":"KNN","position":{"start":{"line":114,"column":1},"end":{"line":114,"column":1}},"key":"gEMx6BZjSZ"}],"identifier":"knn","label":"KNN","html_id":"knn","implicit":true,"key":"KjVJSuFSH8"},{"type":"code","lang":"python","value":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.2)\n\nk = 10\nmodel = KNeighborsClassifier(k) # specify k nearest elements\nmodel.fit(x_train,y_train)\npredictions = model.predict(x_test)","position":{"start":{"line":116,"column":1},"end":{"line":125,"column":1}},"key":"Q5yJrC9do1"},{"type":"heading","depth":2,"position":{"start":{"line":127,"column":1},"end":{"line":127,"column":1}},"children":[{"type":"text","value":"Lec7 Other Supervised Learning Models","position":{"start":{"line":127,"column":1},"end":{"line":127,"column":1}},"key":"AirGp0klbn"}],"identifier":"lec7-other-supervised-learning-models","label":"Lec7 Other Supervised Learning Models","html_id":"lec7-other-supervised-learning-models","implicit":true,"key":"pqmuvRwvCe"},{"type":"heading","depth":3,"position":{"start":{"line":129,"column":1},"end":{"line":129,"column":1}},"children":[{"type":"text","value":"Decision Trees","position":{"start":{"line":129,"column":1},"end":{"line":129,"column":1}},"key":"kkh9iDr8uX"}],"identifier":"decision-trees","label":"Decision Trees","html_id":"decision-trees","implicit":true,"key":"U9qM5CjTkr"},{"type":"code","lang":"python","value":"from sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = tree.DecisionTreeClassifier(max_depth=5)","position":{"start":{"line":131,"column":1},"end":{"line":135,"column":1}},"key":"O5I05PdnLb"},{"type":"paragraph","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"children":[{"type":"text","value":"How to reduce overfitting?","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"ZnhbhWqKi4"}],"key":"vpV86zNhZ4"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":139,"column":1},"end":{"line":141,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":139,"column":1},"end":{"line":139,"column":1}},"children":[{"type":"text","value":"Reduce levels of trees","position":{"start":{"line":139,"column":1},"end":{"line":139,"column":1}},"key":"YSPO1Hp7rn"}],"key":"efCeK819MD"},{"type":"listItem","spread":true,"position":{"start":{"line":140,"column":1},"end":{"line":141,"column":1}},"children":[{"type":"text","value":"Train multiple decision trees (maybe one for each training data) and take its average as final result","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"mFRUmV9yTq"}],"key":"zeX4FKKeRc"}],"key":"eeVxBvEKPe"},{"type":"heading","depth":3,"position":{"start":{"line":142,"column":1},"end":{"line":142,"column":1}},"children":[{"type":"text","value":"Logistic Regression","position":{"start":{"line":142,"column":1},"end":{"line":142,"column":1}},"key":"OGJPMXhdaP"}],"identifier":"logistic-regression","label":"Logistic Regression","html_id":"logistic-regression","implicit":true,"key":"OP3nc54UPM"},{"type":"paragraph","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"children":[{"type":"text","value":"Value always between 0 and 1. Accept if value higher than threshold, reject if lower.","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"EhVcZMstXB"}],"key":"TF85iCXl07"},{"type":"heading","depth":3,"position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"children":[{"type":"text","value":"K-fold Cross Validation","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"zPkSgaeVyQ"}],"identifier":"k-fold-cross-validation","label":"K-fold Cross Validation","html_id":"k-fold-cross-validation","implicit":true,"key":"LVrtVwFhoV"},{"type":"paragraph","position":{"start":{"line":148,"column":1},"end":{"line":148,"column":1}},"children":[{"type":"text","value":"Rather than doing test-train split only once, we do it k times: First separate our sample into k pieces and each time we take one of them as test set, the others as training set. Use ","position":{"start":{"line":148,"column":1},"end":{"line":148,"column":1}},"key":"bNlsPq52RM"},{"type":"inlineCode","value":"from sklearn.model_selection import KFold","position":{"start":{"line":148,"column":1},"end":{"line":148,"column":1}},"key":"JIs2H9PLa7"},{"type":"text","value":" to achieve this. Calculate a score for each of the split and take its average as the final score. This score is usually closer to real errors.","position":{"start":{"line":148,"column":1},"end":{"line":148,"column":1}},"key":"xJoVtRxO7Y"}],"key":"AeRsAZcQxG"},{"type":"code","lang":"python","value":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, accuracy_score\n\nincX = inc_data[['education.num']]\nincY = inc_data['income']\n\nkf = KFold(n_splits = 5)\naccuracy = 0\nfor train_index, test_index in kf.split(incX):\n    X_train = incX.iloc[train_index]\n    Y_train = incY.iloc[train_index]\n    X_test = incX.iloc[test_index]\n    Y_test = incY.iloc[test_index]\n    \n    # best_depth 是我们前一题找到的使分最高的 depth level of decision tree\n    model = tree.DecisionTreeClassifier (max_depth = best_depth)\n    model.fit(X_train, Y_train)\n    pred_test = model.predict(X_test)\n    accuracy += accuracy_score(Y_test, pred_test)\n    \naccuracy /= 5\nprint(accuracy)","position":{"start":{"line":150,"column":1},"end":{"line":173,"column":1}},"key":"tDelYbxS58"},{"type":"heading","depth":2,"position":{"start":{"line":175,"column":1},"end":{"line":175,"column":1}},"children":[{"type":"text","value":"Lec9 Unsupervised Learning","position":{"start":{"line":175,"column":1},"end":{"line":175,"column":1}},"key":"kW2yBcgRCp"}],"identifier":"lec9-unsupervised-learning","label":"Lec9 Unsupervised Learning","html_id":"lec9-unsupervised-learning","implicit":true,"key":"rr3QH3ybmG"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":177,"column":1},"end":{"line":179,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":177,"column":1},"end":{"line":177,"column":1}},"children":[{"type":"text","value":"Supervised Learning: The desired solution (target) is also included in the dataset","position":{"start":{"line":177,"column":1},"end":{"line":177,"column":1}},"key":"xBa9VrpTi5"}],"key":"nQpULkfFJ2"},{"type":"listItem","spread":true,"position":{"start":{"line":178,"column":1},"end":{"line":179,"column":1}},"children":[{"type":"text","value":"Unsupervised Learning: The training data is unlabeled and algorithm tries to learn by itself","position":{"start":{"line":178,"column":1},"end":{"line":178,"column":1}},"key":"bVriHTdHEB"}],"key":"C2EdFB4QhQ"}],"key":"LzhGzkn7Sy"},{"type":"heading","depth":3,"position":{"start":{"line":180,"column":1},"end":{"line":180,"column":1}},"children":[{"type":"text","value":"Hierarchical Clustering","position":{"start":{"line":180,"column":1},"end":{"line":180,"column":1}},"key":"ecMgAj6N9g"}],"identifier":"hierarchical-clustering","label":"Hierarchical Clustering","html_id":"hierarchical-clustering","implicit":true,"key":"TTLeNbv4zD"},{"type":"paragraph","position":{"start":{"line":182,"column":1},"end":{"line":182,"column":1}},"children":[{"type":"text","value":"Hierarchical clustering groups observations into multiple levels of  sets; the top-level set includes all of the data, and the bottom-level  sets contain individual observations. The levels in between contain sets of observations with similar features.","position":{"start":{"line":182,"column":1},"end":{"line":182,"column":1}},"key":"GxslvkmC7f"}],"key":"NvSRm91eWQ"},{"type":"code","lang":"python","value":"from sklearn.preprocessing import StandardScaler\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom matplotlib import pyplot as plt\n\n# Standardize features by removing the mean and scaling to unit variance\ndata = StandardScaler().fit_transform(data)\n# build our model from data\nclust = linkage(data) \n# draw the dendrogram visulization\ndendrogram(clust)\nplt.show()","position":{"start":{"line":184,"column":1},"end":{"line":196,"column":1}},"key":"OteP5ukZ8I"},{"type":"heading","depth":3,"position":{"start":{"line":198,"column":1},"end":{"line":198,"column":1}},"children":[{"type":"text","value":"K-Means Clustering","position":{"start":{"line":198,"column":1},"end":{"line":198,"column":1}},"key":"izGpq6aWZv"}],"identifier":"k-means-clustering","label":"K-Means Clustering","html_id":"k-means-clustering","implicit":true,"key":"cDFlIgD0O4"},{"type":"paragraph","position":{"start":{"line":200,"column":1},"end":{"line":200,"column":1}},"children":[{"type":"text","value":"We want to cluster the data into k groups. We first randomly choose k points in this dataset. Then we assign other data points to the group they are closest to. After assigning all data points to some group, we recompute the center of each group by taking the means of all points in that group. Repeat this process until no points change group assignment after one iteration.","position":{"start":{"line":200,"column":1},"end":{"line":200,"column":1}},"key":"B8oNszCnKL"}],"key":"KLvrGT990K"},{"type":"code","lang":"python","value":"from sklearn import cluster\nk = 3\nkmeans = cluster.KMeans(n_clusters = k) #cluster into k groups\nkmeans.fit(data)","position":{"start":{"line":202,"column":1},"end":{"line":207,"column":1}},"key":"v7PONFRE9H"}],"key":"PSHiCDyUsL"}],"key":"hIYPU4kvHl"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Incomplete CS Notes @ Cornell","url":"/","group":"Incomplete CS Notes @ Cornell"},"next":{"title":"CS2024 C++ Programming","url":"/2020-09-07-cs2024-c-programming","group":"Incomplete CS Notes @ Cornell"}}},"domain":"http://localhost:3000"}